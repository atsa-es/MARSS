\SweaveOpts{keep.source=TRUE, prefix.string=./figures/STS-, eps=FALSE, split=TRUE, height=3, out.height='3in'}
<<RUNFIRST, echo=FALSE, include.source=FALSE>>=
options(prompt = " ", continue = " ", width = 60)
@

\chapter{Structural Time Series Models}
\label{chap:structts}
\chaptermark{STS Models}
%Add footnote with instructions for getting code
\blfootnote{Type \texttt{RShowDoc("Chapter\_Structural\_TS.R",package="MARSS")} at the R command line to open a file with all the code for the examples in this chapter.}


Structural time series models are linear Gaussian state-space models which decompose the series into additive random walks for the level, trend and season. These models can be written as a multivariate state-space model. R provides the \verb@StructTS()@ function in the \{stats\} package to fit the level, level plus trend, and level plus trend plus season versions of structural time series models to univariate data.

Here it is shown how to fit structural time series models with \verb@MARSS()@ using the same initial conditions assumptions as used in the \verb@StructTS()@ function. With \verb@MARSS()@, you are not restricted to univariate time series and you have control over any parameter constraints that you wish to impose. You will see how to do this after the univariate cases are shown.

Required libraries for this chapter:
<<label=Cs00_required-libraries>>=
library(MARSS)
library(tidyr)
library(ggplot2)
library(forecast)
@

\section{Univariate models}
\index{structural ts models!univariate}

\subsection{Level model}
\index{structural ts models!level}

The basic stochastic level model fit by \verb@stats::StructTS()@ is
\begin{equation}\label{eqn:struct.ts.y}
y_t = m_t+v_t \text{ where } v_t \sim \N(0,\sigma^2_\epsilon)  \\
\end{equation}
where $m$ is the level and is a random walk:

\begin{equation}\label{eqn:struct.ts.x}
m_t = m_{t-1}+ w_t \text{ where } w_t \sim \N(0,\sigma^2_\xi)
\end{equation}

The initial conditions assumption used in the \verb@StructTS()@ function is the following and this must be used in the \verb@MARSS()@ model in order to replicate the \verb@StructTS()@ output. The initial condition at $t=0$ for $m$ is stochastic with fixed mean equal to $y_1$ and variance equal to 10000 times the variance of the data, denoted $s^2$.

\begin{equation}\label{eqn:struct.ts.x0}
m_0 \sim \N(y_1, 10000 s^2)
\end{equation}

Here the model is fit to 20 time steps of tree ring data. \verb@fit1@ is the \verb@StructTS()@ output, 
\verb@fit2@ is fit with \verb@MARSS()@ with parameters fixed at the \verb@StructTS()@ estimated values, \verb@fit3@ is the model fit with \verb@MARSS()@ using BFGS, and \verb@fit4@ is the model fit with \verb@MARSS()@ using EM. \verb@fit3@ and \verb@fit4@ are slightly different than \verb@fit1@ because the optimization algorithm is a hill-climbing algorithm for all these fits and stops at slightly different points on the likelihood hill.

Fit with \verb@StructTS()@.
<<label=Cs101_structTS-level, keep.source=TRUE, results=hide>>=
y <- window(treering, start = 0, end = 20)

fit1 <- StructTS(y, type = "level")
@

Fit with \verb@MARSS()@. We set \verb@control=list(allow.degen=FALSE)@ when using the EM algorithm (the default) in order to compare results to the BFGS algorithm used in \verb@StructTS()@. This will not allow variances to go to zero; they may appear that way in the output but that is rounding.
<<label=Cs102_structTS-level, keep.source=TRUE, results=hide>>=
vy <- var(y, na.rm = TRUE) / 100
mod.list <- list(
  x0 = matrix(y[1]), U = "zero", tinitx = 0,
  Q = matrix(fit1$coef[1]), R = matrix(fit1$coef[2]),
  V0 = matrix(1e+06 * vy)
)
fit2 <- MARSS(as.vector(y), model = mod.list)
# Now estimate the parameters
mod.list <- list(
  x0 = matrix(y[1]), U = "zero", tinitx = 0, V0 = matrix(1e+06 * vy),
  Q = matrix("s2xi"), R = matrix("s2eps")
)
fit3 <- MARSS(as.vector(y), model = mod.list, method = "BFGS")
fit4 <- MARSS(as.vector(y),
  model = mod.list,
  control = list(allow.degen = FALSE)
)
@

A difference with \verb@StructTS()@ is that the reported fitted level (the $x$ state estimate)\index{Kalman filter and smoother!StructTS} is the estimate of the state conditioned on the data up to $t$ not $T$. In the \{MARSS\} package, the state estimate (in the \verb@states@ element of the fitted object) is reported conditioned on all the data (up to $T$). To compare the outputs, we need to use \verb@MARSSkfss()@ to get \verb@xtt@ (the estimate of $x$ conditioned on data up to $t$).\index{functions!MARSSkfss}\index{Kalman filter and smoother}

<<label=Cs103_structTS-level, keep.source=TRUE>>=
fit2$kf <- MARSSkfss(fit2)
fit3$kf <- MARSSkfss(fit3)
fit4$kf <- MARSSkfss(fit4)
df <- data.frame(
  StructTS = fit1$fitted, fit2 = fit2$kf$xtt[1, ],
  fit.bfgs = fit3$kf$xtt[1, ], fit.em = fit4$kf$xtt[1, ]
)
head(df)
@

\begin{figure}[htp]
\begin{center}
<<label=Cs104_structTS-level, echo=FALSE, keep.source=TRUE, fig=TRUE, echo=FALSE>>=
require(tidyr)
require(ggplot2)
df1 <- as.data.frame(fit1$fitted)
vars <- colnames(df1)
df2 <- as.data.frame(t(fit3$kf$xtt))
colnames(df2) <- vars
df3 <- as.data.frame(t(fit4$kf$xtt))
colnames(df3) <- vars
df1$model <- "StructTS"
df2$model <- "MARSS BFGS"
df3$model <- "MARSS EM"
df1$t <- as.vector(time(fit1$fitted))
df2$t <- df1$t
df3$t <- df1$t
df <- rbind(df1, df2, df3)
df <- df %>% pivot_longer(all_of(vars))
ggplot(df, aes(x = t, y = value)) +
  geom_line() +
  facet_wrap(~model) +
  ggtitle("Level estimate from model fit with StructTS and MARSS")
@
\end{center}
\caption{Comparison of the level estimates for the stochastic level model.}
\label{fig:STS.fig.level}
\end{figure}

\subsection{Level plus trend model}
\index{structural ts models!trend}

The basic stochastic level plus trend model fit by \verb@stats::StructTS()@ is
\begin{equation}\label{eqn:struct.ts.trend.y}
y_t = m_t+v_t \text{ where } v_t \sim \N(0,\sigma^2_\epsilon)  
\end{equation}
where $m$ and $n$ are the stochastic level and trend. $m_t=m_{t-1}+n_{t-1}+w_t$ and in matrix form this is
\begin{equation}\label{eqn:struct.ts.trend.x}
\begin{bmatrix}
m\\
n
\end{bmatrix}_t = \begin{bmatrix}
1&1\\
0&1
\end{bmatrix}\begin{bmatrix}
m\\
n
\end{bmatrix}_{t-1}+\ww_t \text{ where } \ww_t \sim \MVN\left(0,\begin{bmatrix}
\sigma^2_\xi&0\\
0&\sigma^2_\zeta
\end{bmatrix}\right) 
\end{equation}
The initial conditions assumption used in the \verb@StructTS()@ for this model is the following where $s^2$ is the variance in the data ($\var(y)$):

\begin{equation}\label{eqn:struct.ts.trend.x0}
\begin{bmatrix}
m\\
n
\end{bmatrix}_0 \sim \MVN\left(\begin{bmatrix}
y_1\\
0
\end{bmatrix},
\begin{bmatrix}
10000 s^2& 10000 s^2\\
10000 s^2& 10000 s^2
\end{bmatrix}\right) \\
\end{equation}

Because \verb@MARSS()@ does an inversion of the initial variance matrix as part of code to force positive definite matrices and deal with degenerate models with 0s on diagonals of $\QQ$ or $\RR$, the initial conditions variance  used in \verb@StructTS()@ needs to be made positive definite for \verb@MARSS()@. This is done by adding a small value (1e-10) to the diagonal as shown in the \verb@mod.list@ used for \verb@fit3@ and \verb@fit4@. 

This model will be illustrated with the \verb@UKgas@ data set. For the tree ring data, the trend variance estimate is 0 and that will not illustrate a stochastic trend. The \verb@subset.ts()@ function in the \{forecast\} package is used to subset just the 2nd quarter data.

Fit with \verb@StructTS()@.
<<label=Cs201_structTS-leveltrend, keep.source=TRUE, results=hide>>=
y <- log10(forecast:::subset.ts(UKgas, quarter = 2))
fit1 <- StructTS(y, type = "trend")
@

Fit with \verb@MARSS()@. First we will create a MARSS model with the same parameters as the \verb@StructTS@ fit.
<<label=Cs202_structTS-leveltrend, keep.source=TRUE, results=hide>>=
vy <- var(y, na.rm = TRUE) / 100
B <- matrix(c(1, 0, 1, 1), 2, 2)
Z <- matrix(c(1, 0), 1, 2)
# fitx parameters at fit1 values
mod.list <- list(
  x0 = matrix(c(y[1], 0), 2, 1), U = "zero", tinitx = 0,
  Q = diag(fit1$coef[1:2]), R = matrix(fit1$coef[3]),
  V0 = matrix(1e+06 * vy, 2, 2), Z = Z, B = B
)
fit2 <- MARSS(as.vector(y),
  model = mod.list, fit = FALSE,
  control = list(trace = -1)
)
fit2$par <- fit2$start # otherwise par is NULL since fit=FALSE
@

Now estimate the parameters with \verb@MARSS()@.
<<label=Cs203_structTS-leveltrend, keep.source=TRUE, results=hide>>=
mod.list <- list(
  x0 = matrix(c(y[1], 0), 2, 1), U = "zero", tinitx = 0,
  Q = ldiag(c("s2xi", "s2zeta")), R = matrix("s2eps"),
  V0 = matrix(1e+06 * vy, 2, 2) + diag(1e-10, 2), Z = Z, B = B
)
fit3 <- MARSS(as.vector(y), model = mod.list, method = "BFGS")
fit4 <- MARSS(as.vector(y),
  model = mod.list,
  control = list(allow.degen = FALSE)
)
@

Figure \ref{fig:STS.fig.trend} shows the comparisons for the full level and trend estimates.  The EM algorithm would need a lower tolerance to get closer to the maximum likelihood parameter values.
<<label=Cs204_structTS-leveltrend>>=
fit2$kf <- MARSSkfss(fit2)
fit3$kf <- MARSSkfss(fit3)
fit4$kf <- MARSSkfss(fit4)
data.frame(
  StructTS = fit1$fitted[, 2], fit2 = fit2$kf$xtt[2, ],
  fit.bfgs = fit3$kf$xtt[2, ], fit.em = fit4$kf$xtt[2, ]
)[1:5, ]
@

\begin{figure}[htp]
\begin{center}
<<label=Cs205_structTS-leveltrend, echo=FALSE, keep.source=TRUE, fig=TRUE, echo=FALSE>>=
require(tidyr)
require(ggplot2)
df1 <- as.data.frame(fit1$fitted)
vars <- colnames(df1)
df2 <- as.data.frame(t(fit3$kf$xtt))
colnames(df2) <- vars
df3 <- as.data.frame(t(fit4$kf$xtt))
colnames(df3) <- vars
df1$model <- "StructTS"
df2$model <- "MARSS BFGS"
df3$model <- "MARSS EM"
df1$t <- as.vector(time(fit1$fitted))
df2$t <- df1$t
df3$t <- df1$t
df <- rbind(df1, df2, df3)
df <- df %>% pivot_longer(all_of(vars))
ggplot(df, aes(x = t, y = value, color = model, linetype = model, shape = model)) +
  geom_line() +
  geom_point() +
  facet_wrap(~name, scales = "free") +
  scale_linetype_manual("model", values = c(1, 1, 0)) +
  scale_shape_manual("model", values = c(NA, NA, 16))
@
\end{center}
\caption{Comparison of the level and trend estimates for the stochastic level plus trend model.}
\label{fig:STS.fig.trend}
\end{figure}

\subsection{Seasonal or BSM model}
\index{structural ts models!seasonal}

The seasonal model fit by \verb@StructTS()@ is the level plus trend model with an additional seasonal component $s_t$. The $m_t$ model is the same as for the level plus trend model.
\begin{equation}\label{eqn:struct.bsm.y}
y_t = m_t+s_t+v_t \text{ where } v_t \sim \N(0,\sigma^2_\epsilon)
\end{equation}
where
\begin{equation}\label{eqn:struct.bsm.s}
s_t = -s_{t-1}-\dots-s_{t-f+1}+v_t \text{ where } v_t \sim \N(0,\sigma^2_w)
\end{equation}
$f$ is the frequency of the seasonality. For quarterly data, $f=4$ and the $s_t$ model is
\begin{equation}\label{eqn:struct.bsm.s.qtr}
s_t = -s_{t-1}-s_{t-2}-s_{t-3}+v_t
\end{equation}

Written in matrix form, the model for a quarterly seasonality is the following. $s$ is the seasonality term while $s_1$ and $s_2$ are just keeping track of $s_{t-1}$ and $s_{t-2}$.
\begin{equation}\label{eqn:struct.bsm.y.matrix}
y_t = \begin{bmatrix}1&0&1&0&0\end{bmatrix}\begin{bmatrix}
m\\
n \\
s\\
s_1\\
s_2
\end{bmatrix}_t + v_t
\end{equation}
and the $x$ model is
\begin{equation}\label{eqn:struct.bsm.x.matrix}
\begin{bmatrix}
m\\
n \\
s\\
s_1\\
s_2
\end{bmatrix}_t = \begin{bmatrix}
1&1&0&0&0\\
0&1&0&0&0\\
0&0&-1&-1&-1\\
0&0&1&0&0\\
0&0&0&1&0
\end{bmatrix}\begin{bmatrix}
m\\
n\\
s\\
s_1\\
s_2
\end{bmatrix}_{t-1}+\ww_t
\end{equation}
where
\begin{equation}\ww_t \sim \MVN\left(0,\begin{bmatrix}
\sigma^2_\xi&0&0&0&0\\
0&\sigma^2_\zeta&0&0&0\\
0&0&\sigma^2_w&0&0\\
0&0&0&0&0\\
0&0&0&0&0
\end{bmatrix}\right) \\
\end{equation}
The initial conditions assumption is the following where again $s^2$ is the variance in the data ($\var(y)$):
\begin{equation}\label{eqn:struct.bsm.x0}
\begin{bmatrix}
m\\
n\\
s\\
s_1\\
s_2
\end{bmatrix}_0 \sim \MVN\left(\begin{bmatrix}
y_1\\0\\0\\0\\0
\end{bmatrix},
\begin{bmatrix}
10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2\\
10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2\\
10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2\\
10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2\\
10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2& 10^4 s^2
\end{bmatrix}\right)
\end{equation}

Let's see an example with the UK gas data set used in the help page \verb@?StructTS@.
<<label=Cs301_structTS-bsm, keep.source=TRUE>>=
y <- log10(UKgas)
fit1 <- StructTS(y, type = "BSM")
@
To make $\BB$ for \verb@MARSS()@, we write a little helper function. \verb@nf@ is the frequency.
<<label=Cs302_structTS-bsm>>=
makeB <- function(nf) {
  B <- matrix(0, nf + 1L, nf + 1L)
  B[1L:2L, 1L:2L] <- c(1, 0, 1, 1)
  B[3L, ] <- c(0, 0, rep(-1, nf - 1L))
  if (nf >= 3L) {
    ind <- 3:nf
    B[cbind(ind + 1L, ind)] <- 1
  }
  return(B)
}
@
Now we can fit with \verb@MARSS()@. 
<<label=Cs303_structTS-bsm, keep.source=TRUE, results=hide>>=
nf <- frequency(y)
vy <- var(y) / 100
B <- makeB(nf)
Z <- matrix(c(1, 0, 1, rep(0, nf - 2L)), 1, nf + 1)

Q <- ldiag(list("s2xi", "s2zeta", "s2w", 0, 0))
R <- matrix("s2eps")
V0 <- matrix(1e+06 * vy, nf + 1, nf + 1) + diag(1e-10, nf + 1)
mod.list <- list(
  x0 = matrix(c(y[1], rep(0, nf)), ncol = 1),
  U = "zero", A = "zero", tinitx = 0,
  Q = Q, R = R, V0 = V0, Z = Z, B = B
)
fit3 <- MARSS(as.vector(y), model = mod.list, method = "BFGS")
fit4 <- MARSS(as.vector(y),
  model = mod.list,
  control = list(allow.degen = FALSE)
)
fit4$kf <- MARSSkfss(fit4)
fit3$kf <- MARSSkfss(fit3)
@
Figure \ref{fig:STS.fig.bsm} shows the comparisons.

\begin{figure}[htp]
\begin{center}
<<label=Cs304_structTS-bsm, echo=FALSE, keep.source=TRUE, fig=TRUE, echo=FALSE>>=
require(tidyr)
require(ggplot2)
df1 <- as.data.frame(fit1$fitted)
vars <- colnames(df1)
df2 <- as.data.frame(t(fit3$kf$xtt)[, 1:3])
colnames(df2) <- vars
df3 <- as.data.frame(t(fit4$kf$xtt)[, 1:3])
colnames(df3) <- vars
df1$model <- "StructTS"
df2$model <- "MARSS BFGS"
df3$model <- "MARSS EM"
df1$t <- as.vector(time(fit1$fitted))
df1$Qtr <- as.vector(cycle(fit1$fitted))
df2$t <- df1$t
df2$Qtr <- df1$Qtr
df3$t <- df1$t
df3$Qtr <- df1$Qtr
df <- rbind(df1, df2, df3)
df <- subset(df, Qtr == 1) %>% pivot_longer(all_of(vars))
ggplot(df, aes(x = t, y = value, color = model, linetype = model, shape = model)) +
  geom_line() +
  geom_point() +
  facet_wrap(~name, scales = "free") +
  scale_linetype_manual("model", values = c(1, 1, 0)) +
  scale_shape_manual("model", values = c(NA, NA, 15))
@
\end{center}
\caption{Comparison of the level, trend and season estimates for the BSM model.}
\label{fig:STS.fig.bsm}
\end{figure}

\subsection{Forecasting}
\index{structural ts models!forecasting}\index{forecasting!structural ts models}\index{functions!predict}\index{functions!forecast}

Forecasts can be made with the \verb@predict()@ function or the \verb@forecast()@ function in the \{forecast\} package. Here we will use the BSM model fits to illustrate forecasting.


<<label=Cs401_forecast, results=hide>>=
y <- log10(UKgas)
fit1 <- StructTS(y, type = "BSM")

nf <- frequency(y)
vy <- var(y) / 100
B <- makeB(nf) # defined in the BSM section above
Z <- matrix(c(1, 0, 1, rep(0, nf - 2L)), 1, nf + 1)
V0 <- matrix(1e+06 * vy, nf + 1, nf + 1) + diag(1e-10, nf + 1)
mod.list <- list(
  x0 = matrix(c(y[1], rep(0, nf)), ncol = 1), U = "zero", A = "zero", tinitx = 0,
  Q = diag(c(fit1$coef[1:3], 0, 0)), R = matrix(fit1$coef[4]), V0 = V0, Z = Z, B = B
)
fit2 <- MARSS(as.vector(y), model = mod.list)
@
\verb@fit1@ and \verb@fit2@ are exactly the same since \verb@fit2@ used the \verb@fit1@ estimated parameters.

\verb@stats::predict.StructTS()@ is only for forecasting and takes the fit and \verb@n.ahead@ as arguments. It returns a list with the forecasts in \verb@pred@ and a ts object and their standard errors in \verb@se@. 
<<label=Cs402_forecast>>=
fr1 <- predict(fit1, n.ahead = 5)
fr1
@

The \verb@MARSS::predict.marssMLE()@ does both predicting within the data (similar to other \verb@predict@ methods) and will forecast if \verb@n.ahead@ is passed in. It returns a list with the predictions and forecasts in \verb@pred@ as a data frame in long form (suitable for \verb@ggplot@ calls). The standard errors and intervals (confidence or prediction) are included in \verb@pred@. The standard error is not printed but is in the \verb@pred@ data frame.\index{functions!predict}
<<label=Cs403_forecast>>=
fr2 <- predict(fit2, n.ahead = 5, interval = "prediction")
fr2
@

The estimates are the same. \verb@ft@ is the time steps associated with the forecast.
<<label=Cs404_forecast>>=
rbind(
  pred1 = fr1$pred, pred2 = fr2$pred$estimate[fr2$ft],
  se1 = fr1$se, se2 = fr2$pred$se[fr2$ft]
)
@

If we use the \verb@forecast::forecast.StructTS()@ function instead of \verb@predict()@, we get an object that can be plotted since the \{forecast\} package has a plot method for StructTS objects.
The \{MARSS\} package has a plot method for marssPredict objects returned by \verb@predict()@ and \verb@forecast()@ functions used with marssMLE objects. The \verb@forecast()@ function can be called with \verb@forecast::forecast()@ if you have the \{forecast\} package installed or \verb@forecast.marssMLE()@ if not.\index{forecasting!plotting}\index{plotting!predictions}

The plots from the StructTS and marssMLE objects are similar though they have slightly different formats.
\begin{figure}[htp]
\begin{center}
<<label=Cs405_forecast, keep.source=TRUE, fig=TRUE, fig.width=6, echo=FALSE>>=
fr1 <- forecast:::forecast.StructTS(fit1, h = 10)
fr2 <- forecast.marssMLE(fit2, h = 10)
p1 <- ggplot2::autoplot(fr1, include = 8)
p2 <- ggplot2::autoplot(fr2, include = 8)
gridExtra::grid.arrange(p1, p2, nrow = 1)
@
\end{center}
\caption{Comparison of the forecast plots.}
\label{fig:STS.fig.bsm.forecasts}
\end{figure}

\subsection{Fitted values}
\index{structural ts models!fitted}

\verb@fitted(x)@ applied to a StructTS object will return the expected value of $\XX_t$  (but only the first 3 states) conditioned on the data up to time $t$. It is returned as a ts or mts object depending if there is one state ("level") or multiple ("trend" or "BSM"). For the BSM model, plotting this will show the decomposed times series with the estimated level $m_t$, slope or trend $n_t$ and season $s_t$ terms.
\begin{figure}[htp]
\begin{center}
<<label=Cs501_fitted, fig=TRUE>>=
fitted1 <- fitted(fit1)
plot(fitted1)
@
\end{center}
\caption{Output from a plot of a fitted StructTS object.}
\label{fig:STS.fig.bsm.fitted}
\end{figure}
In the \{MARSS\} package, the estimated states conditioned on the data up to time $t$ is returned with \verb@tsSmooth(x, type="xtt")@.  The function \verb@fitted()@ in the \{MARSS\} package has the more typical meaning of \emph{fitted} for a statistical model (model prediction of $\yy$ or $\xx$).\index{functions!tsSmooth}\index{fitted values}\index{functions!fitted}
<<label=Cs502_fitted>>=
fitted2 <- tsSmooth(fit2, type = "xtt")
fitted2 <- subset(fitted2, .rownames %in% c("X1", "X2", "X3"))
@
This is a data frame in long-form which we can plot with ggplot.
\begin{figure}[htp]
\begin{center}
<<label=Cs503_fitted, fig=TRUE>>=
ggplot(fitted2, aes(x = t, y = .estimate)) +
  geom_line() +
  facet_wrap(~.rownames, ncol = 1, scale = "free_y")
@
\end{center}
\caption{Output from a plot of the states from the marssMLE object using tsSmooth() output.}
\label{fig:STS.fig.bsm.tsSmooth.marss}
\end{figure}
Alternatively we can use \verb@MARSSkfss(x)$xtt@ to return the state estimates as a matrix. Converting the matrix to a ts object makes it easier to plot.
\begin{figure}[htp]
\begin{center}
<<label=Cs504_fitted, fig=TRUE>>=
fitted3 <- MARSSkfss(fit2)$xtt
fitted3 <- ts(t(fitted3[1:3, ]))
plot(fitted3)
@
\end{center}
\caption{Output from a plot of states from a marssMLE object using MARSSkfss().}
\label{fig:STS.fig.bsm.kfss.marss}
\end{figure}

To output the model fitted value for $y$, we add the level and season states together if using \verb@StructTS()@ because for the BSM model, the model for $y_t$ is $m_t+s_t$. With MARSS, this would be output with \verb@fitted(x, type="ytt")@. Alternative we could add the $m_t$ and $s_t$ states from the MARSS output, but \verb@fitted(x, type="ytt")@ allows you to easily compute this for cases that are more complex with a non-identity $\ZZ$, non-zero $\aa$ and covariates $\dd$.\index{functions!fitted}
\begin{figure}[htp]
\begin{center}
<<label=Cs505_fitted, fig=TRUE, echo=FALSE>>=
fitted2 <- fitted(fit2, type = "ytt")
df2 <- data.frame(t = as.numeric(time(fitted1)), fitted = fitted2$.fitted, name = "MARSS")
df1 <- data.frame(t = df2$t, fitted = as.numeric(fitted1[, 1] + fitted1[, 3]), name = "StructTS")
df <- rbind(df1, df2)
df$y <- fitted2$y

ggplot(df) +
  geom_line(aes(x = t, y = fitted)) +
  geom_point(aes(x = t, y = y), col = "blue") +
  facet_wrap(~name, ncol = 1)
@
\end{center}
\caption{Data and model fitted values.}
\label{fig:STS.fig.bsm.fitted.marss}
\end{figure}

\subsection{Residuals}
\index{structural ts models!residuals}\index{residuals!StructTS}\index{functions!residuals}

\verb@residuals(x)@ applied to a StructTS object will return the difference between $y_t$ and the expected value of $Y_t$  conditioned on the data up to time $t$. This is $m_t+s_t$ for the BSM model. It is returned as a ts  object. The residuals are standardized, i.e., divided by the square root of the conditional variance of residuals (conditioned on the data). Note the conditional variance of the residuals is not \verb@var(resids)@; see \verb@?MARSSresiduals@ for a discussion of how the conditional variance of state-space residuals is computed.
<<label=Cs601_residuals>>=
resids1 <- residuals(fit1)
@
In the \{MARSS\} package, the residuals function will return the model residuals conditioned on all the data, data up to time $t-1$ or up to time $t$.\index{residuals!model} To replicate the behavior for StructTS objects, we need to use conditioning up to time $t$ which is \verb@type="tt"@.
<<label=Cs602_residuals>>=
resids2 <- residuals(fit2, type = "tt", standardization = "marginal")
@
This is a data frame in long-form which we can plot with \verb@ggplot()@.
\begin{figure}[htp]
\begin{center}
<<label=Cs603_residuals, fig=TRUE, echo=FALSE>>=
df2 <- data.frame(t = as.numeric(time(resids1)), resids = resids2$.std.resids, name = "MARSS")
df1 <- data.frame(t = df2$t, resids = as.numeric(resids1), name = "StructTS")
df3 <- data.frame(t = df2$t, resids = df1$resids - df2$resids, name = "difference")
df <- rbind(df1, df2, df3)

ggplot(df, aes(x = t, y = resids)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scale = "free_y") +
  ggtitle("Marginal standardized model residuals")
@
\end{center}
\caption{The initial difference in the residuals is due to the small value added to the diagonal of the initial condition variance-covariance matrix to allow MARSS to fit this model.}
\label{fig:STS.fig.bsm.residuals}
\end{figure}

\section{Multivariate models}
\index{structural ts models!multivariate}

The \{MARSS\} package allows one to fit multivariate versions of structural equation models. In this section, we will use the level plus trend model as the example, however the approaches work for all the structural models. The focus for this example will be estimating a changing trend using multiple time series.

\subsection{Multiple observations of same process}

The basic stochastic level plus trend model is
\begin{equation}
y_t = \begin{bmatrix}1&0\end{bmatrix}\begin{bmatrix}m\\n\end{bmatrix}_t + v_t
\end{equation}
\begin{equation}
\begin{bmatrix}m\\n\end{bmatrix}_t = 
\begin{bmatrix}1&1\\0&1\end{bmatrix}
\begin{bmatrix}m\\n\end{bmatrix}_{t-1}+\ww_t \text{ where } 
\ww_t \sim \MVN\left(0,\begin{bmatrix}\sigma^2_\xi&0\\0&\sigma^2_\zeta\end{bmatrix}\right) 
\end{equation}
Now imagine that there are three independent $y_t$ observations of the $m_t$ process. The observation model then becomes.
\begin{equation}
\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}_t = \begin{bmatrix}1&0\\1&0\\1&0\end{bmatrix}\begin{bmatrix}m\\n\end{bmatrix}_t + \vv_t \text{ where }
\vv_t \sim \MVN\left(0,
\begin{bmatrix}\sigma^2_\epsilon&0&0\\0&\sigma^2_\epsilon&0\\0&0&\sigma^2_\epsilon\end{bmatrix}\right) 
\end{equation}

The initial conditions assumption used in the \verb@StructTS()@ for this model is the following and we will keep that with the addition of a small amount to the diagonal to make the initial condition matrix positive definite.
\begin{equation}
\begin{bmatrix}m\\n\end{bmatrix}_0 \sim \MVN\left(
\begin{bmatrix}y_1\\0\end{bmatrix},
\begin{bmatrix}
10000 s^2 + 1e-10& 10000 s^2\\
10000 s^2& 10000 s^2 + 1e-10
\end{bmatrix}\right) \\
\end{equation}

\subsubsection{Simulated data}

We will model a simulated random walk with level and an abrupt trend change. We will generate observations that are the level + substantial error. We will also insert 50\% missing values into the $y$ to illustrate how the method will deal with missing values. Figure \ref{fig:STS.fig.UK.multi} shows the simulated data. We will fit to the points. The line is the true level.
<<label=Cs701_multivariate, keep.source=TRUE, results=hide>>=
set.seed(100)
TT <- 60
t <- 1:TT
q <- 0.01
r <- 0.01
trend <- 0.2 * sin((1:TT) / 4)
level <- cumsum(rnorm(TT, trend, sqrt(q)))

# Simulated data
n <- 5
miss.percent <- 0.5
ym <- matrix(1, n, 1) %*% level + matrix(rnorm(TT * n, 0, sqrt(r * 100)), n, TT)
ym[sample(n * TT, miss.percent * n * TT)] <- NA
@

\begin{figure}[htp]
\begin{center}
<<label=Cs702_multivariate, keep.source=TRUE, fig=TRUE, echo=FALSE>>=
par(mfrow = c(2, 1), mar = c(3, 3, 1, 1))
ylims <- c(min(ym, na.rm = TRUE), max(ym, na.rm = TRUE))
plot(t, trend, ylim = ylims, col = "red", type = "l")
lines(t, level, col = "black")
legend("topright", c("trend", "level"), lty = 1, col = c("red", "black"))
matplot(t, t(ym), pch = 1:n, ylab = "y", xlab = "", ylim = ylims, main = "bad data")
lines(t, level)
@
\end{center}
\caption{Observations (points) and truth (line).}
\label{fig:STS.fig.UK.multi}
\end{figure}

\subsubsection{Model set-up}

Set up the $\xx$ part of the equation for \verb@MARSS()@. This part does not change from the univariate case except that we will estimate the initial condition for the level and will treat the variance for the level as known (at the true value). It can be hard to separate the variances with large observation error. We will assume that we know something about the level process and it is the changing trend that we want to estimate.
<<label=Cs703_multivariate, keep.source=TRUE, results=hide>>=
vy <- var(y, na.rm = TRUE) / 100
mod.list.x <- list(
  x0 = matrix(list("x0", 0), nrow = 2), tinitx = 1,
  V0 = matrix(1e+06 * vy, 2, 2) + diag(1e-10, 2),
  Q = ldiag(list(q, "qt")),
  B = matrix(c(1, 0, 1, 1), 2, 2),
  U = "zero"
)
@
Next we set up the $\yy$ part of the equation. This is the part that changes. We will assume that the observations are independent with the same bias (i.e., expected value of each $y$ is the same). We will relax this assumption later.
<<label=Cs704_multivariate, keep.source=TRUE, results=hide>>=
mod.list.y <- list(
  A = "zero",
  R = "diagonal and equal"
)
@

\subsubsection{Fit model}

Estimate the level and trend from one of the simulated observation time series:
<<label=Cs705_multivariate, keep.source=TRUE, results=hide>>=
Z <- matrix(c(1, 0), 1, 2, byrow = TRUE)
mod.list <- c(mod.list.x, mod.list.y, list(Z = Z))
fitu <- MARSS(ym[1, ], model = mod.list, method = "BFGS", inits = list(x0 = 0))
@
Now estimate the parameters with all the time series.
<<label=Cs706_multivariate, keep.source=TRUE, results=hide>>=
Z <- matrix(c(1, 0), n, 2, byrow = TRUE)
mod.list <- c(mod.list.x, mod.list.y, list(Z = Z))
fitm <- MARSS(ym, model = mod.list, method = "BFGS", inits = list(x0 = 0))
@

\subsubsection{Compare trend estimate to truth}

Our objective is to estimate the level and trend states (Figure \ref{fig:STS.fig.UK.multi.fit}). In this example, the multivariate model is able to estimate the trend variance unlike when we fit to only one time series (the flat line in the trend plot). But if we increase the error added or the missing values, the ability to estimate the trend variance would disappear.
\begin{figure}[htp]
\begin{center}
<<label=Cs707_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
true <- data.frame(
  .rownames = rep(c("X1", "X2"), each = TT), t = t,
  .estimate = c(level, trend), .se = NA, name = "true"
)
statesu <- tsSmooth(fitu)
statesu$name <- "one bad"
statesm <- tsSmooth(fitm)
statesm$name <- "multiple bad"
df <- rbind(true, statesu, statesm)

ggplot(df, aes(x = t, y = .estimate, col = name)) +
  geom_line() +
  facet_wrap(~.rownames, scale = "free_y")
@
\end{center}
\caption{State estimates from the one bad time series versus multivariate bad time series.}
\label{fig:STS.fig.UK.multi.fit}
\end{figure}

\subsection{Covariate affects observations}
\index{structural ts models!covariates}

We can add a known covariate that affects the observations. In this case, it is a step function representing a before-after effect. We will have it affect only the first few observation time series and the effect will be different for each series (Figure \ref{fig:STS.fig.UK.multi.2}).
\begin{figure}[htp]
\begin{center}
<<label=Cs708_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
par(mfrow = c(2, 1), mar = c(3, 3, 1, 1))
covariate <- matrix(c(rep(0, TT - 10), rep(1, 10)), nrow = 1)
ymc <- ym
D <- matrix(c(-1, -1, 0, 1, 1), ncol = 1)
ymc <- ym + D %*% covariate
matplot(t, t(ymc), pch = 1:n, ylab = "y", xlab = "", main = "data")
lines(level)
plot(t, covariate[1, ], col = "blue", lty = 2, type = "l", main = "covariate")
@
\end{center}
\caption{Observations (points) of the true data (line) with covariate effect plus error and missing values added. It affects some time series positively and others negatively.}
\label{fig:STS.fig.UK.multi.2}
\end{figure}

We fit by passing in the covariate into \verb@d@ (because it affects the observations not the process). The estimated versus true effects are shown in Figure \ref{fig:STS.fig.UK.multi.D.est} and the estimated trend is in Figure \ref{fig:STS.fig.UK.multi.fit.2}.
<<label=Cs709_multivariate, keep.source=TRUE, results=hide>>=
Z <- matrix(c(1, 0), n, 2, byrow = TRUE)
mod.list <- c(mod.list.x, mod.list.y, list(Z = Z, d = covariate))
fitmc <- MARSS(ymc, model = mod.list, method = "BFGS", inits = list(x0 = 0))
@

\begin{figure}[htp]
\begin{center}
<<label=Cs710_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
dvals <- data.frame(
  x = paste0("y", 1:n),
  val = c(D, coef(fitmc, type = "matrix")$D),
  name = rep(c("true", "estimate"), each = n)
)
ggplot(dvals, aes(x = x, y = val, col = name)) +
  geom_point() +
  xlab("observation series") +
  ylab("D estimate") +
  ggtitle("D true and estimated values")
@
\end{center}
\caption{Estimate of the effect of the covariate.}
\label{fig:STS.fig.UK.multi.D.est}
\end{figure}

\begin{figure}[htp]
\begin{center}
<<label=Cs711_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
statesmc <- tsSmooth(fitmc)
statesmc$name <- "multiple w covariate"
df <- rbind(true, statesu, statesm, statesmc)

ggplot(df, aes(x = t, y = .estimate, col = name)) +
  geom_line() +
  facet_wrap(~.rownames, scale = "free_y")
@
\end{center}
\caption{State estimates from the univariate good data, multivariate bad data, and multivariate with a covariate.}
\label{fig:STS.fig.UK.multi.fit.2}
\end{figure}

\subsection{Observations with bias and different errors}

Our observations may have different (unknown) levels of observation error and be biased relative to each other (Figure \ref{fig:STS.fig.UK.multi.3}).
\begin{figure}[htp]
\begin{center}
<<label=Cs712_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
r2 <- r * c(100, 10, 10, 200, 400)
a <- runif(n, -1, 1)
err <- rnorm(n * TT, mean = rep(a, each = TT), sd = rep(sqrt(r2), each = TT))
ym2 <- matrix(1, nrow = n) %*% level + matrix(err, nrow = n, byrow = TRUE)
ym2[sample(n * TT, miss.percent * n * TT)] <- NA
matplot(t, t(ym2), pch = 1:n, ylab = "y", xlab = "", main = "data with different error and bias")
lines(level)
@
\end{center}
\caption{Observations (points) with differing error and bias added.}
\label{fig:STS.fig.UK.multi.3}
\end{figure}

We fit by changing the $\RR$ and $\aa$ specifications. The $\RR$ estimates are shown in Figure \ref{fig:STS.fig.UK.multi.R.est}.
<<label=Cs713_multivariate, keep.source=TRUE, results=hide>>=
Z <- matrix(c(1, 0), n, 2, byrow = TRUE)
mod.list <- c(mod.list.x, list(Z = Z, R = "diagonal and unequal", A = "scaling"))
fitm2 <- MARSS(ym2, model = mod.list, method = "BFGS", inits = list(x0 = 0))
@

\begin{figure}[htp]
\begin{center}
<<label=Cs714_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
rvals <- data.frame(
  x = paste0("y", 1:n),
  val = c(r2, coef(fitm2)$R),
  name = rep(c("true", "estimate"), each = n)
)
ggplot(rvals, aes(x = x, y = val, col = name)) +
  geom_point() +
  xlab("observation series") +
  ylab("R variance estimate") +
  ggtitle("R true and estimated values")
@
\end{center}
\caption{Estimate of the observation variances.}
\label{fig:STS.fig.UK.multi.R.est}
\end{figure}

The level will be scaled up or down to fit the first observation time series. It has to set one of the $\aa$ to 0 and by default sets the first one to zero. The estimates are in Figure \ref{fig:STS.fig.UK.multi.fit.4}. 
\begin{figure}[htp]
\begin{center}
<<label=Cs715_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE>>=
statesm2 <- tsSmooth(fitm2)
statesm2$name <- "multiple w different Rs"
df <- rbind(true, statesu, statesm, statesmc, statesm2)

ggplot(df, aes(x = t, y = .estimate, col = name)) +
  geom_line() +
  facet_wrap(~.rownames, scale = "free_y")
@
\end{center}
\caption{State estimates. The level for the model with bias will be shifted up or down. This is not an error but a feature of having to scale to one of the time series and by default, the first is chosen.}
\label{fig:STS.fig.UK.multi.fit.4}
\end{figure}

\subsection{Indepenent realizations of the same process}

In the last section, we had multiple observations of the same process. We can also have multiple realizations of independent processes with the same variance values. In this example, we assume that the level is an independent observation of a shared trend.

\begin{equation}
\begin{bmatrix}m_1\\m_2\\n\end{bmatrix}_t = 
\begin{bmatrix}1&0&1\\0&1&1\\0&0&1\end{bmatrix}
\begin{bmatrix}m_1\\m_2\\n\end{bmatrix}_{t-1}+\ww_t \text{ where } 
\ww_t \sim \MVN\left(0,
\begin{bmatrix}
\sigma^2_\xi&0&0\\
0&\sigma^2_\xi&0\\
0&0&\sigma^2_\zeta\end{bmatrix}\right) 
\end{equation}

Each is observed by an independent $y_t$ observation.
\begin{equation}
\begin{bmatrix}y_1\\y_2\end{bmatrix}_t = \begin{bmatrix}1&0&0\\0&1&0\end{bmatrix}
\begin{bmatrix}m_1\\m_2\\n\end{bmatrix}_t + \vv_t \text{ where }
\vv_t \sim \MVN\left(0,
\begin{bmatrix}\sigma^2_\epsilon&0\\0&\sigma^2_\epsilon\end{bmatrix}\right) 
\end{equation}


\subsubsection{Simulated data}

We simulate data as in the last section but simulate two levels and trends. Figure \ref{fig:STS.fig.multi.two} shows the simulated data.
<<label=Cs716_multivariate, keep.source=TRUE, results=hide>>=
set.seed(100)
TT <- 60
t <- 1:TT
q <- 0.5
qt <- 0.01
r <- 0.1
b <- 0.5
trend <- 0.2 * sin((1:TT) / 4)
level1 <- cumsum(rnorm(TT, trend, sqrt(q)))
level2 <- cumsum(rnorm(TT, trend, sqrt(q)))

# Simulated data
ym <- rbind(level1, level2) + matrix(rnorm(TT * 2, 0, sqrt(r)), 2, TT)
@

\begin{figure}[htp]
\begin{center}
<<label=Cs717_multivariate, keep.source=TRUE, fig=TRUE, echo=FALSE>>=
par(mfrow = c(2, 1), mar = c(3, 3, 1, 1))
ylims <- c(min(ym, na.rm = TRUE), max(ym, na.rm = TRUE))
plot(t, ym[1, ], ylim = ylims, type = "p")
lines(t, level1, col = "black")
plot(t, ym[2, ], ylim = ylims, type = "p")
lines(t, level2, col = "black")
@
\end{center}
\caption{Observations (points) and truth (line). The levels (line) have the same variance but are independent. The level processes share a trend, i.e., $m_{1,t} = m_{1,t-1}+n_{t-1}$ and $m_{2,t} = m_{2,t-1}+n_{t-1}$.}
\label{fig:STS.fig.multi.two}
\end{figure}

\subsubsection{Fit models}

Estimate the level and trend from each observation time series alone:
<<label=Cs718_multivariate, keep.source=TRUE, results=hide>>=
vy <- var(y, na.rm = TRUE) / 100
Z <- matrix(c(1, 0), 1, 2)
mod.list.x <- list(
  x0 = matrix(list("x0", 0), nrow = 2), tinitx = 1,
  V0 = matrix(1e+06 * vy, 2, 2) + diag(1e-10, 2),
  Q = ldiag(list(q, "qt")),
  B = matrix(c(1, 0, 1, 1), 2, 2),
  U = "zero"
)
mod.list <- c(mod.list.x, mod.list.y, list(Z = Z))
fitm1 <- MARSS(ym[1, ], model = mod.list, method = "BFGS", inits = list(x0 = 0))
fitm2 <- MARSS(ym[2, ], model = mod.list, method = "BFGS", inits = list(x0 = 0))
@

Estimate the level and trend from the two simulated observation time series together:
<<label=Cs719_multivariate, keep.source=TRUE, results=hide>>=
Z <- matrix(c(1, 0, 0, 0, 1, 0), 2, 3, byrow = TRUE)
m <- 3
mod.list.x <- list(
  x0 = matrix(list("x0.1", "x0.2", 0), nrow = m), tinitx = 1,
  V0 = matrix(1e+06 * vy, m, m) + diag(1e-10, m),
  Q = ldiag(list("q", "q", "qt")),
  B = matrix(c(1, 0, 1, 0, 1, 1, 0, 0, 1), m, m, byrow = TRUE),
  U = "zero"
)
mod.list <- c(mod.list.x, mod.list.y, list(Z = Z))
fitm3 <- MARSS(ym, model = mod.list, method = "BFGS", inits = list(x0 = 0))
@

\subsubsection{Compare state estimates}

The two time series alone are not able to estimate the trend. Both put all the variation in the data into the level state.

\begin{figure}[htp]
\begin{center}
<<label=Cs720_multivariate, keep.source=TRUE, results=hide, fig=TRUE, echo=FALSE, fig.height=6>>=
true <- data.frame(
  .rownames = rep(c("level 1", "level 2", "trend"), each = TT), t = t,
  .estimate = c(level1, level2, trend), .se = NA, name = "true"
)
statesm1 <- tsSmooth(fitm1)
statesm1$name <- "ts 1 alone"
statesm1$.rownames[statesm1$.rownames == "X2"] <- "trend"
statesm1$.rownames[statesm1$.rownames == "X1"] <- "level 1"
statesm2 <- tsSmooth(fitm2)
statesm2$name <- "ts 2 alone"
statesm2$.rownames[statesm2$.rownames == "X2"] <- "trend"
statesm2$.rownames[statesm2$.rownames == "X1"] <- "level 2"
statesm3 <- tsSmooth(fitm3)
statesm3$name <- "ts 1 & 2 together"
statesm3$.rownames[statesm3$.rownames == "X3"] <- "trend"
statesm3$.rownames[statesm3$.rownames == "X1"] <- "level 1"
statesm3$.rownames[statesm3$.rownames == "X2"] <- "level 2"
df <- rbind(true, statesm1, statesm2, statesm3)

ggplot(df, aes(x = t, y = .estimate, col = name)) +
  geom_line() +
  facet_wrap(~.rownames, scale = "free_y", ncol = 1)
@
\end{center}
\caption{Common trend and separate level estimates.}
\label{fig:STS.fig.indep.fit}
\end{figure}

\section{Summary}

This chapter illustrates how to fit the structural equation models fit by the \verb@StructTS()@ function in base R. The initial conditions used in that function were used here, however that should not be assumed to be the best choice. The default behavior for in the \{MARSS\} package is to treat $\xx_0$ as an estimated parameter with initial conditions variance matrix set to all 0. These models can be a challenge for the EM algorithm and for most examples the BFGS algorithm was used. If using EM, the algorithm will need to be run longer to achieve the maximum likelihood. 

The \{MARSS\} package allows you to fit multivariate versions of these models with a flexible data structure and flexible relationships between the data. It allows you to include covariates and model intervention effects. You can model multiple observations of the same process or different observations of independent processes that share some or all parameter values. You can also model cases where the trend (or seasonality) is shared across processes but not the levels. The ability to use multiple data sets can improve estimation and allow you to estimate an underlying process which might remain hidden if one one data set is used for estimation.

<<reset, echo=FALSE>>=
options(prompt = "> ", continue = " +", width = 120)
@
