\SweaveOpts{keep.source=TRUE, prefix.string=./figures/KFAS-, eps=FALSE, split=TRUE, height=3, out.height='3in'}
<<RUNFIRST, echo=FALSE, include.source=FALSE>>=
options(prompt = " ", continue = " ", width = 60)
@
<<label=Cs01_required-libraries, echo=FALSE>>=
library(MARSS)
library(KFAS)
@
\chapter{Comparison to KFAS Package}
\label{chap:kfas}
\chaptermark{KFAS}
%Add footnote with instructions for getting code
\blfootnote{Type \texttt{RShowDoc("Chapter\_KFAS.R",package="MARSS")} at the R command line to open a file with all the code for the examples in this chapter.}

The MARSS package uses the Kalman filter and smoother in the KFAS package \citep{Helske2017} which implements the more stable filter and smoother algorithm by \citet{KoopmanDurbin2000, DurbinKoopman2012}. The KFAS package also provides fitting of MARSS models in the general exponential class, i.e. with non-Gaussian errors. \verb@MARSSkfas(..., return.kfas.model=TRUE)@ will return the KFAS model object which can then be used in KFAS functions. This chapter shows the KFAS versus MARSS functions for fitting state-space models using the examples in \verb@?KFAS@. 

\section{Structural time series model}
\index{structural ts models!univariate}

\subsection{Fitting models}

This is the Nile River example in \citet{DurbinKoopman2012} and shown in Chapter \ref{chap:CSstrucbreak} on structural breaks. This model is
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
x_t = x_{t-1}+w_t \text{ where } w_t \sim \N(0,q) \\
y_t = x_t+v_t \text{ where } v_t \sim \N(0,r)  \\
\end{gathered}   
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~
With all the MARSS parameters shown, the model is:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
x_t = 1 \times x_{t-1} + 0 + w_t    \text{ where } w_t \sim \N(0,q) \\
y_t = 1 \times x_t + 0 + v_t \text{ where } v_t \sim \N(0,r)  \\
\end{gathered}   
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~

\verb@KFAS::SSModel@ sets up the KFAS model which will be passed to the fitting functions. \verb@KFAS::SSMtrend(degree = 1)@ designates a local level model. \verb@KFAS::fitSSM@ fits the KFAS model. 
<<Cs1_nile-model, results="hide">>=
model_Nile <- SSModel(Nile ~ SSMtrend(degree = 1, Q = list(matrix(NA))), H = matrix(NA))
fit_kfas_default <- fitSSM(model_Nile, c(log(var(Nile)), log(var(Nile))), method = "BFGS")
@

KFAS uses a stochastic prior on the initial condition and the fitting function does not estimate $x_0$. By default, a diffuse prior on $x_0$ is used. The default method for MARSS, in contrast, is to estimate $x_0$ as a parameter and fix $V_0$ (the variance of $x_0$) at 0. This will lead to small differences between the fits. The EM algorithm in MARSS does implement a true diffuse prior but we can specify a stochastic prior to mimic a KFAS fit.

We will set a stochastic prior on $x_1$ with a mean of 0 and variance of 10 by changing \verb@P1@, \verb@P1inf@, and \verb@a1@. Setting \verb@P1inf@ to 0, turns off the diffuse prior. 
<<Cs2_kfas-stoch-prior, results="hide">>=
model_Nile_stoch <- model_Nile
model_Nile_stoch$a1[1,1] <- 0
model_Nile_stoch$P1[1,1] <- 10000
model_Nile_stoch$P1inf[1,1] <- 0
fit_kfas_stoch <- fitSSM(model_Nile_stoch, c(log(var(Nile)), log(var(Nile))), method = "BFGS")
@


With MARSS, the model is specified as:
<<Cs03_marss-nile-mod>>=
mod.nile <- list(
  Z = matrix(1), A = matrix(0), R = matrix("r"),
  B = matrix(1), U = matrix(0), Q = matrix("q"),
  tinitx = 1
)
@
The default initial conditions for MARSS is to estimate $x_1$ as a parameter (and set $V_1$ to zero). This default behavior prevents prior information about the covariance structure of the states from affecting the estimates, though for some models, the initial conditions estimation is not well defined (in which case setting a stochastic prior is helpful).

We will fit with the EM and BFGS algorithm in MARSS. We will start the BFGS algorithm at the same initial conditions used in our KFAS fitting call, although this isn't quite the same because MARSS and KFAS are using different approaches to ensure that the variances stay positive-definite during the BFGS maximization steps.
<<Cs04_marss-fit, eval=TRUE>>=
dat <- t(as.matrix(Nile))
rownames(dat) <- "Nile"
fit_em_default <- MARSS(dat, model = mod.nile, silent = TRUE)
inits=list(Q=matrix(var(Nile)), R=matrix(var(Nile)))
fit_bfgs_default <- MARSS(dat, model = mod.nile, inits = inits, method="BFGS", silent = TRUE)
@

We will also fit a stochastic prior so that we can compare more directly to the same model fit with KFAS.
<<Cs05_marss-stoch-prior>>=
mod.nile$x0 <- matrix(0)
mod.nile$V0 <- matrix(10000)
fit_em_stoch <- MARSS(dat, model = mod.nile, silent = TRUE)
fit_bfgs_stoch <- MARSS(dat, model = mod.nile, inits = inits, method="BFGS", silent = TRUE)
@

\verb@MARSSkfas()@ will return the SSModel object that is passed to \verb@KFAS::KFS()@ (internally in the MARSS functions). MARSS does not use \verb@KFAS::fitSSM()@ but it does use \verb@KFAS::KFS()@ for the filter, smoother and log-likelihood. The SSModel used inside MARSS looks different than \verb@model_Nile@ because the $\aa$ term is in \verb@T@ and the $\uu$ term is in \verb@T@. We can set \verb@Q@ and \verb@H@ to NA to estimate those values. The results are the same as for \verb@fit_kfas_stoch@.
<<Cs06_marss-kfas-model-fit>>=
marss_kfas_model <- MARSSkfas(fit_em_stoch, return.kfas.model=TRUE, return.lag.one=FALSE)$kfas.model
marss_kfas_model$Q[1,1,1] <- NA
marss_kfas_model$H[1,1,1] <- NA
fit_marss_kfas <- fitSSM(marss_kfas_model, c(log(var(Nile)), log(var(Nile))), method = "BFGS")
@

The KFAS parameter estimates are in \verb@$optim.out$par@ and the variances are logged. The negative log-likelihood is in \verb@$optim.out$value@. Here is the comparison of all the models. Note that the default KFAS model is fundamentally different than the default MARSS model because the former uses a diffuse prior while the later is estimating $x_1$ as a parameter.
<<Cs06_compare-fits>>=
vals <- rbind(
  c( exp(fit_kfas_default$optim.out$par), -1*fit_kfas_default$optim.out$value),
  c(coef(fit_em_default)$Q, coef(fit_em_default)$R, logLik(fit_em_default)),
  c(coef(fit_bfgs_default)$Q, coef(fit_bfgs_default)$R, logLik(fit_bfgs_default)),
  c( exp(fit_kfas_stoch$optim.out$par), -1*fit_kfas_stoch$optim.out$value),
  c(coef(fit_em_stoch)$Q, coef(fit_em_stoch)$R, logLik(fit_em_stoch)),
  c(coef(fit_bfgs_stoch)$Q, coef(fit_bfgs_stoch)$R, logLik(fit_bfgs_stoch)),
  c( exp(fit_marss_kfas$optim.out$par), -1*fit_marss_kfas$optim.out$value)
)
rownames(vals) <- c("KFAS default", "MARSS em default", "MARSS bfgs default", 
                    "KFAS stoch", "MARSS em stoch", "MARSS bfgs stoch", "KFAS w marss kfas model")
colnames(vals) <- c("Q", "R", "logLik")
vals
@


\subsection{State filtering and smoothing}
\index{Kalman filter and smoother!KFAS}

For this section, we will compare filter and smoother output from the two packages. For this we need identical models. 
<<>>=
fit_kfas <- fit_kfas_stoch
fit_marss <- fit_em_stoch
fit_marss$par$Q[1,1] <- exp(fit_kfas$optim.out$par)[1]
fit_marss$par$R[1,1] <- exp(fit_kfas$optim.out$par)[2]
@
The Kalman filter and smoother function in KFAS is \verb@KFS()@. This returns a variety of output:
<<Cs1_nile-model, results="hide">>=
kf_kfas <- KFS(fit_kfas$model, filtering = "state", smoothing = "state", simplify = FALSE)
@
The analogous function in MARSS is \verb@MARSSkfas()@. It uses \verb@KFAS::KFS()@ for the implementation of the Koopman and Durbin Kalman filter and smoother algorithm \citep{KoopmanDurbin2000} but transforms the state-space model passed into that function in order to get a variety of variables needed for the EM algorithm, specifically the lag-1 smoother values. 
<<Cs1_nile-model, results="hide">>=
kf_marss <- MARSSkfss(fit_marss)
@

The terminology of the filter/smoother variables is different between \verb@MARSSkfas()@ and \verb@KFAS::KFS()@. Note MARSS also includes \verb@MARSSkfss()@, which is the classic (less stable) Kalman filter and smoother; see for example the chapter on the Kalman filter in \citet{ShumwayStoffer2006}.
<<Cs1_nile-model, results="hide">>=
names(kf_kfas)
names(kf_marss)
@

The MARSS semantics are first letter x or y process, second letter time (usually t), and third letter the time conditioning. So \verb@xtT@ means the estimate of the $\xx$ process conditioned on all the data while \verb@xtt1@ means the estimate of the $\xx$ process conditioned on the data 1 to $t-1$. 
<<Cs10_n, echo=FALSE>>=
n <- 10
@
\begin{itemize}
\item \verb@kf_kfas$a@ is \verb@kf_marss$xtt1@. This is the expected value of $X_t$ conditioned on the data up to time $t-1$. \verb@kf_kfas$att@ is \verb@kf_marss$xtt@. This is the expected value of $X_t$ conditioned on the data up to time $t$.
<<>>=
cbind(a=kf_kfas$a[1:n], xtt1=kf_marss$xtt1[1:n], att=kf_kfas$att[1:n], xtt=kf_marss$xtt[1:n])
@
\item \verb@kf_kfas$alphahat@ is \verb@kf_marss$xtT@. This is the expected value of $X_t$ conditioned on all the data.
<<>>=
cbind(kf_kfas$alphahat[1:n], kf_marss$xtT[1:n])
@
\item \verb@kf_kfas$v@ is \verb@kf_marss$Innov@. These are the innovations or one-step-ahead model residuals. \verb@kf_kfas$F@ is \verb@kf_marss$Sigma@. These are variance of innovations.
<<>>=
cbind(v=kf_kfas$v[1:n], Innov=kf_marss$Innov[1:n], F=kf_kfas$F[1:n], Sigma=kf_marss$Sigma[1:n])
@
\item \verb@kf_kfas$P@ is \verb@kf_marss$Vtt1@. This is the conditional variance of $X_t$ conditioned on the data up to time $t-1$. \verb@kf_kfas$Ptt@ is \verb@kf_marss$Vtt@. This is the conditional variance of $X_t$ conditioned on the data up to time $t$.
<<>>=
cbind(P=kf_kfas$P[1:n], Vtt1=kf_marss$Vtt1[1:n], Ptt=kf_kfas$Ptt[1:n], Vtt=kf_marss$Vtt[1:n])
@
\item "r", "r0", "r1", "N", "N0", "N1" and "N2" are specific to the Koopman and Durbin algorithm and are not returned by \verb@MARSSkfss()@ though you could get them by using the SSModel object returned by \verb@MARSSkfas()@.
\end{itemize}

\subsection{Observation filtering and smoothing}
\index{Observation filtering and smoothing!KFAS}

The \verb@KFAS::KFS()@ will also return the observation or signal filtered and smoothed variables. 
<<Cs1_nile-model, results="hide">>=
f_kfas <- KFS(fit_kfas$model, filtering = "signal", smoothing = "signal", simplify = FALSE)
@
The analogous function in MARSS is \verb@fitted(..., type=...)@ with type "ytT", "ytt" and "ytt1". 
<<Cs1_nile-model, results="hide">>=
f_marss <- MARSShatyt(fit_marss, only.kem=FALSE)
ey_marss <- MARSShatyt(fit_marss, only.kem=FALSE)
@

The function \verb@MARSShatyt()@ is the counterpart to \verb@MARSSkfss()@ and returns the equivalent values but for the observation equation. This is very different than what \verb@KFS()@ returns for the signal. The expected value of $\YY_t$ conditioned on $\YY_t = \yy_t$ is simply $\yy_t$ and the covariance of $\YY_t$ and $\XX_t$ conditioned on $\YY_t=\yy_t$ would be 0. These values are not this when there are missing values and these expectations are crucial to the general missing values EM algorithm.

The terminology of the filter/smoother variables is again different.
<<Cs1_nile-model, results="hide">>=
names(ey_kfas)
names(ey_marss)
@

The MARSS semantics are first letter x or y process, second letter time (usually t), and third letter the time conditioning. So \verb@xtT@ means the estimate of the $\xx$ process conditioned on all the data while \verb@xtt1@ means the estimate of the $\xx$ process conditioned on the data 1 to $t-1$. 
<<Cs10_n, echo=FALSE>>=
n <- 10
@
\begin{itemize}
\item \verb@f_kfas$m@ is the one-step ahead prediction of $\yy_t$. In MARSS, this is returned by \verb@fitted(fit_marss, type="ytt1")$.fitted@. "ytt1" means the expected value of $\y_t$ conditioned on the data up to time $t-1$. 
<<>>=
ytt1_fit <- fitted(fit_marss, type="ytt1")$.fitted
ytt1_hatyt <- MARSShatyt(fit_marss)$ytt1
cbind(m=f_kfas$m[1:n], fitted=ytt1_fit[1:n], MARSShatyt=ytt1_hatyt[1:n])
@
\item \verb@kf_kfas$P_mu@ is is the variance of the expected value of $\YY_t$ conditioned on the data 1 to $t-1$. In MARSS, this is returned in the standard errors returned by \verb@fitted()@ with interval="confidence". Confidence intervals are the intervals on the expected value (i.e. mean). \verb@MARSShatyt(fit_marss)$var.Eytt1@ returns the same values but only because with conditioning 1 to $t-1$ there is no data at time $t$. \verb@fitted()@ is the correct function to use if you want the model fitted values, such as "m", "muhat" returned by \verb@KFS()@. You can output the values as matrices instead of a data frame if you need the variance-covariance matrices not just standard errors.
<<>>=
var.Eytt1_fit <- fitted(fit_marss, type="ytt1", interval="confidence")$.se^2
var.Eytt1_hatyt <- MARSShatyt(fit_marss, only.kem=FALSE)$var.Eytt1
cbind(P_mu=kf_kfas$P_mu[1:n], fitted=var.Eytt1_fit[1:n], MARSShatyt=var.Eytt1_hatyt[1:n])
@
\item \verb@f_kfas$muhat@ is the smoothed prediction of $\yy_t$. It is the expected value of $\ZZ \XX_t+\aa$ conditioned on the data up to time $T$, i.e. all the data. In MARSS, this is returned by \verb@fitted(fit_marss, type="ytT")$.fitted@. \verb@MARSShatyt(fit_marss)$ytT@ does not return this. \verb@MARSShatyt()@ returns the expected value of $\YY_t$ conditioned on the data up to time $T$, i.e. all the data, which if there are no missing data is simply the observed data.
<<>>=
ytT_fit <- fitted(fit_marss, type="ytT")$.fitted
ytT_hatyt <- MARSShatyt(fit_marss)$ytT
cbind(a=f_kfas$muhat[1:n], fitted=ytt1_fit[1:n], MARSShatyt=ytt1_hatyt[1:n], Nile=Nile[1:n])
@
\item \verb@f_kfas$V_mu@ is is the variance of the expected value of $Y_t$ conditioned on all the data. In MARSS, this is returned in the standard errors returned by \verb@fitted()@ with interval="confidence". Again, \verb@var.Eytt1@ returned by \verb@MARSShatyt()@ is not this because it returns the variance of the expected value of $\YY_t|\YY_t=\yy_t$ not $\ZZ \XX_t + \aa|\YY_t=\yy_t$. The latter is the model prediction. In this case, there are no missing values so $\YY_t|\YY_t=\yy_t$ is 0.
<<>>=
var.EytT_fit <- fitted(fit_marss, type="ytT", interval="confidence")$.se^2
var.EytT_hatyt <- MARSShatyt(fit_marss, only.kem=FALSE)$var.EytT
cbind(V_mu=f_kfas$V_mu[1:n], fitted=var.EytT_fit[1:n], MARSShatyt=var.EytT_hatyt[1:n])
@
\end{itemize}


# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf_Nile <- predict(model_Nile, interval = "confidence", level = 0.9)
pred_Nile <- predict(model_Nile, interval = "prediction", level = 0.9)

ts.plot(cbind(Nile, pred_Nile, conf_Nile[, -1]), col = c(1:2, 3, 3, 4, 4),
        ylab = "Predicted Annual flow", main = "River Nile")


# Missing observations, using the same parameter estimates

NileNA <- Nile
NileNA[c(21:40, 61:80)] <- NA
model_NileNA <- SSModel(NileNA ~ SSMtrend(1, Q = list(model_Nile$Q)),
H = model_Nile$H)

out_NileNA <- KFS(model_NileNA, "mean", "mean")

# Filtered and smoothed states
ts.plot(NileNA, fitted(out_NileNA, filtered = TRUE), fitted(out_NileNA),
  col = 1:3, ylab = "Predicted Annual flow",
  main = "River Nile")


# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details

data("GlobalTemp")

model_temp <- SSModel(GlobalTemp ~ SSMtrend(1, Q = NA, type = "common"),
  H = matrix(NA, 2, 2))

# Estimating the variance parameters
inits <- chol(cov(GlobalTemp))[c(1, 4, 3)]
inits[1:2] <- log(inits[1:2])
fit_temp <- fitSSM(model_temp, c(0.5*log(.1), inits), method = "BFGS")

out_temp <- KFS(fit_temp$model)

ts.plot(cbind(model_temp$y, coef(out_temp)), col = 1:3)
legend("bottomright",
  legend = c(colnames(GlobalTemp), "Smoothed signal"), col = 1:3, lty = 1)


# }
# NOT RUN {
# Seatbelts data
# See Durbin and Koopman (2012)

model_drivers <- SSModel(log(drivers) ~ SSMtrend(1, Q = list(NA))+
   SSMseasonal(period = 12, sea.type = "trigonometric", Q = NA) +
   log(PetrolPrice) + law, data = Seatbelts, H = NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:

# option 1:
ownupdatefn <- function(pars, model){
  model$H[] <- exp(pars[1])
  diag(model$Q[, , 1]) <- exp(c(pars[2], rep(pars[3], 11)))
  model #for optim, replace this with -logLik(model) and call optim directly
}

fit_drivers <- fitSSM(model_drivers,
  log(c(var(log(Seatbelts[, "drivers"])), 0.001, 0.0001)),
  ownupdatefn, method = "BFGS")

out_drivers <- KFS(fit_drivers$model, smoothing = c("state", "mean"))
out_drivers
ts.plot(out_drivers$model$y, fitted(out_drivers), lty = 1:2, col = 1:2,
  main = "Observations and smoothed signal with and without seasonal component")
lines(signal(out_drivers, states = c("regression", "trend"))$signal,
  col = 4, lty = 1)
legend("bottomleft", col = c(1, 2, 4), lty = c(1, 2, 1),
  legend = c("Observations", "Smoothed signal", "Smoothed level"))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component

model_drivers2 <- SSModel(log(cbind(front, rear)) ~ -1 +
    log(PetrolPrice) + log(kms) +
    SSMregression(~law, data = Seatbelts, index = 1) +
    SSMcustom(Z = diag(2), T = diag(2), R = matrix(1, 2, 1),
      Q = matrix(1), P1inf = diag(2)) +
    SSMseasonal(period = 12, sea.type = "trigonometric"),
  data = Seatbelts, H = matrix(NA, 2, 2))

# An alternative way for defining the rank deficient trend component:

# model_drivers2 <- SSModel(log(cbind(front, rear)) ~ -1 +
#     log(PetrolPrice) + log(kms) +
#     SSMregression(~law, data = Seatbelts, index = 1) +
#     SSMtrend(degree = 1, Q = list(matrix(0, 2, 2))) +
#     SSMseasonal(period = 12, sea.type = "trigonometric"),
#   data = Seatbelts, H = matrix(NA, 2, 2))
#
# Modify model manually:
# model_drivers2$Q <- array(1, c(1, 1, 1))
# model_drivers2$R <- model_drivers2$R[, -2, , drop = FALSE]
# attr(model_drivers2, "k") <- 1L
# attr(model_drivers2, "eta_types") <- attr(model_drivers2, "eta_types")[1]


likfn <- function(pars, model, estimate = TRUE){
  diag(model$H[, , 1]) <- exp(0.5 * pars[1:2])
  model$H[1, 2, 1] <- model$H[2, 1, 1] <-
    tanh(pars[3]) * prod(sqrt(exp(0.5 * pars[1:2])))
  model$R[28:29] <- exp(pars[4:5])
  if(estimate) return(-logLik(model))
  model
}

fit_drivers2 <- optim(f = likfn, p = c(-8, -8, 1, -1, -3), method = "BFGS",
  model = model_drivers2)
model_drivers2 <- likfn(fit_drivers2$p, model_drivers2, estimate = FALSE)
model_drivers2$R[28:29, , 1]%*%t(model_drivers2$R[28:29, , 1])
model_drivers2$H

out_drivers2 <- KFS(model_drivers2)
out_drivers2
ts.plot(signal(out_drivers2, states = c("custom", "regression"))$signal,
  model_drivers2$y, col = 1:4)

# For confidence or prediction intervals, use predict on the original model
pred <- predict(model_drivers2,
  states = c("custom", "regression"), interval = "prediction")

# Note that even though the intervals were computed without seasonal pattern,
# PetrolPrice induces seasonal pattern to predictions
ts.plot(pred$front, pred$rear, model_drivers2$y,
  col = c(1, 2, 2, 3, 4, 4, 5, 6), lty = c(1, 2, 2, 1, 2, 2, 1, 1))
# }
# NOT RUN {
## Simulate ARMA(2, 2) process
set.seed(1)
y <- arima.sim(n = 1000, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),
               innov = rnorm(1000) * sqrt(0.5))


model_arima <- SSModel(y ~ SSMarima(ar = c(0, 0), ma = c(0, 0), Q = 1), H = 0)

likfn <- function(pars, model, estimate = TRUE){
  tmp <- try(SSMarima(artransform(pars[1:2]), artransform(pars[3:4]),
    Q = exp(pars[5])), silent = TRUE)
  if(!inherits(tmp, "try-error")){
    model["T", "arima"] <- tmp$T
    model["R", "arima"] <- tmp$R
    model["P1", "arima"] <- tmp$P1
    model["Q", "arima"] <- tmp$Q
    if(estimate){
      -logLik(model)
    } else model
  } else {
    if(estimate){
      1e100
    } else model
  }
}

fit_arima <- optim(par = c(rep(0, 4), log(1)), fn = likfn, method = "BFGS",
  model = model_arima)
model_arima <- likfn(fit_arima$par, model_arima, FALSE)

# AR coefficients:
model_arima$T[2:3, 2, 1]
# MA coefficients:
model_arima$R[3:4]
# sigma2:
model_arima$Q[1]
# intercept
KFS(model_arima)
# same with arima:
arima(y, c(2, 0, 2))
# small differences because the intercept is handled differently in arima

# }
# NOT RUN {
# Poisson model
# See Durbin and Koopman (2012)
model_van <- SSModel(VanKilled ~ law + SSMtrend(1, Q = list(matrix(NA)))+
               SSMseasonal(period = 12, sea.type = "dummy", Q = NA),
               data = Seatbelts, distribution = "poisson")

# Estimate variance parameters
fit_van <- fitSSM(model_van, c(-4, -7), method = "BFGS")

model_van <- fit_van$model

# use approximating model, gives posterior modes
out_nosim <- KFS(model_van, nsim = 0)
# State smoothing via importance sampling
out_sim <- KFS(model_van, nsim = 1000)

out_nosim
out_sim
# }
# NOT RUN {
## using deterministic inputs in observation and state equations
model_Nile <- SSModel(Nile ~ 
  SSMcustom(Z=1, T = 1, R = 0, a1 = 100, P1inf = 0, P1 = 0, Q = 0, state_names = "d_t") +
  SSMcustom(Z=0, T = 1, R = 0, a1 = 100, P1inf = 0, P1 = 0, Q = 0, state_names = "c_t") +
  SSMtrend(1, Q = 1500), H = 15000)
model_Nile$T
model_Nile$T[1, 3, 1] <- 1 # add c_t to level
model_Nile0 <- SSModel(Nile ~ 
  SSMtrend(2, Q = list(1500, 0), a1 = c(0, 100), P1inf = diag(c(1, 0))), 
  H = 15000)

ts.plot(KFS(model_Nile0)$mu, KFS(model_Nile)$mu, col = 1:2)

##########################################################
### Examples of generalized linear modelling with KFAS ###
##########################################################

# Same example as in ?glm
counts <- c(18, 17, 15, 20, 10, 20, 25, 13, 12)
outcome <- gl(3, 1, 9)
treatment <- gl(3, 3)
glm_D93 <- glm(counts ~ outcome + treatment, family = poisson())

model_D93 <- SSModel(counts ~ outcome + treatment,
  distribution = "poisson")

out_D93 <- KFS(model_D93)
coef(out_D93, last = TRUE)
coef(glm_D93)

summary(glm_D93)$cov.s
out_D93$V[, , 1]

# approximating model as in GLM
out_D93_nosim <- KFS(model_D93, smoothing = c("state", "signal", "mean"))

# with importance sampling. Number of simulations is too small here,
# with large enough nsim the importance sampling actually gives
# very similar results as the approximating model in this case
set.seed(1)
out_D93_sim <- KFS(model_D93,
  smoothing = c("state", "signal", "mean"), nsim = 1000)


## linear predictor
# GLM
glm_D93$linear.predictor
# approximate model, this is the posterior mode of p(theta|y)
c(out_D93_nosim$thetahat)
# importance sampling on theta, gives E(theta|y)
c(out_D93_sim$thetahat)


## predictions on response scale
# GLM
fitted(glm_D93)
# approximate model with backtransform, equals GLM
fitted(out_D93_nosim)
# importance sampling on exp(theta)
fitted(out_D93_sim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm_D93, type = "link", se.fit = TRUE)$se.fit^2)
# approx, equals to GLM results
c(out_D93_nosim$V_theta)
# importance sampling on theta
c(out_D93_sim$V_theta)


# prediction variances on response scale
# GLM
as.numeric(predict(glm_D93, type = "response", se.fit = TRUE)$se.fit^2)
# approx, equals to GLM results
c(out_D93_nosim$V_mu)
# importance sampling on theta
c(out_D93_sim$V_mu)

# A Gamma example modified from ?glm
# Now with log-link, and identical intercept terms
clotting <- data.frame(
u = c(5,10,15,20,30,40,60,80,100),
lot1 = c(118,58,42,35,27,25,21,19,18),
lot2 = c(69,35,26,21,18,16,13,12,12))

model_gamma <- SSModel(cbind(lot1, lot2) ~ -1 + log(u) +
    SSMregression(~ 1, type = "common", remove.intercept = FALSE),
  data = clotting, distribution = "gamma")

update_shapes <- function(pars, model) {
  model$u[, 1] <- pars[1]
  model$u[, 2] <- pars[2]
  model
}
fit_gamma <- fitSSM(model_gamma, inits = c(1, 1), updatefn = update_shapes,
method = "L-BFGS-B", lower = 0, upper = 100)
logLik(fit_gamma$model)
KFS(fit_gamma$model)
fit_gamma$model["u", times = 1]



# }
# NOT RUN {
####################################
### Linear mixed model with KFAS ###
####################################

# example from ?lmer of lme4 pacakge
data("sleepstudy", package = "lme4")

model_lmm <- SSModel(Reaction ~ Days +
    SSMregression(~ Days, Q = array(0, c(2, 2, 180)),
       P1 = matrix(NA, 2, 2), remove.intercept = FALSE), sleepstudy, H = NA)

# The first 10 time points the third and fouth state
# defined with SSMregression correspond to the first subject, and next 10 time points
# are related to second subject and so on.

# need to use ordinary $ assignment as [ assignment operator for SSModel
# object guards against dimension altering
model_lmm$T <- array(model_lmm["T"], c(4, 4, 180))
attr(model_lmm, "tv")[3] <- 1L #needs to be integer type!

# "cut the connection" between the subjects
times <- seq(10, 180, by = 10)
model_lmm["T",states = 3:4, times = times] <- 0

# for the first subject the variance of the random effect is defined via P1
# for others, we use Q
model_lmm["Q", times = times] <- NA

update_lmm <- function(pars = init, model){
  P1 <- diag(exp(pars[1:2]))
  P1[1, 2] <- pars[3]
  model["P1", states = 3:4] <- model["Q", times = times] <-
    crossprod(P1)
  model["H"] <- exp(pars[4])
  model
}

inits <- c(0, 0, 0, 3)

fit_lmm <- fitSSM(model_lmm, inits, update_lmm, method = "BFGS")
out_lmm <- KFS(fit_lmm$model)
# unconditional covariance matrix of random effects
fit_lmm$model["P1", states = 3:4]

# conditional covariance matrix of random effects
# same for each subject and time point due to model structure
# these differ from the ones obtained from lmer as these are not conditioned
# on the fixed effects
out_lmm$V[3:4,3:4,1]
# }
# NOT RUN {
# Example of Cubic spline smoothing
# See Durbin and Koopman (2012)
require("MASS")
data("mcycle")

model <- SSModel(accel ~ -1 +
    SSMcustom(Z = matrix(c(1, 0), 1, 2),
      T = array(diag(2), c(2, 2, nrow(mcycle))),
      Q = array(0, c(2, 2, nrow(mcycle))),
      P1inf = diag(2), P1 = diag(0, 2)), data = mcycle)

model$T[1, 2, ] <- c(diff(mcycle$times), 1)
model$Q[1, 1, ] <- c(diff(mcycle$times), 1)^3/3
model$Q[1, 2, ] <- model$Q[2, 1, ] <- c(diff(mcycle$times), 1)^2/2
model$Q[2, 2, ] <- c(diff(mcycle$times), 1)


updatefn <- function(pars, model, ...){
  model["H"] <- exp(pars[1])
  model["Q"] <- model["Q"] * exp(pars[2])
  model
}

fit <- fitSSM(model, inits = c(4, 4), updatefn = updatefn, method = "BFGS")

pred <- predict(fit$model, interval = "conf", level = 0.95)
plot(x = mcycle$times, y = mcycle$accel, pch = 19)
lines(x = mcycle$times, y = pred[, 1])
lines(x = mcycle$times, y = pred[, 2], lty = 2)
lines(x = mcycle$times, y = pred[, 3], lty = 2)
# }
# NOT RUN {

# }

\section{Summary}

This chapter illustrates how to fit the structural equation models fit by the \verb@StructTS()@ function in base R. The initial conditions used in that function were used here, however that should not be assumed to be the best choice. The default behavior for in the MARSS package is to treat $\xx_0$ as an estimated parameter with $\VV_0$ set to all 0. These models can be a challenge for the EM algorithm and for most examples the BFGS algorithm was used. If using EM, the algorithm will need to be run longer to achieve the maximum likelihood. 

The MARSS package allows you to fit multivariate versions of these models with a flexible data structure and flexible relationships between the data. It allows you to include covariates and model intervention effects. You can model multiple observations of the same process or different observations of independent processes that share some or all parameter values. You can also model cases where the trend (or seasonality) is shared across processes but not the levels. The ability to use multiple data sets can improve estimation and allow you to estimate an underlying process which might remain hidden if one one data set is used for estimation.

<<reset, echo=FALSE>>=
options(prompt = "> ", continue = " +", width = 120)
@
