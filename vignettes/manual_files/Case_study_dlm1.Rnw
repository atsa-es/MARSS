\SweaveOpts{keep.source=TRUE, prefix.string=./figures/CSDLM-, eps=FALSE, split=TRUE}
\chapter{Univariate dynamic linear models (DLMs)}
\label{chap:dlm-univariate}
\chaptermark{Univariate dynamic linear models}

<<RUNFIRST, echo=FALSE>>=
library(MARSS)
options(width=60)
options(prompt=" ", continue=" ")
op <- par(no.readonly = TRUE)   
library(xtable)
tabledir="tables/"
@
%Add footnote with instructions for getting code
\blfootnote{Type \texttt{RShowDoc("Chapter\_UnivariateDLM.R",package="MARSS")} at the R command line to open a file with all the code for the examples in this chapter.}

\section{Overview of dynamic linear models}
\index{MARSS model!DLM example}\index{dynamic linear modeling!univariate}In this chapter, we will use MARSS to analyze dynamic linear models (DLMs), wherein the parameters in a regression model are treated as time-varying. DLMs are used commonly in econometrics, but have received less attention in the ecological literature \citep[c.f.][]{Lamonetal1998, ScheuerellWilliams2005}. Our treatment of DLMs is rather cursory---we direct the reader to excellent textbooks by \citet{Poleetal1994} and \citet{Petrisetal2009} for more in-depth treatments of DLMs. The former focuses on Bayesian estimation whereas the latter addresses both likelihood-based and Bayesian estimation methods.

We begin our description of DLMs with a static regression model, wherein the $i$-th observation is a linear function of an intercept, predictor variable(s), and a random error term. For example, if we had one predictor variable ($F$), we could write the model as
\begin{equation}\label{eqn:lm}
y_i = \alpha + \beta F_i + v_i,
\end{equation}
where the $\alpha$ is the intercept, $\beta$ is the regression slope, $F_i$ is the predictor variable matched to the $i^{th}$ observation ($y_i$), and $v_i \sim \N(0,r)$. It is important to note here that there is no implicit ordering of the index $i$. That is, we could shuffle any/all of the $(y_i, F_i)$ pairs in our dataset with no effect on our ability to estimate the model parameters. We can write the model in Equation \ref{eqn:lm} using vector notation, such that
\begin{align}\label{eqn:lmVec}
y_i &= \begin{bmatrix}1&F_i\end{bmatrix} 
\begin{bmatrix}\alpha\\ \beta\end{bmatrix} + v_i \nonumber\\
&= \FF_i^{\top}\ttheta + v_i,
\end{align}
and $\FF_i^{\top} = \begin{bmatrix}\alpha \\ \beta\end{bmatrix}$ and $\ttheta = \begin{bmatrix}\alpha & \beta\end{bmatrix}^{\top}$.

In a DLM, however, the regression parameters are dynamic in that they evolve over time. For a single observation at time $t$, we can write
\begin{equation}\label{eqn:dlm1}
y_t = \FF_{t}^{\top}\ttheta_t + v_t,
\end{equation}
where $\FF_t$ is a column vector of regression variables at time $t$, $\ttheta_t$ is a column vector of regression parameters at time $t$ and $v_{t}\sim\N(0,r)$. While seemingly identical, this formulation presents two features that distinguish it from Equation \ref{eqn:lmVec}. First, the observed data are explicitly time ordered (i.e., $\yy=\lbrace{y_1,y_2,y_3,...,y_T}\rbrace$), which means we expect them to contain implicit information. Second, the relationship between the observed datum and the predictor variables are unique at every time $t$ (i.e., $\ttheta=\lbrace{\ttheta_1,\ttheta_2,\ttheta_3,...,\ttheta_T}\rbrace$).

However, closer examination of Equation \ref{eqn:dlm1} reveals an apparent complication for parameter estimation. With only one datum at each time step $t$, we could, at best, estimate only one regression parameter, and even then, the 1:1 correspondence between data and parameters would preclude any estimation of parameter uncertainty. To address this shortcoming, we return to the time ordering of model parameters. Rather than assume the regression parameters are independent from one time step to another, we instead model them as an autoregressive process where
\begin{equation}\label{eqn:dlm2}
\ttheta_t = \GG_t\ttheta_{t-1} + \ww_t,
\end{equation}
$\GG_t$ is the parameter ``evolution'' matrix, and $\ww_t$ is a vector of process errors, such that $\ww_t\sim\MVN(\mathbf{0},\QQ)$. The elements of $\GG_t$ may be known and fixed \textit{a priori}, or unknown and estimated from the data. Although we allow for $\GG_t$ to be time-varying, we will typically assume that it is time invariant.

The idea is that the evolution matrix $\GG_t$ deterministically maps the parameter space from one time step to the next, so the parameters at time $t$ are temporally related to those before and after. However, the process is stochastic and the mapping includes stochasitic error, which leads to a degradation of information over time. If the diagonal elements of $\QQ$ are relatively large, then the parameters can vary widely from $t$ to $t+1$. If $\QQ\ = \mathbf{0}$, then $\ttheta_1=\ttheta_2=\ttheta_T$ and we are back to the static model in Equation \ref{eqn:lm}.

%------------------------------------
\section{Example of a univariate DLM}
%------------------------------------
Let's consider an example from the literature. \citet{ScheuerellWilliams2005} used a DLM to examine the relationship between marine survival of Chinook salmon and an index of ocean upwelling strength along the west coast of the USA. Upwelling brings cool, nutrient-rich waters from the deep ocean to shallower coastal areas. Scheuerell and Williams hypothesized that stronger upwelling in April should create better growing conditions for phytoplankton, which would then translate into more zooplankton. In turn, juvenile salmon (``smolts'') entering the ocean in May and June should find better foraging opportunities. Thus, for smolts entering the ocean in year $t$,
\begin{equation}\label{eqn:dlmSW1}
survival_t = \alpha_t + \beta_t F_t + v_t \text{ with } v_{t}\sim\N(0,r),
\end{equation}
and $F_t$ is the coastal upwelling index\footnote{cubic meters of seawater per second per 100 m of coastline} for the month of April in year $t$. 

Both the intercept and slope are time varying, so
% full univariate DLM as MARSS
\begin{align}\label{eqn:dlmSW2}
\alpha_t &= \alpha_{t-1} + w_t^{(1)} \text{ with } w_t^{(1)} \sim \N(0,q_1); \text{ and}\\
\beta_t &= \beta_{t-1} + w_t^{(2)} \text{ with } w_t^{(2)} \sim \N(0,q_2).
\end{align}
If we define $\ttheta_t = \begin{bmatrix}\alpha_t & \beta_t\end{bmatrix}^\top$, $\GG_t = \II$ for all $t$, $\ww_t = \begin{bmatrix}w_t^{(1)} & w_t^{(2)}\end{bmatrix}^\top$, and $\QQ = \begin{bmatrix}q_1 & 0\\ 0 & q_2\end{bmatrix}$, we get Equation \ref{eqn:dlm2}. If we define $y_t = survival_t$ and $\FF_t = \begin{bmatrix}1 & F_t\end{bmatrix}^{\top}$, we can write out the full univariate DLM as a state-space model with the following form:
\begin{equation}\label{eqn:dlmSW3}
\begin{gathered}
\ttheta_t = \GG_t\ttheta_{t-1} + \ww_t \text{ with } \ww_t \sim \MVN(\mathbf{0},\QQ);\\
y_t = \FF_t^{\top}\ttheta_t + v_t \text{ with } v_t\sim\N(0,r);\\
\ttheta_0 \sim \MVN(\pipi_0,\LAM_0).
\end{gathered}
\end{equation}
Equation \ref{eqn:dlmSW3} is equivalent to our standard MARSS model: 
\begin{equation}\label{eqn:MARSSdlm}
\begin{gathered}
\xx_t = \BB_t\xx_{t-1} + \uu_t + \CC_t\cc_t + \ww_t \text{ with } \ww_t \sim \MVN(0,\QQ_t);\\
\yy_t = \ZZ_t\xx_t + \aa_t + \DD_t\dd_t + \vv_t \text{ with } \vv_t \sim \MVN(0,\RR_t);\\
\xx_0 \sim \MVN(\pipi,\LAM);
\end{gathered}
\end{equation}
where $\xx_t = \ttheta_t$, $\BB_t = \GG_t$, $\uu_t = \CC_t = \cc_t = \zer$, $\yy_t = y_t$ (i.e., $\yy_t$ is 1 $\mathsf{x}$ 1), $\ZZ_t = \FF_t^{\top}$, $\aa_t = \DD_t = \dd_t = \zer$, and $\RR_t = r$ (i.e., $\RR_t$ is 1 $\mathsf{x}$ 1).
%------------------------------------------------
\subsection{Fitting a univariate DLM with MARSS}
%------------------------------------------------
Now let's go ahead and analyze the DLM specified in Equations \ref{eqn:dlmSW1}--\ref{eqn:dlmSW3}. We begin by getting the data set, which has 3 columns for 1) the year the salmon smolts migrated to the ocean ($year$), 2) logit-transformed survival\footnote{Survival in the original context was defined as the proportion of juveniles that survive to adulthood. Thus, we use the logit function, defined as $logit(p)=log_e(p/[1-p])$, to map survival from the open interval (0,1) onto the interval $(-\infty,\infty)$, which allows us to meet our assumption of normally distributed observation errors.} ($logit.s$), and 3) the coastal upwelling index for April ($CUI.apr$). There are 42 years of data (1964--2005).
<<Cs_01_read.in.data, eval=TRUE>>=
data(SalmonSurvCUI)
years = SalmonSurvCUI[,1]
TT = length(years)
# response data: logit(survival)
dat = matrix(SalmonSurvCUI[,2],nrow=1)
@

As we have seen in other chapters, standardizing our covariate(s) to have zero-mean and unit-variance can be helpful in model fitting and interpretation. In this case, it is a good idea because the variance of $CUI.apr$ is orders of magnitude greater than $survival$.
<<Cs_02_z.score, eval=TRUE>>=
CUI = SalmonSurvCUI[,3]
CUI.z = zscore(CUI)
# number of regression params (slope + intercept)
m = dim(CUI.z)[1] + 1
@
Plots of logit-transformed survival and the z-scored April upwelling index are shown in Figure \ref{fig:CSX.fig1}.

\begin{figure}[htp]
\begin{center}
<<Cs_030_plotdata, eval=TRUE, echo=FALSE, fig=TRUE, height=4, width=6>>=
par(mfrow=c(m,1), mar=c(4,4,0.1,0), oma=c(0,0,2,0.5))
plot(years, dat, xlab="", ylab="Logit(s)", bty="n", xaxt="n", pch=16, col="darkgreen", type="b")
plot(years, CUI.z, xlab="", ylab="CUI", bty="n", xaxt="n", pch=16, col="blue", type="b")
axis(1,at=seq(1965,2005,5))
mtext("Year of ocean entry", 1, line=3)
@
\end{center}
\caption{Time series of logit-transformed marine survival estimates for Snake River spring/summer Chinook salmon (top) and z-scores of the coastal upwelling index at 45N 125W (bottom). The $x$-axis indicates the year that the salmon smolts entered the ocean.}
\label{fig:CSX.fig1}
\end{figure}

Next, we need to set up the appropriate matrices and vectors for MARSS. Let's begin with those for the process equation because they are straightforward.
<<Cs_031_univ.DLM.proc, eval=TRUE>>=
# for process eqn
B = diag(m)                   # 2x2; Identity
U = matrix(0,nrow=m,ncol=1)   # 2x1; both elements = 0
Q = matrix(list(0),m,m)       # 2x2; all 0 for now
diag(Q) = c("q1","q2")        # 2x2; diag = (q1,q2)
@

Defining the correct form for the observation model is a little more tricky, however, because of how we model the effect(s) of explanatory variables. In a DLM, we need to use $\ZZ_t$ (instead of $\dd_t$) as the matrix of known regressors/drivers that affect $\yy_t$, and $\xx_t$ (instead of $\DD_t$) as the regression parameters. Therefore, we need to set $\ZZ_t$ equal to an $n$ $\mathsf{x}$ $m$ $\mathsf{x}$ $T$ array, where $n$ is the number of response variables (= 1; $y_t$ is univariate), $m$ is the number of regression parameters (= intercept + slope = 2), and $T$ is the length of the time series (= 42). 
<<Cs_04_univ.DLM.obs, eval=TRUE>>=
# for observation eqn
Z = array(NA, c(1,m,TT))   # NxMxT; empty for now
Z[1,1,] = rep(1,TT)        # Nx1; 1's for intercept
Z[1,2,] = CUI.z            # Nx1; regr variable
A = matrix(0)              # 1x1; scalar = 0
R = matrix("r")            # 1x1; scalar = r
@

Lastly, we need to define our lists of initial starting values and model matrices/vectors. 
<<Cs_05_univ.DLM.list, eval=TRUE>>=
# only need starting values for regr parameters
inits.list = list(x0=matrix(c(0, 0), nrow=m))
# list of model matrices & vectors
mod.list = list(B=B, U=U, Q=Q, Z=Z, A=A, R=R)
@
And now we can fit our DLM with MARSS.
<<Cs_06_univ.DLM.fit, eval=TRUE>>=
dlm1 = MARSS(dat, inits=inits.list, model=mod.list)
@

Notice that the MARSS output does not list any estimates of the regression parameters themselves. Why not? Remember that in a DLM the matrix of states $(\xx)$ contains the estimates of the regression parameters $(\ttheta)$. Therefore, we need to look in \verb@dlm1$states@ for the MLEs of the regression parameters, and in \verb@dlm1$states.se@ for their standard errors.

Time series of the estimated intercept and slope are shown in Figure \ref{fig:CSX.fig2}. It appears as though the intercept is much more dynamic than the slope, as indicated by a much larger estimate of process variance for the former (\verb@Q.q1@). In fact, although the effect of April upwelling appears to be increasing over time, it doesn't really become important as an explanatory variable until about 1990 when the approximate 95\% confidence interval for the slope no longer overlaps zero.
%----------------------------
% plot regression parameters
%----------------------------
\begin{figure}[htp]
\begin{center}
<<Cs_07_plotdlm1, eval=TRUE, echo=FALSE, fig=TRUE, height=4, width=6>>=
ylabs = c(expression(alpha[t]), expression(beta[t]))
colr = c("darkgreen","blue")
par(mfrow=c(m,1), mar=c(4,4,0.1,0), oma=c(0,0,2,0.5))
for(i in 1:m) {
  mn = dlm1$states[i,]
  se = dlm1$states.se[i,]
  plot(years,mn,xlab="",ylab=ylabs[i],bty="n",xaxt="n",type="n",
  ylim=c(min(mn-2*se),max(mn+2*se)))
  lines(years, rep(0,TT), lty="dashed")
  lines(years, mn, col=colr[i], lwd=3)
  lines(years, mn+2*se, col=colr[i])
  lines(years, mn-2*se, col=colr[i])
}
axis(1,at=seq(1965,2005,5))
mtext("Year of ocean entry", 1, line=3)
@
\end{center}
\caption{Time series of estimated mean states (thick lines) for the intercept (top) and slope (bottom) parameters from the univariate DLM specified by Equations \ref{eqn:dlmSW1}--\ref{eqn:dlmSW3}. Thin lines denote the mean $\pm$ 2 standard deviations.}
\label{fig:CSX.fig2}
\end{figure}

%----------------------------------------------
\section{Forecasting with a univariate DLM}
%----------------------------------------------
\index{dynamic linear modeling!forecasting}\index{forecasting}\citet{ScheuerellWilliams2005} were interested in how well upwelling could be used to forecast expected survival of salmon. Let's look at how well our model does in that context. To do so, we need the predictive distributions for the regression parameters and observation.

Beginning with our definition for the distribution of the parameters at time $t=0$, $\ttheta_0 \sim \MVN(\pipi_0,\LAM_0)$ in Equation \ref{eqn:dlmSW3}, we write
\begin{equation}\label{eqn:dlmFore1}
\ttheta_{t-1}|y_{1:t-1} \sim \MVN(\pipi_{t-1},\LAM_{t-1})
\end{equation}
to indicate the distribution of $\ttheta$ at time $t-1$ conditioned on the observed data through time $t-1$ (i.e., $y_{1:t-1}$). Then, we can write the one-step ahead predictive distribution for $\ttheta_t$ given $y_{1:t-1}$ as
\begin{align}\label{eqn:dlmFore2}
\ttheta_{t}|y_{1:t-1} &\sim \MVN(\et_{t},\PH_{t}), \text{ where} \nonumber\\
\et_{t} &= \GG_t\pipi_{t-1}, \text{ and}\\
\PH_{t} &= \GG_t\LAM_{t-1}\GG_t^{\top} + \QQ \nonumber.
\end{align}
Consequently, the one-step ahead predictive distribution for the observation at time $t$ given $y_{1:t-1}$ is
\begin{align}\label{eqn:dlmFore3}
y_{t}|y_{1:t-1} &\sim \N(\zeta_{t},\Psi_{t}), \text{ where} \nonumber\\
\zeta_{t} &= \FF_t\et_{t}, \text{ and}\\
\Psi_{t} &= \FF_t\Phi_{t}\FF_t^{\top} + \RR \nonumber.
\end{align}

%----------------------------------------------
\subsection{Forecasting a univariate DLM with MARSS}
%----------------------------------------------
Working from Equation \ref{eqn:dlmFore3}, we can now use MARSS to compute the expected value of the forecast at time $t$ $(\E [y_t|y_{1:t-1}] = \zeta_t)$, and its variance $(\var [y_t|y_{1:t-1}] = \Psi_t)$. For the expectation, we need $\FF_t\et_t$. Recall that $\FF_t$ is our $1 \times m$ matrix of explanatory variables at time $t$ ($\FF_t$ is called $\ZZ_t$ in MARSS notation). The one-step ahead forecasts of the parameters at time $t$ $(\et_t)$ are calculated as part of the Kalman filter algorithm---they are termed $\tilde{x}_t^{t-1}$ in MARSS notation and stored as \verb@xtt1@ in the list produced by the \verb@MARSSkfss()@ function.
<<Cs_08_univ.DLM.fore.mean, eval=TRUE>>=
# get list of Kalman filter output
kf.out = MARSSkfss(dlm1)
# forecasts of regr parameters; 2xT matrix
eta = kf.out$xtt1
# ts of E(forecasts)
fore.mean = vector()
for(t in 1:TT) {
  fore.mean[t] = Z[,,t] %*% eta[,t,drop=F]
}
@

For the variance of the forecasts, we need $\FF_t\Phi_{t}\FF_t^{\top} + \RR$. As with the mean, $\FF_t \equiv \ZZ_t$. The variances of the one-step ahead forecasts of the parameters at time $t$ $(\Phi_t)$ are also calculated as part of the Kalman filter algorithm---they are stored as \verb@Vtt1@ in the list produced by the \verb@MARSSkfss()@ function. Lastly, the observation variance $\RR$ is part of the standard MARSS output.
<<Cs_09_univ.DLM.fore.Var, eval=TRUE>>=
# variance of regr parameters; 1x2xT array
Phi = kf.out$Vtt1
# obs variance; 1x1 matrix
R.est = coef(dlm1, type="matrix")$R
# ts of Var(forecasts)
fore.var = vector()
for(t in 1:TT) {
  tZ = matrix(Z[,,t],m,1) # transpose of Z
  fore.var[t] = Z[,,t] %*% Phi[,,t] %*% tZ + R.est
}
@

Plots of the model mean forecasts with their estimated uncertainty are shown in Figure \ref{fig:CSX.fig3}. Nearly all of the observed values fell within the approximate prediction interval. Notice that we have a forecasted value for the first year of the time series (1964), which may seem at odds with our notion of forecasting at time $t$ based on data available only through time $t-1$. In this case, however, MARSS is actually estimating the states at $t=0$ ($\ttheta_0$), which allows us to compute a forecast for the first time point.
%-----------------------------
% forecast plot - logit space
%-----------------------------
\begin{figure}[htp]
\begin{center}
<<Cs_10_plot-dlm-Forecast-Logit, eval=TRUE, echo=FALSE, fig=TRUE, height=3, width=6>>=
par(mar=c(4,4,0.1,0), oma=c(0,0,2,0.5))
ylims=c(min(fore.mean-2*sqrt(fore.var)),max(fore.mean+2*sqrt(fore.var)))
plot(years, t(dat), type="p", pch=16, ylim=ylims,
     col="blue", xlab="", ylab="Logit(s)", xaxt="n")
lines(years, fore.mean, type="l", xaxt="n", ylab="", lwd=3)
lines(years, fore.mean+2*sqrt(fore.var))
lines(years, fore.mean-2*sqrt(fore.var))
axis(1,at=seq(1965,2005,5))
mtext("Year of ocean entry", 1, line=3)
@
\end{center}
\caption{Time series of logit-transformed survival data (blue dots) and model mean one-step ahead forecasts (thick line). Thin lines denote the approximate 95\% prediction intervals.}
\label{fig:CSX.fig3}
\end{figure}

Although our model forecasts look reasonable in logit-space, it is worthwhile to examine how well they look when the survival data and forecasts are back-transformed onto the interval [0,1] (Figure \ref{fig:CSX.fig4}). In that case, the accuracy does not seem to be affected, but the precision appears much worse, especially during the early and late portions of the time series when survival is changing rapidly.
%------------------------------
% forecast plot - normal space
%------------------------------
\begin{figure}[htp]
\begin{center}
<<Cs_11_plot-dlm-Forecast-Raw, eval=TRUE, echo=FALSE, fig=TRUE, height=3, width=6>>=
invLogit = function(x) {1/(1+exp(-x))}
ff = invLogit(fore.mean)
fup = invLogit(fore.mean+2*sqrt(fore.var))
flo = invLogit(fore.mean-2*sqrt(fore.var))
par(mar=c(4,4,0.1,0), oma=c(0,0,2,0.5))
ylims=c(min(flo),max(fup))
plot(years, invLogit(t(dat)), type="p", pch=16, ylim=ylims,
     col="blue", xlab="", ylab="Survival", xaxt="n")
lines(years, ff, type="l", xaxt="n", ylab="", lwd=3)
lines(years, fup)
lines(years, flo)
axis(1,at=seq(1965,2005,5))
mtext("Year of ocean entry", 1, line=3)
@
\end{center}
\caption{Time series of survival data (blue dots) and model mean forecasts (thick line). Thin lines denote the approximate 95\% prediction intervals.}
\label{fig:CSX.fig4}
\end{figure}
%-------------------------------------
\subsection{DLM forecast diagnostics}
%-------------------------------------
\index{forecasting!diagnostics}
\begin{samepage}
As with other time series models, evaluation of a DLM should include model diagnostics. In a forecasting context, we are often interested in the forecast errors, which are simply the observed data minus the forecasts $(e_t = y_t - \zeta_t)$. In particular, the following assumptions should hold true for $e_t$:
\begin{enumerate}
\item $e_t \sim \N(0,\sigma^2)$;
\item $\cov(e_t,e_{t-k}) = 0$.
\end{enumerate}
\end{samepage}

In the literature on state-space models, the set of $e_t$ are commonly referred to as ``innovations''. \verb@MARSS()@ calculates the innovations as part of the Kalman filter algorithm---they are stored as \verb@Innov@ in the list produced by the \verb@MARSSkfss()@ function.
<<Cs_12_dlm-forecast-errors, eval=TRUE, echo=TRUE>>=
# forecast errors
innov = kf.out$Innov
@

Let's see if our innovations meet the model assumptions. Beginning with (1), we can use a Q-Q plot to see whether the innovations are normally distributed with a mean of zero. We'll use the \verb@qqnorm()@ function to plot the quantiles of the innovations on the $y$-axis versus the theoretical quantiles from a Normal distribution on the $x$-axis. If the 2 distributions are similar, the points should fall on the line defined by $y = x$.
<<dlm-QQplot, eval=FALSE, echo=TRUE>>=
# Q-Q plot of innovations
qqnorm(t(innov), main="", pch=16, col="blue")
# add y=x line for easier interpretation
qqline(t(innov))
@
%----------------------
% diagnostics plot: QQ
%----------------------
\begin{figure}[htp]
\begin{center}
<<Cs_13_plot-dlmQQ, eval=TRUE, echo=FALSE, fig=TRUE, height=2, width=4>>=
# use layout to get nicer plots
layout(matrix(c(0,1,1,1,0),1,5,byrow=TRUE))
# set up L plotting space
par(mar=c(4,4,1,0), oma=c(0,0,0,0.5))
# Q-Q plot of innovations
qqnorm(t(innov), main="", pch=16, col="blue")
qqline(t(innov))
# set up R plotting space
#par(mar=c(4,0,1,1)) #, oma=c(0,0,0,0.5))
# boxplot of innovations
#boxplot(t(innov), axes=FALSE)
@
\end{center}
\caption{Q-Q plot of the forecast errors (innovations) for the DLM specified in Equations \ref{eqn:dlmSW1}--\ref{eqn:dlmSW3}.}
\label{fig:CSX.fig5}
\end{figure}

The Q-Q plot (Figure \ref{fig:CSX.fig5}) indicates that the innovations appear to be more-or-less normally distributed (i.e., most points fall on the line). Furthermore, it looks like the mean of the innovations is about 0, but we should use a more reliable test than simple visual inspection. We can formally test whether the mean of the innovations is significantly different from 0 by using a one-sample $t$-test.  based on a null hypothesis of $\E[e_t]=0$. To do so, we will use the function \verb@t.test()@ and base our inference on a significance value of $\alpha = 0.05$.
<<Cs_14_dlm-Innov-t-test, eval=TRUE, echo=TRUE>>=
# p-value for t-test of H0: E(innov) = 0
t.test(t(innov), mu=0)$p.value
@
The $p$-value $>>$ 0.05 so we cannot reject the null hypothesis that $\E[e_t]=0$.

Moving on to assumption (2), we can use the sample autocorrelation function (ACF) to examine whether the innovations covary with a time-lagged version of themselves. Using the \verb@acf()@ function, we can compute and plot the correlations of $e_t$ and $e_{t-k}$ for various values of $k$. Assumption (2) will be met if none of the correlation coefficients exceed the 95\% confidence intervals defined by $\pm \, z_{0.975} / \sqrt{n}$.
<<dlmACFplot, eval=FALSE, echo=TRUE>>=
# plot ACF of innovations
acf(t(innov), lag.max=10)
@
%-----------------------
% diagnostics plot: ACF
%-----------------------
\begin{figure}[htp]
\begin{center}
<<Cs_15_plot-dlm-ACF, eval=TRUE, echo=FALSE, fig=TRUE, height=2, width=4>>=
# use layout to get nicer plots
layout(matrix(c(0,1,1,1,0),1,5,byrow=TRUE))
# set up plotting space
par(mar=c(4,4,1,0), oma=c(0,0,0,0.5))
# ACF of innovations
acf(t(innov), lwd=2, lag.max=10)
@
\end{center}
\caption{Autocorrelation plot of the forecast errors (innovations) for the DLM specified in Equations \ref{eqn:dlmSW1}--\ref{eqn:dlmSW3}. Horizontal blue lines define the upper and lower 95\% confidence intervals.}
\label{fig:CSX.fig6}
\end{figure}
The ACF plot (Figure \ref{fig:CSX.fig6}) shows no significant autocorrelation in the innovations at lags 1--10, so it appears that both of our model assumptions have been met.

%---------
% THE END
%---------
<<Reset, echo=FALSE>>=
options(prompt="> ", continue="+ ")
@