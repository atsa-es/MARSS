%\VignetteIndexEntry{Residuals} 
%\VignettePackage{MARSS}
\documentclass[]{article}
%set margins to 1in without fullsty
	\addtolength{\oddsidemargin}{-.875in}
	\addtolength{\evensidemargin}{-.875in}
	\addtolength{\textwidth}{1.75in}

	\addtolength{\topmargin}{-.875in}
	\addtolength{\textheight}{1.75in}
%\usepackage{fullpage} %more standardized margins

% choose options for [] as required from the list
% in the Reference Guide, Sect. 2.2

\usepackage{multirow}
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage[round]{natbib}

% Math stuff
\usepackage{amsmath} % the standard math package
\usepackage{amsfonts} % the standard math package
%%%% bold maths symbol system:
\def\uupsilon{\pmb{\upsilon}}
\def\llambda{\pmb{\lambda}}
\def\bbeta{\pmb{\beta}}
\def\aalpha{\pmb{\alpha}}
\def\zzeta{\pmb{\zeta}}
\def\etaeta{\mbox{\boldmath $\eta$}}
\def\xixi{\mbox{\boldmath $\xi$}}
\def\ep{\mbox{\boldmath $\epsilon$}}
\def\DEL{\mbox{\boldmath $\Delta$}}
\def\PHI{\mbox{\boldmath $\Phi$}}
\def\PI{\mbox{\boldmath $\Pi$}}
\def\LAM{\mbox{\boldmath $\Lambda$}}
\def\LAMm{\mathbb{L}}
\def\GAM{\mbox{\boldmath $\Gamma$}}
\def\OMG{\mbox{\boldmath $\Omega$}}
\def\SI{\mbox{\boldmath $\Sigma$}}
\def\TH{\mbox{\boldmath $\Theta$}}
\def\UPS{\mbox{\boldmath $\Upsilon$}}
\def\XI{\mbox{\boldmath $\Xi$}}
\def\AA{\mbox{$\mathbf A$}}	\def\aa{\mbox{$\mathbf a$}}
\def\Ab{\mbox{$\mathbf D$}} \def\Aa{\mbox{$\mathbf d$}} \def\Am{\PI}
\def\BB{\mbox{$\mathbf B$}}	\def\bb{\mbox{$\mathbf b$}} \def\Bb{\mbox{$\mathbf J$}} \def\Ba{\mbox{$\mathbf L$}} \def\Bm{\UPS}
\def\CC{\mbox{$\mathbf C$}}	\def\cc{\mbox{$\mathbf c$}}
\def\Ca{\Delta} \def\Cb{\GAM}
\def\DD{\mbox{$\mathbf D$}}	\def\dd{\mbox{$\mathbf d$}}
\def\EE{\mbox{$\mathbf E$}}	\def\ee{\mbox{$\mathbf e$}}
\def\E{\,\textup{\textrm{E}}}	
\def\EXy{\,\textup{\textrm{E}}_{\text{{\bf XY}}}}
\def\FF{\mbox{$\mathbf F$}} \def\ff{\mbox{$\mathbf f$}}
\def\GG{\mbox{$\mathbf G$}}	\def\gg{\mbox{$\mathbf g$}}
\def\HH{\mbox{$\mathbf H$}}	\def\hh{\mbox{$\mathbf h$}}
\def\II{\mbox{$\mathbf I$}} \def\ii{\mbox{$\mathbf i$}}
\def\IIm{\mbox{$\mathbf I$}}
\def\JJ{\mbox{$\mathbf J$}}
\def\KK{\mbox{$\mathbf K$}}
\def\LL{\mbox{$\mathbf L$}}	\def\ll{\mbox{$\mathbf l$}}
\def\MM{\mbox{$\mathbf M$}}  \def\mm{\mbox{$\mathbf m$}}
\def\N{\,\textup{\textrm{N}}}
\def\MVN{\,\textup{\textrm{MVN}}}
\def\OO{\mbox{$\mathbf O$}}
\def\PP{\mbox{$\mathbf P$}}  \def\pp{\mbox{$\mathbf p$}}
\def\QQ{\mbox{$\mathbf Q$}}	 \def\qq{\mbox{$\mathbf q$}} \def\Qb{\mbox{$\mathbf G$}}  \def\Qm{\mathbb{Q}}
\def\RR{\mbox{$\mathbf R$}}	 \def\rr{\mbox{$\mathbf r$}} \def\Rb{\mbox{$\mathbf H$}}	\def\Rm{\mathbb{R}}
\def\Ss{\mbox{$\mathbf S$}}
\def\UU{\mbox{$\mathbf U$}}	\def\uu{\mbox{$\mathbf u$}}
\def\Ub{\mbox{$\mathbf C$}} \def\Ua{\mbox{$\mathbf c$}} \def\Um{\UPS}
\def\VV{\mbox{$\mathbf V$}}	\def\vv{\mbox{$\mathbf v$}}
\def\WW{\mbox{$\mathbf W$}}	\def\ww{\mbox{$\mathbf w$}}
%\def\XX{\mbox{$\mathbf X$}}
\def\XX{\mbox{$\pmb{X}$}}	\def\xx{\mbox{$\pmb{x}$}}
%\def\xx{\mbox{$\mathbf x$}}
%\def\YY{\mbox{$\mathbf Y$}}
\def\YY{\mbox{$\pmb{Y}$}}	\def\yy{\mbox{$\pmb{y}$}}
%\def\yy{\mbox{$\mathbf y$}}
\def\ZZ{\mbox{$\mathbf Z$}}	\def\zz{\mbox{$\mathbf z$}}	\def\Zb{\mbox{$\mathbf M$}} \def\Za{\mbox{$\mathbf N$}} \def\Zm{\XI}
\def\zer{\mbox{\boldmath $0$}}
\def\chol{\,\textup{\textrm{chol}}}
\def\vec{\,\textup{\textrm{vec}}}
\def\var{\,\textup{\textrm{var}}}
\def\cov{\,\textup{\textrm{cov}}}
\def\diag{\,\textup{\textrm{diag}}}
\def\trace{\,\textup{\textrm{trace}}}
\def\hatxt{\widetilde{\mbox{$\mathbf x$}}_t}
\def\hatxone{\widetilde{\mbox{$\mathbf x$}}_1}
\def\hatxzero{\widetilde{\mbox{$\mathbf x$}}_0}
\def\hatxtm{\widetilde{\mbox{$\mathbf x$}}_{t-1}}
\def\hatxQtm{\widetilde{\mathbb{x}}_{t-1}}
\def\hatyt{\widetilde{\mbox{$\mathbf y$}}_t}
\def\hatyyt{\widetilde{\mbox{$\mathbf y$}\mbox{$\mathbf y$}^\top}_t}
\def\hatyone{\widetilde{\mbox{$\mathbf y$}}_1}
\def\hatwt{\widetilde{\mbox{$\mathbf w$}}_t}
\def\hatOt{\widetilde{\OO}_t}
\def\hatWt{\widetilde{\WW}_t}
\def\hatYXt{\widetilde{\mbox{$\mathbf{y}\mathbf{x}$}}_t}
\def\hatXYt{\widetilde{\mbox{$\mathbf{x}\mathbf{y}$}}_t}
\def\hatYXttm{\widetilde{\mbox{$\mathbf{y}\mathbf{x}$}}_{t,t-1}}
\def\hatPt{\widetilde{\PP}_t}
\def\hatPtm{\widetilde{\PP}_{t-1}}
\def\hatPQtm{\widetilde{\mathbb{P}}_{t-1}}
\def\hatPttm{\widetilde{\PP}_{t,t-1}}
\def\hatPQttm{\widetilde{\mathbb{P}}_{t,t-1}}
\def\hatPtmt{\widetilde{\PP}_{t-1,t}}
\def\hatVt{\widetilde{\VV}_t}
\def\hatVtm{\widetilde{\VV}_{t-1}}
\def\hatVttm{\widetilde{\VV}_{t,t-1}}
\def\hatUt{\widetilde{\UU}_t}
\def\hatSt{\widetilde{\Ss}_t}
\def\hatSttm{\widetilde{\Ss}_{t,t-1}}
\def\hatSttp{\widetilde{\Ss}_{t,t+1}}
\def\hatBmt{\widetilde{\Bm}_t}
\def\hatCat{\widetilde{\Ca}_t}
\def\hatCbt{\widetilde{\Cb}_t}
\def\hatZmt{\widetilde{\Zm}_t}
\def\YYr{\dot{\mbox{$\pmb{Y}$}}}
\def\yyr{\dot{\mbox{$\pmb{y}$}}}
\def\aar{\dot{\mbox{$\mathbf a$}}}
\def\ZZr{\dot{\mbox{$\mathbf Z$}}}
\def\RRr{\dot{\mbox{$\mathbf R$}}}
\def\IR{\nabla}
\usepackage[round]{natbib} % to get references that are like in ecology papers
% \citet{} for inline citation name (year); \citep for citation in parens (name year)

%allow lines in matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\setcounter{tocdepth}{1} %no subsections in toc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\SweaveOpts{concordance=TRUE}
\author{E. E. Holmes\footnote{Northwest Fisheries Science Center, NOAA Fisheries, Seattle, WA 98112, 
       eli.holmes@noaa.gov, http://faculty.washington.edu/eeholmes}}
\title{Computation of Intervals for MARSS Models}
\maketitle
\begin{abstract}
This report shows how to compute the variance of the joint conditional model and state residuals for multivariate autoregressive Gaussian state-space (MARSS) models. The MARSS model can be written: $\mathbf{x}_t=\mathbf{B}\mathbf{x}_{t-1}+\mathbf{u}+\mathbf{w}_t$, $\mathbf{y}_t=\mathbf{Z}\mathbf{x}_t+\mathbf{a}+\mathbf{v}_t$, where $\mathbf{w}_t$ and $\mathbf{v}_t$ are multivariate normal error-terms with variance-covariance matrices $\mathbf{Q}$ and \mathbf{R} respectively. The joint conditional residuals are the $\mathbf{w}_t$ and $\mathbf{v}_t$ conditioned on a set of, possibly incomplete, data $\mathbf{y}$. Harvey, Koopman and Penzer (1998) show a recursive algorithm for these residuals. I show the equation for the residuals using the conditional variances of the states and the conditional covariance between unobserved data and states. This allows one to compute the variance of un-observed residuals, which needed for leave-one-out cross-validation tests and to compute the conditional variance of the missing data. I show how to modify the Harvey et al. algorithm in the case of missing values and how to modify it to return the non-normalized conditional residuals.
\end{abstract}
Keywords: Time-series analysis, Kalman filter, residuals, maximum-likelihood, vector autoregressive model, dynamic linear model, parameter estimation, state-space
\vfill
{\noindent \small citation: Holmes, E. E. 2014. Computation of standardized residuals for (MARSS) models. Technical Report. arXiv:1411.0045 }
 \newpage
 
\section{Overview}

This report discusses the computation of the confidence and prediction intervals for for MARSS models of the  form:
\begin{equation}\label{eq:residsMARSS}
\begin{gathered}
\xx_t = \BB_t\xx_{t-1} + \uu_t + \ww_t, \text{ where } \WW_t \sim \MVN(0,\QQ_t)\\
\yy_t = \ZZ_t\xx_t + \aa_t + \vv_t, \text{ where } \VV_t \sim \MVN(0,\RR_t)\\
\XX_0 \sim \MVN(\xixi,\LAM)
\end{gathered}
\end{equation}

Given a set of observed data $\yy_t$ and states $\xx_t$, the model residuals are $\yy_t - (\ZZ_t \xx_t + \aa_t)=\vv_t$.  The model residual is a random variable since $\yy_t$ and $\xx_t$ are drawn from the joint multivariate distribution of $\YY_t$ and $\XX_t$ defined by the MARSS equation.
The unconditional\footnote{meaning not conditioning on any particular set of observed data but rather taking the expectation across all possible values of $\yy_t$ and $\xx_t$.} variance of the model residuals is
\begin{equation}\label{eqn:unconditiondistofVt}
\var(\YY_t - (\ZZ_t \XX_t + \aa_t)) = \var(\VV_t) = \RR_t\\
\end{equation}
based on the definition of $\VV_t$.  

Once we have data, $\RR_t$ is not the variance of our model residuals because our residuals are now conditioned on a set of observed data.
There are two types of conditional model residuals used in MARSS analyses: innovations and smoothations.  Innovations are the model residuals at time $t$ using the expected value of $\XX_t$ conditioned on the data from 1 to $t-1$.  Smoothations  are the model residuals using the expected value of $\xx_t$ conditioned on all the data, $t=1$ to $T$.  Smoothations are used in computing standardized residuals for outlier and structural break detection \citep{Harveyetal1998, deJongPenzer1998, CommandeurKoopman2007}.  

\section{Distribution of the MARSS conditional residuals}

Why are residual and prediction intervals different? Here is a simple example. Say you observe 100 data points (y) from a Normal distribution with mean u and variance q. The estimate of u is the mean of y. Let's call that \eqn{\hat{u}}. The estimate of q is the variance of the sample residuals: \eqn{var(y-\hat{u})}, but q is not the variance of \eqn{y-\hat{u}}! q is the variance of \eqn{y-u}. The variance of \eqn{y-\hat{u}} is smaller than q because y were used to estimate \eqn{\hat{u}}. The y are closer (on average) to the sample mean (mean of y) than y is to u (population mean). That is why residual intervals are narrower than prediction intervals. If you are doing residual analysis (looking for outliers), then you uses the residual intervals.

\section{Confidence intervals for the estimate of \eqn{E[X_t]}}

After going around in circles, I settled on just adding ytT to tidy. I also decided that VtT is an estimate of the standard error of E[X(t)|data]-x(t), where x(t) is true state.  As you have more and more replicates of Y(t), VtT does shrink and xtT approaches x(t).  So VtT is behaving like a standard error where the thing being estimated is x(t) and its estimate is E[X(t)].  Not sure how to prove this, but since everything is normal, VtT should be an estimate of variance_Y|x E[X(t)|Y=y]-x(t).  So imagine many Y generated from Zx(t)+u+v(t) and then estimate E[X(t)]. The variance of E[X(t)]-x(t) should be VtT.  That variance may be over both X and Y|x however. So 
var_Y|x E[X(t)|Y=y]-x(t) or var_X E[X(t)|Y=y]-X(t) 
or E_X var_Y|x E[X(t)|Y=y]-x(t)

Below is the going around in circles part.

This is for the confidence interval of \hat{E[X(t)]}-E[X(t)] which is not really what we want. We want the interval for E[X(t)]-x(t), i.e. how far are we from the true x(t) or how certain are we about the true x(t)? Idea I think is that E[X(t)]-X(t)|\hat{\Theta} is a decent estimate of E[X(t)]-X(t)|\Theta.  But maybe I want E[X(t)]-x(t)|\hat{\Theta} where \hat{\Theta} is a random variable drawn from its var-cov matrix and x(t) is the true x(t).

Currently the confidence and prediction intervals for the states (the x(t)) and observations (the y(t)) use the point estimates of the model parameters, and thus treat these as known. Computation of the intervals with parameter uncertainty is conceptually straight forward. Let's start with a review of computing the standard error of a sample statistic.

\strong{Standard error} The standard error of the estimate of \eqn{E[X_t]}---whether conditioned on all the data (\code{xtT}), data up to t-1 (\code{xtt1}) or data up to t (\code{xtt})---is a measure of the how far the estimate  is from the true \eqn{E[X_t]} using the true \eqn{\Theta}. The standard error of the estimate of \eqn{E[X_t]}} uses the information about our uncertainty in \eqn{\hat{\Theta}}. With more data, i.e. a longer time series or multiple \eqn{y_t} at each t, the uncertainty in our estimated \eqn{\Theta} shrinks and the standard error of the estimate of \eqn{E[X_t]}} shrinks. This is the standard notion of the standard error of a sample statistic.

So how do you get the standard error of the estimate of \eqn{E[X_t]}}? This can be computed analytically for MARSS models, but let's imagine doing this with a parametric bootstrap.  If \eqn{\Theta} is known (say you are doing a computer simulation and specify \eqn{\Theta}), the standard error can be computed from the variance-covariance matrix of \eqn{\hat{\Theta}}. The latter is computed from the Fisher Information matrix (see \code{\link{MARSShessian}}). Then we use a parametric bootstrap to get the distribution of estimates of \eqn{E[X_t]} from bootstrapped time series of a specific length, number of missing values, and structure. 

The process is: draw \eqn{\hat{\Theta}_b}'s from the variance-covariance matrix, simulate a time series from that \eqn{\hat{\Theta}_b}, compute \code{xtT} (or \code{xtt} or \code{xtt1}), and repeat 1000s of times. The function \code{\link{MARSSsimulate}} will do most of this process for you, everything but running the bootstrap data through \code{\link{MARSSkf}} to get \code{xtT} (or \code{xtt} or \code{xtt1}).  That is the general concept of how one gets the distribution of sample statistics and computes their standard error when working in a frequentist framework with maximum-likelihood parameter estimates (the statistical framework that the MARSS package operates within). From that the distribution of the sample statistics (in our case the xtT's), you can get their standard error at time t, which is the square root of the variance of the estimates at time t.

So that was the case where \eqn{\Theta} is known. If we only have an estimate of \eqn{\Theta} from our one data set, then the above procedure yields the \strong{sample} standard error of the estimate of \eqn{E[X_t]}, i.e. \eqn{E[X_t]|\hat{\Theta}}. 

\strong{Confidence interval} A confidence interval is associated with a specific sample. It is an interval on a statistic (like mean or in our case \eqn{E[X_t]}), computed from a sample. This interval is constructed such that it covers the true value of the statistic for \eqn{1-\alpha} percent of all dat samples. So for our bootstrapped data sets from the true \eqn{\Theta}, we could construct an interval for each bootstrap data set and if constructed correctly, the interval would include \eqn{E[X_t]|\Theta} in \eqn{1-\alpha} percent of the bootstrapped data sets (\eqn{|\Theta} was added to be explicit about which \eqn{E[X_t]} we are talking about). There are an infinite number of ways that you could contruct this interval. The one used overwhelmingly in frequentist statistics is one that has nice compact properties.

How do we construct the confidence interval? MARSS models are in the class of multivariate Gaussian models and we can compute the confidence interval from the sample standard error of the estimate of \eqn{E[X_t]|\hat{\Theta}}. \eqn{E[X_t]} is multivariate. We can compute univariate confidence intervals that apply to only one of the X, we can compute multivariate confidence regions, or we can compute the simultaneous univariate confidence regions. The latter means an interval such that if the interval for one X in \eqn{E[X_t]} covers then all cover (all or none).  

The univariate confidence intervals are the easy ones to compute. Let's use \code{se.xt} for the sample standard error of the estimate of \eqn{E[X_t]|\hat{\Theta}}---the square root of the diagonal of the variance-covariance matrix for the estimates of \eqn{E[X_t]}. The approximate CIs can be computed by \code{qnorm(alpha/2)*se.xt + Ex}, where \code{Ex} is \code{xtT}, \code{xtt} or \code{xtt1} depending on what \eqn{E[X_t]} we are interested in.

Using \code{qnorm()} is an approximation and is treating \eqn{\Theta} as known instead of as an estimate. The Normal approximation does not have heavy enough tails and leads to intervals that are too narrow. In a univariate Normal case, you'd use a t-distribution and \code{qt(alpha/2, df)}. For a MARSS model, the degrees of freedom can be different for different x_t in X_t (different rows) and just depends on your model structure. You could construct bootstrap CIs but that is incredibly slow since you have to bootstrap your bootstraps. This type of Normal approximation for confidence intervals is commonly used for more complex models, but just aware that it does mean that the confidence intervals are a bit narrow. 

\bibliography{./EMDerivation}
\bibliographystyle{apalike}

\end{document}
