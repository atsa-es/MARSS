%\VignetteIndexEntry{Residuals} 
%\VignettePackage{MARSS}
\documentclass[]{article}
%set margins to 1in without fullsty
	\addtolength{\oddsidemargin}{-.875in}
	\addtolength{\evensidemargin}{-.875in}
	\addtolength{\textwidth}{1.75in}

	\addtolength{\topmargin}{-.875in}
	\addtolength{\textheight}{1.75in}
%\usepackage{fullpage} %more standardized margins

% choose options for [] as required from the list
% in the Reference Guide, Sect. 2.2

\usepackage{multirow}
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage[round]{natbib}

% Math stuff
\usepackage{amsmath} % the standard math package
\usepackage{amsfonts} % the standard math package
%%%% bold maths symbol system:
\def\uupsilon{\pmb{\upsilon}}
\def\llambda{\pmb{\lambda}}
\def\bbeta{\pmb{\beta}}
\def\aalpha{\pmb{\alpha}}
\def\zzeta{\pmb{\zeta}}
\def\etaeta{\mbox{\boldmath $\eta$}}
\def\xixi{\mbox{\boldmath $\xi$}}
\def\ep{\mbox{\boldmath $\epsilon$}}
\def\DEL{\mbox{\boldmath $\Delta$}}
\def\PHI{\mbox{\boldmath $\Phi$}}
\def\PI{\mbox{\boldmath $\Pi$}}
\def\LAM{\mbox{\boldmath $\Lambda$}}
\def\LAMm{\mathbb{L}}
\def\GAM{\mbox{\boldmath $\Gamma$}}
\def\OMG{\mbox{\boldmath $\Omega$}}
\def\SI{\mbox{\boldmath $\Sigma$}}
\def\TH{\mbox{\boldmath $\Theta$}}
\def\UPS{\mbox{\boldmath $\Upsilon$}}
\def\XI{\mbox{\boldmath $\Xi$}}
\def\AA{\mbox{$\mathbf A$}}	\def\aa{\mbox{$\mathbf a$}}
\def\Ab{\mbox{$\mathbf D$}} \def\Aa{\mbox{$\mathbf d$}} \def\Am{\PI}
\def\BB{\mbox{$\mathbf B$}}	\def\bb{\mbox{$\mathbf b$}} \def\Bb{\mbox{$\mathbf J$}} \def\Ba{\mbox{$\mathbf L$}} \def\Bm{\UPS}
\def\CC{\mbox{$\mathbf C$}}	\def\cc{\mbox{$\mathbf c$}}
\def\Ca{\Delta} \def\Cb{\GAM}
\def\DD{\mbox{$\mathbf D$}}	\def\dd{\mbox{$\mathbf d$}}
\def\EE{\mbox{$\mathbf E$}}	\def\ee{\mbox{$\mathbf e$}}
\def\E{\,\textup{\textrm{E}}}	
\def\EXy{\,\textup{\textrm{E}}_{\text{{\bf XY}}}}
\def\FF{\mbox{$\mathbf F$}} \def\ff{\mbox{$\mathbf f$}}
\def\GG{\mbox{$\mathbf G$}}	\def\gg{\mbox{$\mathbf g$}}
\def\HH{\mbox{$\mathbf H$}}	\def\hh{\mbox{$\mathbf h$}}
\def\II{\mbox{$\mathbf I$}} \def\ii{\mbox{$\mathbf i$}}
\def\IIm{\mbox{$\mathbf I$}}
\def\JJ{\mbox{$\mathbf J$}}
\def\KK{\mbox{$\mathbf K$}}
\def\LL{\mbox{$\mathbf L$}}	\def\ll{\mbox{$\mathbf l$}}
\def\MM{\mbox{$\mathbf M$}}  \def\mm{\mbox{$\mathbf m$}}
\def\N{\,\textup{\textrm{N}}}
\def\MVN{\,\textup{\textrm{MVN}}}
\def\OO{\mbox{$\mathbf O$}}
\def\PP{\mbox{$\mathbf P$}}  \def\pp{\mbox{$\mathbf p$}}
\def\QQ{\mbox{$\mathbf Q$}}	 \def\qq{\mbox{$\mathbf q$}} \def\Qb{\mbox{$\mathbf G$}}  \def\Qm{\mathbb{Q}}
\def\RR{\mbox{$\mathbf R$}}	 \def\rr{\mbox{$\mathbf r$}} \def\Rb{\mbox{$\mathbf H$}}	\def\Rm{\mathbb{R}}
\def\Ss{\mbox{$\mathbf S$}}
\def\UU{\mbox{$\mathbf U$}}	\def\uu{\mbox{$\mathbf u$}}
\def\Ub{\mbox{$\mathbf C$}} \def\Ua{\mbox{$\mathbf c$}} \def\Um{\UPS}
%\def\VV{\mbox{$\mathbf V$}}	\def\vv{\mbox{$\mathbf v$}}
\def\VV{\mbox{$\pmb{V}$}}	\def\vv{\mbox{$\pmb{v}$}}
%\def\WW{\mbox{$\mathbf W$}}	\def\ww{\mbox{$\mathbf w$}}
\def\WW{\mbox{$\pmb{W}$}}	\def\ww{\mbox{$\pmb{w}$}}
%\def\XX{\mbox{$\mathbf X$}}
\def\XX{\mbox{$\pmb{X}$}}	\def\xx{\mbox{$\pmb{x}$}}
%\def\xx{\mbox{$\mathbf x$}}
%\def\YY{\mbox{$\mathbf Y$}}
\def\YY{\mbox{$\pmb{Y}$}}	\def\yy{\mbox{$\pmb{y}$}}
%\def\yy{\mbox{$\mathbf y$}}
\def\ZZ{\mbox{$\mathbf Z$}}	\def\zz{\mbox{$\mathbf z$}}	\def\Zb{\mbox{$\mathbf M$}} \def\Za{\mbox{$\mathbf N$}} \def\Zm{\XI}
\def\zer{\mbox{\boldmath $0$}}
\def\chol{\,\textup{\textrm{chol}}}
\def\vec{\,\textup{\textrm{vec}}}
\def\var{\,\textup{\textrm{var}}}
\def\cov{\,\textup{\textrm{cov}}}
\def\diag{\,\textup{\textrm{diag}}}
\def\trace{\,\textup{\textrm{trace}}}
\def\hatxt{\widetilde{\xx}_t}
\def\hatxone{\widetilde{\mbox{$\xx$}}_1}
\def\hatxzero{\widetilde{\mbox{$\xx$}}_0}
\def\hatxtm{\widetilde{\mbox{$\xx$}}_{t-1}}
\def\hatxQtm{\widetilde{\mathbb{x}}_{t-1}}
\def\hatyt{\widetilde{\mbox{$\yy$}}_t}
\def\hatyyt{\widetilde{\mbox{$\mathbf y$}\mbox{$\mathbf y$}^\top}_t}
\def\hatyone{\widetilde{\mbox{$\yy$}}_1}
\def\hatOt{\widetilde{\OO}_t}
\def\hatWt{\widetilde{\WW}_t}
\def\hatwt{\widetilde{\mbox{$\ww$}}_t}
\def\hatYXt{\widetilde{\mbox{$\yy\xx$}}_t}
\def\hatXYt{\widetilde{\mbox{$\xx\yy$}}_t}
\def\hatYXttm{\widetilde{\mbox{$\yy\xx$}}_{t,t-1}}
\def\hatPt{\widetilde{\PP}_t}
\def\hatPtm{\widetilde{\PP}_{t-1}}
\def\hatPQtm{\widetilde{\mathbb{P}}_{t-1}}
\def\hatPttm{\widetilde{\PP}_{t,t-1}}
\def\hatPQttm{\widetilde{\mathbb{P}}_{t,t-1}}
\def\hatPtmt{\widetilde{\PP}_{t-1,t}}
\def\hatVt{\widetilde{\VV}_t}
\def\hatVtm{\widetilde{\VV}_{t-1}}
\def\hatVttm{\widetilde{\VV}_{t,t-1}}
\def\hatUt{\widetilde{\UU}_t}
\def\hatSt{\widetilde{\Ss}_t}
\def\hatSttm{\widetilde{\Ss}_{t,t-1}}
\def\hatSttp{\widetilde{\Ss}_{t,t+1}}
\def\hatBmt{\widetilde{\Bm}_t}
\def\hatCat{\widetilde{\Ca}_t}
\def\hatCbt{\widetilde{\Cb}_t}
\def\hatZmt{\widetilde{\Zm}_t}
\def\YYr{\dot{\mbox{$\pmb{Y}$}}}
\def\yyr{\dot{\mbox{$\pmb{y}$}}}
\def\aar{\dot{\mbox{$\mathbf a$}}}
\def\ZZr{\dot{\mbox{$\mathbf Z$}}}
\def\RRr{\dot{\mbox{$\mathbf R$}}}
\def\IR{\nabla}
\usepackage[round]{natbib} % to get references that are like in ecology papers
% \citet{} for inline citation name (year); \citep for citation in parens (name year)

%allow lines in matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\setcounter{tocdepth}{1} %no subsections in toc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\SweaveOpts{concordance=TRUE}
\author{E. E. Holmes\footnote{Northwest Fisheries Science Center, NOAA Fisheries, Seattle, WA 98112, 
       eli.holmes@noaa.gov, http://faculty.washington.edu/eeholmes}}
\title{Computation of Standardized Residuals for MARSS Models}
\maketitle
\begin{abstract}
This report shows how to compute the variance of the joint conditional model and state residuals for multivariate autoregressive Gaussian state-space (MARSS) models. The bulk of the report focuses on 'smoothations', which are the residuals conditioned on all the data $t=1$ to $T$. The final part of the report covers 'innovations', which are residuals conditioned on the data $t=1$ to $t-1$.  

The MARSS model can be written: $\xx_t=\BB\xx_{t-1}+\uu+\ww_t$, $\yy_t=\ZZ\xx_t+\zz+\vv_t$, where $\ww_t$ and $\vv_t$ are multivariate normal error-terms with variance-covariance matrices $\QQ$ and $\RR$ respectively. The joint conditional residuals are the $\ww_t$ and $\vv_t$ conditioned on the observed data, which may be incomplete (missing values). Harvey, Koopman and Penzer (1998) show a recursive algorithm for the smoothation residuals (conditioned on all the data). I show an alternate way to compute these residuals using the conditional variances of the states and the conditional covariance between unobserved data and states. This allows one to compute the variance of un-observed residuals (residuals associated with missing data), which is needed for leave-one-out cross-validation tests and to compute the conditional variance of the missing data. I show how to modify the Harvey et al. algorithm in the case of missing values and how to modify it to return the non-normalized conditional residuals. The main report is on the smoothation residuals (conditioned on all the data). The final section shows the innovations residuals (conditioned on the data up to $t-1$).
\end{abstract}
Keywords: Time-series analysis, Kalman filter, residuals, maximum-likelihood, vector autoregressive model, dynamic linear model, parameter estimation, state-space
\vfill
{\noindent \small citation: Holmes, E. E. 2014. Computation of standardized residuals for (MARSS) models. Technical Report. arXiv:1411.0045 }
 \newpage
 
\section{Overview}

This report discusses the computation of the variance of the conditional model (and state) residuals for MARSS models of the  form:
\begin{equation}\label{eq:residsMARSS}
\begin{gathered}
\xx_t = \BB_t\xx_{t-1} + \uu_t + \ww_t, \text{ where } \WW_t \sim \MVN(0,\QQ_t)\\
\yy_t = \ZZ_t\xx_t + \aa_t + \vv_t, \text{ where } \VV_t \sim \MVN(0,\RR_t)\\
\XX_0 \sim \MVN(\xixi,\LAM)
\end{gathered}
\end{equation}

Given a set of observed data $\yy_t$ and states $\xx_t$, the model residuals are $\yy_t - (\ZZ_t \xx_t + \aa_t)=\vv_t$.  The model residual is a random variable since $\yy_t$ and $\xx_t$ are drawn from the joint multivariate distribution of $\YY_t$ and $\XX_t$ defined by the MARSS equation.
The unconditional\footnote{meaning not conditioning on any particular set of observed data but rather taking the expectation across all possible values of $\yy_t$ and $\xx_t$.} variance of the model residuals is
\begin{equation}\label{eqn:unconditiondistofVt}
\var_{XY}[\YY_t - (\ZZ_t \XX_t + \aa_t)] = \var[\VV_t] = \RR_t\\
\end{equation}
based on the definition of $\VV_t$.  $\var_{XY}$ indicates that the integration is over the joint-unconditional distribution of $\XX$ and $\YY$ (over all time not just $t$).

Once we have data, $\RR_t$ is not the variance of our model residuals because our residuals are now conditioned\footnote{`conditioned' means that the probability distribution of the residual has changed. The distribution is now the distribution given that $\YY(1)=\yy(1)$. Expectations $\E[ ]$ and variances $\var[ ]$ are integrals over the value that a random variable might take multiplied by the probability of that value. When presenting an   `expectation', the probability distribution is normally implicit but for derivations involving conditional expectiations, it is important to be explicit about the distribution that is being integrated over.} on a set of observed data. There are two types of conditional model residuals used in MARSS analyses: innovations and smoothations.  Innovations are the model residuals at time $t$ using the expected value of $\XX_t$ conditioned on the data from 1 to $t-1$.  Smoothations  are the model residuals using the expected value of $\XX_t$ conditioned on all the data, $t=1$ to $T$.  Smoothations are used in computing standardized residuals for outlier and structural break detection \citep{Harveyetal1998, deJongPenzer1998, CommandeurKoopman2007}.  

\section{Distribution of the MARSS smoothation residuals}\label{sec:smoothations}

This section discusses computation of the variance of the model and state residuals conditioned on all the data from $t=1$ to $T$.  MARSS residuals are often used for outlier detection and shock detection, and in this case you only need the distribution of the model residuals for the observed values.  However if you wanted to do a leave-one-out cross-validation, you would need to know the distribution of the residuals for data points you left out (treated as unobserved).  The equations in this report give you the former and the latter, while the algorithm by \citet{Harveyetal1998} gives only the former.

Throughout, I follow the convention that capital letters are random variables and small letters are a realization from the random variable.  This only applies to random variables; parameters are not random variables\footnote{in a frequentist framework}. In this report, the distribution over which the integration is done in an expectation $\E[ ]$ or variance $\var[ ]$ is given by the subscript, e.g. $\E_A[ ]$ indicates an unconditional expectation over the distribution of $\AA$ without conditioning on another random variable while $\E_{A|b}[ ]$ would indicate an expectation over the distribution of $\AA$ conditioned on $\BB=\bb$. A joint expectation of $f(\AA,\BB)$ would be $\E_{AB}[f(\AA,\BB)]$, where 'AB' denotes the joint probability distribution. This can be written in terms of the conditional expectation using the law of total expectation $\E_B[\E_{A|b}[f(\AA,\BB)|\BB=\bb]]$.

\subsection{Model residuals conditioned on all the data}

Define the smoothations $\hat{\vv}_t$ as:
\begin{equation}\label{eq:vtT}
\hat{\vv}_t = \yy_t - \ZZ_t\hatxt - \aa_t,
\end{equation}
where  $\hatxt$ is $\E[\XX_t|\yy^{(1)}]$ and is output by the Kalman smoother. $\yy^{(1)}$ means all the observed data from $t=1$ to $T$. $\yy^{(1)}$ is a sample from the random variable $\YY^{(1)}$. The unobserved $\yy$ will be termed $\yy^{(2)}$ and is a sample from the random variable $\YY^{(2)}$. When $\YY$ appears without a superscript, it means both $\YY^{(1)}$ and $\YY^{(2)}$ together. Similarly $\yy$ means both $\yy^{(1)}$ and $\yy^{(2)}$ together---the observed data that we use to estimate $\hatxt$ and the unobserved data that we do not use and may or may not know.

$\hat{\vv}_t$ exists for both $\yy^{(1)}$ and $\yy^{(2)}$. $\hat{\vv}_t$ is sample from the random variable $\hat{\VV}_t$ since $\YY^{(1)}$ is a random variable and the data we have collected $\yy^{(1)}$ is a sample from that.  We want to compute the unconditional mean and variance of this random variable $\hat{\VV}_t$; unconditional here means we take the expectations over all possibles values that $\yy$, both $\yy^{(1)}$ and $\yy^{(2)}$, might take.  The mean is 0 and we are concerned only with computing the variance:
\begin{equation}\label{eq:var.vtT}
\var[\hat{\VV}_t] = \var_{Y^{(1)}}[\YY_t - \ZZ_t\E[\XX_t|\YY^{(1)}=\yy^{(1)}] - \aa_t]
\end{equation}
Notice we have an unconditional variance over $\YY^{(1)}$ on the outside (that subscript on $\var$) and a conditional variance over a specific value of $\YY^{(1)}$ on the inside (in the $\E[]$).

To compute this, I will use the ``law of total variance'':
\begin{equation}
\var[A] = \var_B[\E_{A|b}[A|B=b]] + \E_B[\var_{A|b}[A|B=b]]
\end{equation}
The subscripts on the inner expectations make it explicit that the expectations are being taken over the conditional distributions.  However, going forward, I will write this more succinctly as
\begin{equation}
\var[A] = \var_B[\E[A|b]] + \E_B[\var[A|b]]
\end{equation}
It is understood that $\E[A|b]$ is the conditional expectation of $A$ conditioned on $B=b$ and $\var[A|b]$ is the conditional variance\footnote{Normally the law of total variance is written $\var[A] = \var[\E[A|B]] + \E[\var[A|B]]$}.

From the law of total variance, we can write
\begin{equation}\label{varvvtgeneral}
\var[\hat{\VV}_t] = \var_{Y^{(1)}}[\E[\hat{\VV}_t|\yy^{(1)}]] + \E_{Y^{(1)}}[\var[\hat{\VV}_t|\yy^{(1)}]]
\end{equation}
The $\var$ and $\E$ are expectations over  all possible values of $\YY^{(1)}$.

\subsubsection{First term in Equation \ref{varvvtgeneral}}

Notice that 
\begin{equation}
\E[\hat{\VV}_t|\yy^{(1)}]=\E[\YY_t|\yy^{(1)}] - \ZZ_t\E[\XX_t|\yy^{(1)}] - \aa_t \\
=(\YY_t + \ZZ_t \XX_t + \aa_t)|\yy^{(1)} = \E[\VV_t|\yy^{(1)}] 
\end{equation}
So the first term is $\var_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}]]$.  

From the law of total variance, we can write
\begin{equation}\label{eqn:varianceVt}
\var[\VV_t] = \var_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}]] + \E_{Y^{(1)}}[\var[\VV_t|\yy^{(1)}]]
\end{equation}
From Equation \ref{eqn:varianceVt}, we can solve for $\var_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}]]$:
\begin{equation}
\var_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}]] = \var[\VV_t] - \E_{Y^{(1)}}[\var[\VV_t|\yy^{(1)}]]
\end{equation}
From Equation \ref{eqn:unconditiondistofVt}, $\var[\VV_t]=\RR_t$.  The second term to the right of the equal sign, $\var[\VV_t|\yy^{(1)}]$, is the variance of $\VV_t$ holding $\yy^{(1)}$ fixed but allowing $\XX_t$ (and the rest of the $\XX$) to be random variables:
\begin{equation}\label{eqn:varvtcondy}
\var[\VV_t|\yy^{(1)}] = \var[ \YY_t - \ZZ_t\XX_t-\aa_t | \yy^{(1)} ].
\end{equation}
$\aa_t$ is a fixed value and can be dropped. 

Equation \ref{eqn:varvtcondy} can then be written as\footnote{If there were no missing data, i.e. if $\yy^{(1)}=\yy$, then $\hatUt$ and $\hatSt$ would be zero and this would reduce to $\ZZ_t \hatVt \ZZ_t^\top$. But we are concerned with the case where there are missing values. Those missing values need not be for all $t$. That is, there may be some observed $y$ at time t and some missing $y$. $\yy_t$ is multivariate.}
\begin{equation}
\begin{split}
\var[\VV_t|\yy^{(1)}] &= \var[ \YY_t - \ZZ_t\XX_t | \yy^{(1)} ]\\
&=\var[ - \ZZ_t\XX_t | \yy^{(1)} ] + \var[ \YY_t|\yy^{(1)}] + \cov[ \YY_t, - \ZZ_t\XX_t | \yy^{(1)} ] + \cov[ - \ZZ_t\XX_t, \YY_t | \yy^{(1)} ]\\
&=\ZZ_t \hatVt \ZZ_t^\top + \hatUt - \hatSt\ZZ_t^\top - \ZZ_t\hatSt^\top
\end{split}
\end{equation}
$\hatVt = \var[ \XX_t | \yy^{(1)} ]$ and is output by the Kalman smoother. $\hatUt=\var[\YY_t|\yy^{(1)}]$ and $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$. The equations for these are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package\footnote{$\hatUt$ is \texttt{ytT} and $\hatSt$ is \texttt{OtT}-\texttt{ytT \%*\% t(ytT)} in the \texttt{MARSShatyt} output.}.

$\hatVt$, $\hatUt$ and $\hatSt$ are conditional multivariate Normal variances and their values do not depend on the values of $\yy^{(1)}$.  They only depend on the parameters values, $\QQ$, $\BB$, $\RR$, etc., in the MARSS equation. Thus $\E_{Y^{(1)}}[\var[\VV_t|\yy^{(1)}]] = \var[\VV_t|\yy^{(1)}]$. Putting this all together, we can write the variance of $\hat{\VV}_t$ as
\begin{equation}\label{eqn:conditionalvtfinal}
\var_{Y^{(1)}}[\E[\hat{\VV}_t|\yy^{(1)})]]  = \var[\VV_t] - \var[\VV_t|\yy^{(1)}] = \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top
\end{equation}

\subsubsection{Second term in Equation \ref{varvvtgeneral}}

Consider the second term in Equation \ref{varvvtgeneral}.  This term is 
\begin{equation}
\E_{Y^{(1)}}[\var[(\YY_t-\ZZ_t\E[\XX_t|\yy^{(1)}]-\aa_t)|\yy^{(1)}]]
\end{equation}
$\E[\XX_t|\yy^{(1)}]$ is a fixed value; it is not $\XX_t$ but its expected value. $\aa_t$ is also fixed. Thus the second term reduces to $\E_{Y^{(1)}}[\var[\YY|\yy^{(1)}]]=\E_{Y^{(1)}}[\hatUt]$.  
$\hatUt$ is a variance and is not a function of $\yy^{(1)}}$. It is only a function of the MARSS parameters.  Thus the second term in Equation \ref{varvvtgeneral} is simply $\hatUt$.

\subsubsection{Putting together the two terms}
\begin{equation}
\begin{split}
\var[\hat{\VV}_t] &= \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top + \hatUt\\
&= \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top
\end{split}
\end{equation}
This will reduce to $\RR_t - \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ has no missing values and to $\RR_t + \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ is all missing values. The behavior if $\yy_t$ has some missing and some not missing values depends on whether $\RR_t$ is a diagonal matrix or not (i.e. if the $\yy_t^{(1)}$ and $\yy_t^{(2)}$ are correlated).

\subsection{State residuals conditioned on the data}

The state residuals are $\xx_t - (\BB_t \xx_{t-1} + \uu_t)=\ww_t$.  The unconditional expected value of the state residuals is $\E[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \E[\WW_t] = 0$ and the unconditional variance of the state residuals is
\begin{equation}
\var[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \var[\WW_t] = \QQ_t
\end{equation}
based on the definition of $\WW_t$.
The conditional state residuals (conditioned on the data from $t=1$ to $t=T$) are defined as
\begin{equation}
\hat{\ww}_t = \hatxt - \BB_t\hatxtm - \uu_t.
\end{equation}
It is a sample from the random variable $\hat{\WW}_t$; random over different possible data sets.  The expected value of $\hat{\WW}_t$ is 0, and we can compute $\var_{Y^{(1)}}[\hat{\WW}_t]$ from the law of total variance.
\begin{equation}
\var[\WW_t] = \var_{Y^{(1)}}[\E[\WW_t|\yy^{(1)}]] + \E_{Y^{(1)}}[\var[\WW_t|\yy^{(1)}]]
\end{equation}
Using the observation that $\hat{\WW}_t=\E[\WW_t|\YY^{(1)}=\yy^{(1)}]$, we can write:
\begin{equation}
\var[\WW_t] = \var_{Y^{(1)}}[\hat{\WW}_t] + \E_{Y^{(1)}}[\var[\WW_t|\yy^{(1)}]]
\end{equation}
Solve for $\var_{Y^{(1)}}[\hat{\WW}_t]$ to get:
\begin{equation}\label{eqn:varwwt}
\var_{Y^{(1)}}[\hat{\WW}_t] = \var[\WW_t] - \E_{Y^{(1)}}[\var[\WW_t|\yy^{(1)}]]
\end{equation}

The variance in the expectation on the far right is
\begin{equation}
\var[\WW_t|\yy^{(1)}] = \var[ (\XX_t - \BB_t\XX_{t-1}-\uu_t) | \yy^{(1)} ]
\end{equation}
$\uu_t$  is not a random variable and can be dropped. Thus,
\begin{equation}
\begin{split}
\var[\WW_t&|\yy^{(1)}] = \var[ (\XX_t - \BB_t\XX_{t-1}) | \yy^{(1)} ] \\
& = \var[ \XX_t | \yy^{(1)} ] + \var[\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[\XX_t, -\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[ -\BB_t\XX_{t-1}, \XX_t | \yy^{(1)} ]\\
& = \hatVt + \BB_t \hatVtm \BB_t^\top - \hatVttm \BB_t^\top - \BB_t\widetilde{\VV}_{t-1,t}
\end{split}
\end{equation}
This conditional variance does not depend on the actual values of $\yy$.  It depends only on the parameters values, $\QQ$, $\BB$, $\RR$, etc.
Putting together the above and $\var[\WW_t]=\QQ_t$ into Equation \ref{eqn:varwwt}, the variance of the conditional state residuals is
\begin{equation}
\var_{Y^{(1)}}[\hat{\WW}_t]  =  \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t}
\end{equation}

\subsection{Covariance of the conditional model and state residuals}
The unconditional model and state residuals, $\VV_t$ and $\WW_t$, are independent (by definition), i.e. $\cov[\VV_t,\WW_t]=0$.  However the conditional model and state residuals, $\cov[\hat{\VV}_t,\hat{\WW}_t]$, are not independent since both depend on $\yy^{(1)}$.  
Using the law of total covariance, we can write
\begin{equation}\label{eqn:covhatVtWt1}
\cov[\hat{\VV}_t,\hat{\WW}_t] = 
\cov_{Y^{(1)}}[\E[\hat{\VV}_t|\yy^{(1)}],\E[\hat{\WW}_t|\yy^{(1)}]] + \E_{Y^{(1)}}[\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]]
\end{equation}
The covariance in the second term on the right can be written out as
\begin{equation}
\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}] =  \E_{Y^{(1)}}[\cov[\YY_t-\ZZ_t\E[\XX_t|\yy^{(1)}]-\aa_t, \E[\XX_{t-1}|\yy^{(1)}]-\BB_t\E[\XX_{t-1}|\yy^{(1)}]-\uu_t|\yy^{(1)}]]
\end{equation}
The $\E[\XX_t|\yy^{(1)}]$ are fixed values for a given set of data. The covariance of a random variable with a fixed value is 0, thus $\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]$ is 0.  Thus Equation \ref{eqn:covhatVtWt1} reduces to
\begin{equation}\label{eqn:covhatVtWt2}
\cov[\hat{\VV}_t,\hat{\WW}_t] = \cov_{Y^{(1)}}[\E[\hat{\VV}_t|\yy^{(1)}],\E[\hat{\WW}_t|\yy^{(1)}]] + 0 = \cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]
\end{equation}
since $\E[\hat{\VV}_t|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}]$ and $\E[\hat{\WW}_t|\yy^{(1)}]=\E[\WW_t|\yy^{(1)}]$.

In the same way we used the law of total variance, we can use the law of total covariance  to obtain $\cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]$:
\begin{equation}\label{eqn:covhatVtWt3}
\cov[\VV_t, \WW_t] = \E_{Y^{(1)}}[\cov[\VV_t, \WW_t|\yy^{(1)}]] + \cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]\\ 
\end{equation}
The unconditional covariance of $\VV_t$ and $\WW_t$ is 0. Thus the right side of Equation \ref{eqn:covhatVtWt3} is 0 and combining Equation \ref{eqn:covhatVtWt2} and \ref{eqn:covhatVtWt3}, we get
\begin{equation}\label{eqn:conditionalcovvtwt}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\yy^{(1)}] ] 
\end{equation}
and our problem reduces to solving for the conditional covariance of the model and state residuals.  

The conditional covariance $\cov[\VV_t, \WW_t|\yy^{(1)}]$ can be written out as
\begin{equation}
\cov[\VV_t, \WW_t|\yy^{(1)}] = \cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_t-\BB_t\XX_{t-1}-\uu_t|\yy^{(1)}]
\end{equation}
$\aa_t$ and $\uu_t$ are fixed values and can be dropped. Thus
\begin{equation}
\begin{split}
\cov&[\VV_t, \WW_t|\yy^{(1)}] =\cov[\YY_t-\ZZ_t\XX_t, \XX_t-\BB_t\XX_{t-1}|\yy^{(1)}] \\
& =\cov[\YY_t,\XX_t|\yy^{(1)}] + \cov[\YY_t,-\BB_t\XX_{t-1}|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,\XX_t|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,-\BB_t\XX_{t-1}|\yy^{(1)}]\\
& = \hatSt - \hatSttm\BB_t^\top - \ZZ_t\hatVt + \ZZ_t\hatVttm\BB_t^\top
\end{split}
\end{equation}
where $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$ and $\hatSttm=\cov[\YY_t,\XX_{t-1}|\yy^{(1)}]$; the equations for $\hatSt$ and $\hatSttm$ are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package.

$\hatVt$, $\hatVttm$, $\hatSt$ and $\hatSttm$ are only functions of the MARSS parameters not of $\yy^{(1)}$.  Thus 
\begin{equation}
\E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\yy^{(1)}] ]= \cov[\VV_t, \WW_t|\yy^{(1)}] = \hatSt - \hatSttm\BB_t^\top + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt
\end{equation}
$\cov[\hat{\VV}_t,\hat{\WW}_t]$ is the negative of this (Equation \ref{eqn:conditionalcovvtwt}), thus
\begin{equation}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \hatSt + \hatSttm\BB_t^\top - \ZZ_t\hatVttm\BB_t^\top + \ZZ_t\hatVt
\end{equation}

The Harvey et al. algorithm shown below gives the joint distribution of the model residuals at time $t$ and state residuals at time $t+1$.  Using the law of total covariance as above, the covariance in this case is
\begin{equation}
\cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_{t+1}|\yy^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\yy^{(1)}] ] 
\end{equation}
and
\begin{equation}
\begin{split}
\cov[\VV_t, \WW_{t+1}|\yy^{(1)}] & =\cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_{t+1}-\BB_{t+1}\XX_t-\uu_{t+1}|\yy^{(1)}] \\
& =\cov[\YY_t-\ZZ_t\XX_t, \XX_{t+1}-\BB_{t+1}\XX_t|\yy^{(1)}] \\
& = \hatSttp - \hatSt\BB_{t+1}^\top - \ZZ_t\widetilde{\VV}_{t,t+1} + \ZZ_t\hatVt\BB_{t+1}^\top
\end{split}
\end{equation}
Thus,
$$\cov_{Y^{(1)}}[\E[\VV_t|\yy],\E[\WW_{t+1}|\yy^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\yy^{(1)}] ] = - \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top.$$

\subsection{Joint distribution of the conditional residuals}
We now the write the variance of the joint distribution of the conditional residuals. Define
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_t\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \hatxt - \BB_t\hatxtm-\uu_t \end{bmatrix}.
\end{equation}
where $\hatxt$ and $\hatxtm$ are conditioned on $\yy{(1)}$, the observed $\yy$.
$\hat{\varepsilon}_t$ is a sample drawn from the distribution of $\hat{\mathcal{E}}_t$ conditioned on observations at the $(1)$ locations in $\YY$.  The expected value of $\hat{\mathcal{E}}_t$ over all possible $\yy$ is 0 and the variance of $\hat{\mathcal{E}}_t$  is
\begin{equation}\label{eqn:jointcondresid1general}
 \begin{bmatrix}[c|c]
 \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top+\ZZ_t\hatSt^\top&
 \hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt \\
 \rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
 (\hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt)^\top& 
 \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t} \end{bmatrix}
\end{equation}

If the residuals are defined as in \citet{Harveyetal1998},
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \tilde{\xx}_{t+1} - \BB_{t+1}\hatxt-\uu_{t+1} \end{bmatrix}
\end{equation}
and the variance of $\hat{\mathcal{E}}_t$ is
\begin{equation}\label{eqn:jointcondresid2}
\begin{bmatrix}[c|c]
\RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top&
- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top \\
\rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
(- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top)^\top& 
\QQ_{t+1} - \widetilde{\VV}_{t+1} - \BB_{t+1} \hatVt \BB_{t+1}^\top + \widetilde{\VV}_{t+1,t} \BB_{t+1}^\top + \BB_{t+1}\widetilde{\VV}_{t,t+1} \end{bmatrix}
\end{equation}

The above gives the variance of both `observed' model residuals (the ones associated with $\yy^{(1)}$) and the unobserved model residuals (the ones associated with $\yy^{(2)}$).  
When there are no missing values in $\yy_t$, the $\hatSt$ and $\hatSttm$ terms equal 0 and drop out.

\section{Harvey et al. 1998 algorithm for the conditional residuals}
\citet[pgs 112-113]{Harveyetal1998} give a recursive algorithm for computing the variance of the conditional residuals when the time-varying MARSS equation is written as: 
\begin{equation}\label{eqn:residsMARSSHarvey}
\begin{gathered}
\xx_{t+1} = \BB_{t+1}\xx_t + \uu_{t+1} + \GG_{t+1}\epsilon_{t},\\
\yy_t = \ZZ_t\xx_t + \aa_t + \HH_t\epsilon_t,\\
\mbox{ where } \epsilon_t \sim \MVN(0,\II_{m+n \times m+n}), \GG_t\GG_t^\top=\QQ_t\text{ and }\HH_t\HH_t^\top=\RR_t
\end{gathered}
\end{equation}
$\GG_t$ has $m$  rows and $m+n$ columns with the last $n$ columns all 0; $\HH_t$ has $n$ rows and $m+n$ columns with the last $m$ columns all zero.  The algorithm in \citet{Harveyetal1998} gives the variance of the `normalized' residuals, the $\epsilon_t$.  I have modified their algorithm so it  returns the `non-normalized' residuals:
$$\varepsilon_t=\begin{bmatrix}\HH_t\epsilon_t\\ \GG_{t+1}\epsilon_t\end{bmatrix}=\begin{bmatrix}\vv_t\\ \ww_{t+1} \end{bmatrix}.$$

The Harvey et al. algorithm is a backwards recursion using the following output from the Kalman filter: the one-step ahead prediction covariance $\FF_t$, the Kalman gain $\KK_t$, and $\widetilde{\VV}_t^{t-1}=\var[\XX_t|\yy^{(1,1:{t-1}}]$. These are output from \texttt{MARSSkfss()} in \texttt{Sigma}, \texttt{Kt}, and \texttt{Vtt1}.

\section{Algorithm}

Start from $t=T$ and work backwards to $t=1$. At time $T$, $r_T=0_{1 \times m}$ and $N_T=0_{m \times m}$. $\BB_{t+1}$ and $\QQ_{t+1}$ can be set to NA or 0. They will not appear in the algorithm at time $T$ since $r_T=0$ and $N_T=0$. Note that the $\ww$ residual and its associated variance and covariance with $\vv$ at time $T$ is NA since this residual would be for $\xx_T$ to $\xx_{T+1}$.

% algorithm with \GG_t and \HH_t
% \begin{equation}\label{eqn:Harveyalgo}
% \begin{gathered}
% \FF_t = \ZZ_t\hatVt\ZZ_t^\top+\RR_t\\
% G_t= \GG_t\QQ_t\GG_t^\top, \mbox{  }H_t = \HH_t\RR_t\HH_t^\top,  \mbox{ } K_t = \BB_t\KK^{*}_t\\
% L_t = \BB_t - K_t\ZZ_t, \mbox{ } J_t= H_t - K_t G_t, \mbox{ } u_t = \FF_t^{-1} - K_t^\top r_t\\
% r_{t-1} = \ZZ_t^\top u_t + \BB_t^\top r_t, \mbox{ } N_{t-1} = K_t^\top N_t K_t + L_t^\top N_t L_t
% \end{gathered}
% \end{equation}

\begin{equation}\label{eqn:Harveyalgo}
\begin{gathered}
\QQ^*_{t+1}=\begin{bmatrix}\QQ_{t+1}&0_{m \times n}\end{bmatrix}, \mbox{    } \RR^*_t=\begin{bmatrix}0_{n \times m}&\RR_t^*\end{bmatrix}\\
\FF_t = \ZZ_t^*\widetilde{\VV}_t^{t-1}{\ZZ_t^*}^\top+\RR_t^*\mbox{,   } K_t = \BB_{t+1}\KK_t = \BB_{t+1} \widetilde{\VV}_t^{t-1}{\ZZ_t^*}^\top \FF_t^{-1}  \\
L_t = \BB_{t+1} - K_t\ZZ_t^*, \mbox{    } J_t= \QQ^*_{t+1} - K_t \RR^*_t, \mbox{    } u_t = \FF_t^{-1} \hat{\vv}_t - K_t^\top r_t\\
r_{t-1} = {\ZZ_t^*}^\top u_t + \BB_{t+1}^\top r_t, \mbox{    } N_{t-1} = {\ZZ_t^*}^\top \FF_t^{-1} \ZZ_t^* + L_t^\top N_t L_t \\
\mtext{repeat for }$t-1$
\end{gathered}
\end{equation}
Bolded terms are the same as in Equation \ref{eqn:residsMARSSHarvey} or output by \texttt{MARSSkfss()}.  Unbolded terms are terms used in \citet{Harveyetal1998}.  The * on $\ZZ_t$ and $\RR_t$, indicates that they are the missing value modified versions  discussed in \citet[section 6.4]{ShumwayStoffer2006} and \citet{Holmes2012}: the rows of $\ZZ_t$ corresponding to missing rows of $\yy_t$ are set to zero and the $(i,j)$ and $(j,i)$ terms of $\RR_t$ corresponding the missing rows of $\yy_t$ are set to zero.  For the latter, this means if the $i$-th row of $\yy_t$ is missing, then then all the $(i,j)$ and $(j,i)$ terms, including $(i,i)$ are set to 0. It is assumed that a missing values modified inverse of $\FF_t$ is used: any 0's on the diagonal of $\FF_t$ are replaced with 1, the inverse is taken, and 1s on diagonals is replaced back with 0s.

The residuals \citep[eqn 24]{Harveyetal1998} are
\begin{equation}\label{eqn:Harveyresiduals}
\hat{\varepsilon}^*_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =({\RR^*_t})^\top u_t + ({\QQ^*_{t+1}})^\top r_t
\end{equation}
with mean of 0 ($\E_{Y^{(1)}}(\hat{\varepsilon}_t)=0$) and variance
\begin{equation}\label{eqn:Harveyvariance}
\Sigma_t^* = \var_{Y^{(1)}}(\hat{\varepsilon}_t) ={\RR^*_t}^\top \FF_t^{-1} \RR^*_t + J_t^\top N_t J_t
\end{equation}
The * signifies that these are the missing values modified $\hat{\varepsilon}_t$ and $\Sigma_t$; see comments above.

\subsection{Difference in notation}
In Equation 20 in \citet{Harveyetal1998}, their $T_t$ is my $\BB_{t+1}$ and their $H_t H_t^\top$ is my $\QQ_{t+1}$.  Notice the difference in the time indexing. My time indexing on $\BB$ and $\QQ$ matches the left $\xx$ while in theirs, $T$ and $H$ indexing matches the right $\xx$. Thus in my implementation of their algorithm \citep[eqns. 21-24]{Harveyetal1998}, $\BB_{t+1}$ appears in place of $T_t$ and $\QQ_{t+1}$ appears in place of $H_t$. See comments below on normalization and the difference between $\QQ$ and $H$. 

\citet[eqns. 19, 20]{Harveyetal1998} use $G_t$ to refer to the $\chol(\RR_t)^\top$ (essentially) and $H_t$ to refer to $\chol(\QQ_t)^\top$.  I have replaced these with $\RR_t^*$ and $\QQ_t^*$, respectively, which causes my variant of their algorithm (Equation \ref{eqn:Harveyalgo}) to give the `non-normalized' variance of the residuals. The residuals function in the MARSS package has an option to give either normalized or non-normalized residuals.

$\KK_t$ is the Kalman gain output by the MARSS package.  The Kalman gain as used in the \citet{Harveyetal1998} algorithm is $K_t=\BB_{t+1}\KK_t$. Notice that Equation 21 in \citet{Harveyetal1998} has $H_t G_t^\top$ in the equation for $K_t$ (Kalman gain). This is the covariance of the state and observation errors, which is allowed given the way they write the errors in their Equations 19 and 20. The way the MARSS package model is written, the state and observation errors are independent of each other. Thus $H_t G_t^\top = 0$ and this term drops out of the $\KK_t$ equation.

\subsection{Computing the standardized residuals}
The standardized residuals are computed by multiplying $\hat{\varepsilon}_t$ by the inverse of the square root of the variance-covariance matrix from which $\hat{\varepsilon}_t$ is ``drawn'':
\begin{equation}
(\Sigma_t^*)^{-1/2}\hat{\varepsilon}_t^*
\end{equation}
Notice that the missing values modified $\hat{\varepsilon}_t^*$ and $\Sigma_t^*$ are used. if the $i$-th row of $\yy_t$ is missing, the $i$-th row of $\hat{\varepsilon}_t$ is set to 0 and the $i$-th row and column of $\Sigma_t$ is set to all 0.
There will be 0s on the diagonal of $\Sigma_t^*$ so your code will need to deal with these.

\section{Distribution of the MARSS innovation residuals}

One-step-ahead predictions (innovations) are often shown for MARSS models and these are used for likelihood calculations. Innovations are the prediction of $\mathbf{y}$ given data up to $t-1$. Although not termed 'innovations', we can also compute the predictions of $\mathbf{x}$ conditioned on the data up the $t-1$. This section gives the residual variance for these one-step-ahead predictions. I walk through the derivation of the variance matrix very pendantically following the procedures in Section \ref{sec:smoothations}. In texts on state-space models, this derivation would be written simply as 'it follows obviously that' because in this case the $\yy_t$ is independent of the model preditions (which use only data up to $t-1$) and the variance-covariance matrix is indeed 'obvious'.

\subsubsection{Variance of the one-step-ahead model residuals}

Define the innovations $\hat{\vv}_t$ as:
\begin{equation}\label{eq:vtt1}
\hat{\vv}_t = \yy_t - \ZZ_t\hatxt - \aa_t,
\end{equation}
where  $\hatxt$ is now $\E[\XX_t|\yy^{(1)}_{1:t-1}]$ and is output by the Kalman filter. The difference between the smoothation (Equation \ref{eq:vtT}) and innovation is that the former is conditioned on all the data $t=1$ to $T$ and the latter is conditioned on $t=1$ to $t-1$ ($\yy^{(1)}_{1:t-1}$). As before $(1)$ denotes the observed data (non-missing) and $(2)$ denotes the missing data.  $\hat{\vv}_t$ is sample from the random variable $\hat{\VV}_t$ since $\YY^{(1)}_{1:t-1}$ is a random variable and the data we have collected $\yy^{(1)}_{1:t-1}$ is a sample from that. 

As before, we want to compute the unconditional mean and variance of this random variable $\hat{\VV}_t$; unconditional here means we take the expectations over all possibles values that $\yy_{1:t-1}$, both $\yy^{(1)}_{1:t-1}$ and $\yy^{(2)}_{1:t-1}$, might take:
\begin{equation}\label{eq:var.vtt1}
\var[\hat{\VV}_t] = \var_{Y_{1:t-1}}[\YY_t - \ZZ_t\E[\XX_t|\yy^{(1)}_{1:t-1}] - \aa_t]
\end{equation}
While this looks similar to the smoothation case (Equation \ref{eq:var.vtT}), it is different in that $\yy_t$ does not appear in $\E[\XX_t|\yy^{(1)}_{1:t-1}]$; the conditioning is only on data up to $t-1$. Looking at Equation \ref{eq:var.vtt1}, you might guess that $\var[\hat{\VV}_t] = \RR_t - \ZZ_t\var[ \XX_t | \yy^{(1)}_{1:t-1}] ]\ZZ_t^\top$ but that is not the case. The "-" should be a "+". Let's walk throught the derivation to see why that is.

As before, I will use the ``law of total variance'':
\begin{equation}
\var_A[A] = \var_B[\E_{A|b}[A|b]] + \E_B[\var_{A|b}[A|b]]
\end{equation}
which again I write more succinctly as
\begin{equation}
\var_A[A] = \var_B[\E[A|b]] + \E_B[\var[A|b]]
\end{equation}
From the law of total variance , we can write
\begin{equation}\label{var.vvt1.totalvar}
\var_{V_t}[\hat{\VV}_t] = \var_{Y_{1:t-1}}[\E[\hat{\VV}_t|\yy^{(1)}_{1:t-1}]] + \E_{Y_{1:t-1}}[\var[\hat{\VV}_t|\yy^{(1)}_{1:t-1}]]
\end{equation}
$\var_{Y_{1:t-1}}$ and $\E_{Y_{1:t-1}}$ are expectations over both $\YY^{(1)}_{1:t-1}$ and $\YY^{(2)}_{1:t-1}$, so all possible values of $\YY_{1:t-1}$. Notice that we are not conditioning on any data after $t-1$.

\subsubsection{First term in Equation \ref{var.vvt1.totalvar}}

Because $\yy_t$ does not appear in the conditional of $\E[\hat{\VV}_t|\yy^{(1)}_{1:t-1}]$, it may be obvious that this is equal to $\E[\hat{\VV}_t]$ and thus 0. However, let's walk through this since it is due to the structure of the model not a general property.

Using the equation for $\hat{\vv}_t$ (Equation \ref{eq:vtt1}), we can write
\begin{equation}\label{eq:Evtt1}
\E[\hat{\VV}_t|\yy^{(1)}_{1:t-1}]=\E[\YY_t|\yy^{(1)}_{1:t-1}] - \ZZ_t\E[\XX_t|\yy^{(1)}_{1:t-1}] - \aa_t
\end{equation}
Because $\yy_t$ does not appear in $\E[\YY_t|\yy^{(1)}_{1:t-1}]$,
\begin{equation}
\E[\YY_t|\yy^{(1)}_{1:t-1}] = \ZZ_t\E[\XX_t|\yy^{(1)}_{1:t-1}] + \aa_t
\end{equation}
So the right side of Equation \ref{eq:Evtt1} is 0 and the first term in Equation \ref{var.vvt1.totalvar} is 0:
\begin{equation}
\var_{Y_{1:t-1}}[\E[\hat{\VV}_t|\yy^{(1)}_{1:t-1}]] = 0
\end{equation}

\subsubsection{Second term in Equation \ref{var.vvt1.totalvar}}

This term is 
\begin{equation}
\E_{Y_{1:t-1}}[\var[\hat{\VV}_t|\yy^{(1)}_{1:t-1}] =
\E_{Y_{1:t-1}}[\var[\YY_t-\ZZ_t\E[\XX_t|\yy^{(1)}_{1:t-1}]-\aa_t|\yy^{(1)}_{1:t-1}]]
\end{equation}

$\E[\XX_t|\yy^{(1)}_{1:t-1}]$ is a fixed value, it is not $\XX_t$ but its expected value, and it appears inside a conditional variance not the unconditional variance. If the variance was over $\YY_{1:t-1}$, the expectation would be a random variable but the variance is conditioned on $\YY_{1:t-1}=\yy_{1:t-1}$ so the expectation is a fixed value.  Thus the second term reduces to $\E_{Y_{1:t-1}}[\var[\YY_t|\yy^{(1)}_{1:t-1}]]$. The conditional variance of $\YY_t$ is from Equation \ref{eq:residsMARSS}
\begin{equation}
\var[\YY_t|\yy^{(1)}_{1:t-1}] = \var[\ZZ_t \XX_t + \aa_t + \VV_t|\yy^{(1)}_{1:t-1}] \\
= \ZZ_t \var[\XX_t||\yy^{(1)}_{1:t-1}] \ZZ_t^\top + \var[\VV_t|\yy^{(1)}_{1:t-1}].
\end{equation}


$\hatUt$ is not a function of $\yy$ is is only a function of the MARSS parameters.  Thus the second term in Equation \ref{varvvtgeneral} is simply $\hatUt$.

\subsubsection{Putting together the two terms}
\begin{equation}
\begin{split}
\var[\hat{\VV}_t] &= \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top + \hatUt\\
&= \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top
\end{split}
\end{equation}
This will reduce to $\RR_t - \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ has no missing values and to $\RR_t + \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ is all missing values.

\subsection{State residuals conditioned on the data}
The state residuals are $\xx_t - (\BB_t \xx_{t-1} + \uu_t)=\ww_t$.  The unconditional expected value of the state residuals is $\E(\XX_t - (\BB_t \XX_{t-1} + \uu_t)) = \E(\WW_t) = 0$ and the unconditional variance of the state residuals is
\begin{equation}
\var[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \var[\WW_t] = \QQ_t
\end{equation}
based on the definition of $\WW_t$.
The conditional state residuals (conditioned on the full data) are defined as
\begin{equation}
\hat{\ww}_t = \hatxt - \BB_t\hatxtm - \uu_t.
\end{equation}
It is a sample from the random variable $\hat{\WW}_t$; random over different possible data sets.  The expected value of $\hat{\WW}_t$ is 0, and we can compute $\var_{Y^{(1)}}[\hat{\WW}_t]$ from the law of total variance using the observation that $\hat{\ww}_t=\E[\WW_t|\yy^{(1)}]$.
\begin{equation}
\var[\WW_t] = \var_{Y^{(1)}}[\E[\WW_t|\yy^{(1)}]] + \E_{Y^{(1)}}[\var[\WW_t|\yy^{(1)}]]
\end{equation}
Thus,
\begin{equation}\label{eqn:var.wtt1}
\var_{Y^{(1)}}[\hat{\ww}_t] = \var_{Y^{(1)}}[\E[\WW_t|\yy^{(1)}]] = \var[\WW_t] - \E_{Y^{(1)}}[\var[\WW_t|\yy^{(1)}]]
\end{equation}

The variance in the expectation on the far right is
\begin{equation}
\begin{split}
\var[\WW_t&|\yy^{(1)}] = \var[ \XX_t - \BB_t\XX_{t-1}-\uu_t | \yy^{(1)} ]\\
&\uu\mbox{ is not a random variable and can be dropped}\\
& = \var[ \XX_t - \BB_t\XX_{t-1} | \yy^{(1)} ] \\
& = \var[ \XX_t | \yy^{(1)} ] + \var[\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[\XX_t, -\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[ -\BB_t\XX_{t-1}, \XX_t | \yy^{(1)} ]\\
& = \hatVt + \BB_t \hatVtm \BB_t^\top - \hatVttm \BB_t^\top - \BB_t\widetilde{\VV}_{t-1,t}
\end{split}
\end{equation}
This conditional variance does not depend on the actual values of $\yy$.  It depends only on the parameters values, $\QQ$, $\BB$, $\RR$, etc.
Using the above and $\var[\WW_t]=\QQ_t$ in Equation \ref{eqn:varwwt}, the variance of the conditional state residuals is
\begin{equation}
\var_{Y^{(1)}}[\hat{\WW}_t]  =  \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t}
\end{equation}

\subsection{Covariance of the conditional model and state residuals}
The unconditional model and state residuals, $\VV_t$ and $\WW_t$, are independent (by definition), i.e. $\cov[\VV_t,\WW_t]=0$.  However the conditional model and state residuals, $\cov[\hat{\VV}_t,\hat{\WW}_t]$, are not independent since both depend on $\yy^{(1)}$.  
Using the law of total covariance, we can write
\begin{equation}\label{eqn:covhatVtWt1.t1}
\cov[\hat{\VV}_t,\hat{\WW}_t] = 
\cov_{Y^{(1)}}[\E[\hat{\VV}_t|\yy^{(1)}],\E[\hat{\WW}_t|\yy^{(1)}]] + \E_{Y^{(1)}}[\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]]
\end{equation}
The covariance in the second term on the right can be written out as
\begin{equation}
\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}] =  \cov[\YY_t-\ZZ_t\E[\XX_t|\yy^{(1)}]-\aa_t, \E[\XX_{t}|\yy^{(1)}]-\BB_t\E[\XX_{t-1}|\yy^{(1)}]-\uu_t|\yy^{(1)}]
\end{equation}
The $\E[\XX_t|\yy^{(1)}]$, $\E[\XX_{t-1}|\yy^{(1)}]$ and $\uu_t$ in the second term are fixed values for a given set of data, thus the second term in the covariance is a fixed value. The covariance of a random variable with a fixed value is 0, thus $\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]$ is 0.  Thus Equation \ref{eqn:covhatVtWt1.t1} reduces to
\begin{equation}\label{eqn:covhatVtWt2.t1}
\cov[\hat{\VV}_t,\hat{\WW}_t] = \cov_{Y^{(1)}}[\E[\hat{\VV}_t|\yy^{(1)}],\E[\hat{\WW}_t|\yy^{(1)}]] + 0 = \cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]
\end{equation}
Since $\E[\hat{\VV}_t|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}]$ and $\E[\hat{\WW}_t|\yy^{(1)}]=\E[\WW_t|\yy^{(1)}]$.
In the same way we used the law of total variance, we can use the law of total covariance  to obtain $\cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]$:
\begin{equation}\label{eqn:covhatVtWt3.t1}
\cov[\VV_t, \WW_t] = \E_{Y^{(1)}}[\cov[\VV_t, \WW_t|\yy^{(1)}]] + \cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]\\ 
\end{equation}
The unconditional covariance of $\VV_t$ and $\WW_t$ is 0. Thus the right side of Equation \ref{eqn:covhatVtWt3} is 0 and combining Equation \ref{eqn:covhatVtWt2} and \ref{eqn:covhatVtWt3},
\begin{equation}\label{eqn:conditionalcovvtwt.t1}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\yy^{(1)}] ] 
\end{equation}
and our problem reduces to solving for the conditional covariance of the model and state residuals.  

The conditional covariance $\cov[\VV_t, \WW_t|\yy^{(1)}]$ can be written out as
\begin{equation}
\cov[\VV_t, \WW_t|\yy^{(1)}] = \cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_t-\BB_t\XX_{t-1}-\uu_t|\yy^{(1)}]
\end{equation}
$\aa_t$ and $\uu_t$ are fixed values and can be dropped. Thus
\begin{equation}
\begin{split}
\cov&[\VV_t, \WW_t|\yy^{(1)}] =\cov[\YY_t-\ZZ_t\XX_t, \XX_t-\BB_t\XX_{t-1}|\yy^{(1)}y] \\
& =\cov[\YY_t,\XX_t|\yy^{(1)}] + \cov[\YY_t,-\BB_t\XX_{t-1}|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,\XX_t] + \cov[-\ZZ_t\XX_t,-\BB_t\XX_{t-1}]\\
& = \hatSt - \hatSttm\BB_t^\top - \ZZ_t\hatVt + \ZZ_t\hatVttm\BB_t^\top
\end{split}
\end{equation}
where $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$ and $\hatSttm=\cov[\YY_t,\XX_{t-1}|\yy^{(1)}]$; the equations for $\hatSt$ and $\hatSttm$ are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package.
$\hatVt$, $\hatVttm$, $\hatSt$ and $\hatSttm$ are only functions of the MARSS parameters not of $\yy$.  Thus 
\begin{equation}
\E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\yy^{(1)}] ]= \cov[\VV_t, \WW_t|\yy^{(1)}] = \hatSt - \hatSttm\BB_t^\top + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt
\end{equation}
$\cov[\hat{\VV}_t,\hat{\WW}_t]$ is the negative of this (Equation \ref{eqn:conditionalcovvtwt}), thus
\begin{equation}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \hatSt + \hatSttm\BB_t^\top - \ZZ_t\hatVttm\BB_t^\top + \ZZ_t\hatVt
\end{equation}

The Harvey et al. algorithm shown below gives the joint distribution of the model residuals at time $t$ and state residuals at time $t+1$.  Using the law of total covariance as above The covariance in this case is
\begin{equation}
\cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_{t+1}|\yy^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\yy^{(1)}] ] 
\end{equation}
and
\begin{equation}
\begin{split}
\cov[\VV_t, \WW_{t+1}|\yy^{(1)}] & =\cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_{t+1}-\BB_{t+1}\XX_t-\uu_{t+1}|\yy^{(1)}] \\
& =\cov[\YY_t-\ZZ_t\XX_t, \XX_{t+1}-\BB_{t+1}\XX_t|\yy^{(1)}] \\
& = \hatSttp - \hatSt\BB_{t+1}^\top - \ZZ_t\widetilde{\VV}_{t,t+1} + \ZZ_t\hatVt\BB_{t+1}^\top
\end{split}
\end{equation}
Thus,
$$\cov_{Y^{(1)}}[\E[\VV_t|\yy],\E[\WW_{t+1}|\yy^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\yy^{(1)}] ] = - \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top.$$

\subsection{Joint distribution of the conditional residuals}
We now the write the variance of the joint distribution of the conditional residuals. Define
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_t\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \hatxt - \BB_t\hatxtm-\uu_t \end{bmatrix}.
\end{equation}
where $\hatxt$ and $\hatxtm$ are conditioned on $\yy{(1)}$, the observed $\yy$.
$\hat{\varepsilon}_t$ is a sample drawn from the distribution of $\hat{\mathcal{E}}_t$ conditioned on observations at the $(1)$ locations in $\YY$.  The expected value of $\hat{\mathcal{E}}_t$ over all possible $\yy$ is 0 and the variance of $\hat{\mathcal{E}}_t$  is
\begin{equation}\label{eqn:jointcondresid1general.t1}
 \begin{bmatrix}[c|c]
 \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top+\ZZ_t\hatSt^\top&
 \hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt \\
 \rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
 (\hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt)^\top& 
 \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t} \end{bmatrix}
\end{equation}

If the residuals are defined as in \citet{Harveyetal1998},
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \tilde{\xx}_{t+1} - \BB_{t+1}\hatxt-\uu_{t+1} \end{bmatrix}
\end{equation}
and the variance of $\hat{\mathcal{E}}_t$ is
\begin{equation}\label{eqn:jointcondresid2.t1}
\begin{bmatrix}[c|c]
\RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top&
- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top \\
\rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
(- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top)^\top& 
\QQ_{t+1} - \widetilde{\VV}_{t+1} - \BB_{t+1} \hatVt \BB_{t+1}^\top + \widetilde{\VV}_{t+1,t} \BB_{t+1}^\top + \BB_{t+1}\widetilde{\VV}_{t,t+1} \end{bmatrix}
\end{equation}

The above gives the variance of both `observed' model residuals (the ones associated with $\yy^{(1)}$) and the unobserved model residuals (the ones associated with $\yy^{(2)}$).  
When there are no missing values in $\yy_t$, the $\hatSt$ and $\hatSttm$ terms equal 0 and drop out.



Thus we write Equation \ref{eq:vtt1} directly as:
\begin{equation}\label{eq:vtt1.2}
\var[\hat{\VV}_t] = \var_{Y^{(1)}}[\YY_t] - \ZZ_t\E[\XX_t|\yy^{(1,1:t-1)}]
\end{equation}
$\aa_t$ drops out because it is not a random variable.



\begin{equation}
\begin{split}
\var[\hat{\VV}_t] &= \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top + \hatUt\\
&= \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top
\end{split}
\end{equation}
This will reduce to $\RR_t - \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ has no missing values and to $\RR_t + \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ is all missing values.



\bibliography{./EMDerivation}
\bibliographystyle{apalike}

\end{document}
