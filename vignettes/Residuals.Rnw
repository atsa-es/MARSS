%\VignetteIndexEntry{EM Derivation} 
%\VignettePackage{MARSS}
\documentclass[]{article}
\usepackage{fullpage} %more standardized margins
\usepackage{url} %more standardized margins

% choose options for [] as required from the list
% in the Reference Guide, Sect. 2.2

\usepackage{multirow}
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage[round]{natbib}

% Math stuff
\usepackage{amsmath} % the standard math package
\usepackage{amsfonts} % the standard math package
%%%% bold maths symbol system:
\def\uupsilon{\pmb{\upsilon}}
\def\llambda{\pmb{\lambda}}
\def\bbeta{\pmb{\beta}}
\def\aalpha{\pmb{\alpha}}
\def\zzeta{\pmb{\zeta}}
\def\etaeta{\mbox{\boldmath $\eta$}}
\def\xixi{\mbox{\boldmath $\xi$}}
\def\ep{\mbox{\boldmath $\epsilon$}}
\def\DEL{\mbox{\boldmath $\Delta$}}
\def\PHI{\mbox{\boldmath $\Phi$}}
\def\PI{\mbox{\boldmath $\Pi$}}
\def\LAM{\mbox{\boldmath $\Lambda$}}
\def\LAMm{\mathbb{L}}
\def\GAM{\mbox{\boldmath $\Gamma$}}
\def\OMG{\mbox{\boldmath $\Omega$}}
\def\SI{\mbox{\boldmath $\Sigma$}}
\def\TH{\mbox{\boldmath $\Theta$}}
\def\UPS{\mbox{\boldmath $\Upsilon$}}
\def\XI{\mbox{\boldmath $\Xi$}}
\def\AA{\mbox{$\mathbf A$}}	\def\aa{\mbox{$\mathbf a$}}
\def\Ab{\mbox{$\mathbf D$}} \def\Aa{\mbox{$\mathbf d$}} \def\Am{\PI}
\def\BB{\mbox{$\mathbf B$}}	\def\bb{\mbox{$\mathbf b$}} \def\Bb{\mbox{$\mathbf J$}} \def\Ba{\mbox{$\mathbf L$}} \def\Bm{\UPS}
\def\CC{\mbox{$\mathbf C$}}	\def\cc{\mbox{$\mathbf c$}}
\def\Ca{\Delta} \def\Cb{\GAM}
\def\DD{\mbox{$\mathbf D$}}	\def\dd{\mbox{$\mathbf d$}}
\def\EE{\mbox{$\mathbf E$}}	\def\ee{\mbox{$\mathbf e$}}
\def\E{\,\textup{\textrm{E}}}	
\def\EXy{\,\textup{\textrm{E}}_{\text{{\bf XY}}}}
\def\FF{\mbox{$\mathbf F$}} \def\ff{\mbox{$\mathbf f$}}
\def\GG{\mbox{$\mathbf G$}}	\def\gg{\mbox{$\mathbf g$}}
\def\HH{\mbox{$\mathbf H$}}	\def\hh{\mbox{$\mathbf h$}}
\def\II{\mbox{$\mathbf I$}} \def\ii{\mbox{$\mathbf i$}}
\def\IIm{\mbox{$\mathbf I$}}
\def\JJ{\mbox{$\mathbf J$}}
\def\KK{\mbox{$\mathbf K$}}
\def\LL{\mbox{$\mathbf L$}}	\def\ll{\mbox{$\mathbf l$}}
\def\MM{\mbox{$\mathbf M$}}  \def\mm{\mbox{$\mathbf m$}}
\def\N{\,\textup{\textrm{N}}}
\def\MVN{\,\textup{\textrm{MVN}}}
\def\OO{\mbox{$\mathbf O$}}
\def\PP{\mbox{$\mathbf P$}}  \def\pp{\mbox{$\mathbf p$}}
\def\QQ{\mbox{$\mathbf Q$}}	 \def\qq{\mbox{$\mathbf q$}} \def\Qb{\mbox{$\mathbf G$}}  \def\Qm{\mathbb{Q}}
\def\RR{\mbox{$\mathbf R$}}	 \def\rr{\mbox{$\mathbf r$}} \def\Rb{\mbox{$\mathbf H$}}	\def\Rm{\mathbb{R}}
\def\Ss{\mbox{$\mathbf S$}}
\def\UU{\mbox{$\mathbf U$}}	\def\uu{\mbox{$\mathbf u$}}
\def\Ub{\mbox{$\mathbf C$}} \def\Ua{\mbox{$\mathbf c$}} \def\Um{\UPS}
\def\VV{\mbox{$\mathbf V$}}	\def\vv{\mbox{$\mathbf v$}}
\def\WW{\mbox{$\mathbf W$}}	\def\ww{\mbox{$\mathbf w$}}
%\def\XX{\mbox{$\mathbf X$}}
\def\XX{\mbox{$\pmb{X}$}}	\def\xx{\mbox{$\pmb{x}$}}
%\def\xx{\mbox{$\mathbf x$}}
%\def\YY{\mbox{$\mathbf Y$}}
\def\YY{\mbox{$\pmb{Y}$}}	\def\yy{\mbox{$\pmb{y}$}}
%\def\yy{\mbox{$\mathbf y$}}
\def\ZZ{\mbox{$\mathbf Z$}}	\def\zz{\mbox{$\mathbf z$}}	\def\Zb{\mbox{$\mathbf M$}} \def\Za{\mbox{$\mathbf N$}} \def\Zm{\XI}
\def\zer{\mbox{\boldmath $0$}}
\def\chol{\,\textup{\textrm{chol}}}
\def\vec{\,\textup{\textrm{vec}}}
\def\var{\,\textup{\textrm{var}}}
\def\cov{\,\textup{\textrm{cov}}}
\def\diag{\,\textup{\textrm{diag}}}
\def\trace{\,\textup{\textrm{trace}}}
\def\hatxt{\widetilde{\mbox{$\mathbf x$}}_t}
\def\hatxone{\widetilde{\mbox{$\mathbf x$}}_1}
\def\hatxzero{\widetilde{\mbox{$\mathbf x$}}_0}
\def\hatxtm{\widetilde{\mbox{$\mathbf x$}}_{t-1}}
\def\hatxQtm{\widetilde{\mathbb{x}}_{t-1}}
\def\hatyt{\widetilde{\mbox{$\mathbf y$}}_t}
\def\hatyyt{\widetilde{\mbox{$\mathbf y$}\mbox{$\mathbf y$}^\top}_t}
\def\hatyone{\widetilde{\mbox{$\mathbf y$}}_1}
\def\hatwt{\widetilde{\mbox{$\mathbf w$}}_t}
\def\hatOt{\widetilde{\OO}_t}
\def\hatWt{\widetilde{\WW}_t}
\def\hatYXt{\widetilde{\mbox{$\mathbf{y}\mathbf{x}$}}_t}
\def\hatXYt{\widetilde{\mbox{$\mathbf{x}\mathbf{y}$}}_t}
\def\hatYXttm{\widetilde{\mbox{$\mathbf{y}\mathbf{x}$}}_{t,t-1}}
\def\hatPt{\widetilde{\PP}_t}
\def\hatPtm{\widetilde{\PP}_{t-1}}
\def\hatPQtm{\widetilde{\mathbb{P}}_{t-1}}
\def\hatPttm{\widetilde{\PP}_{t,t-1}}
\def\hatPQttm{\widetilde{\mathbb{P}}_{t,t-1}}
\def\hatPtmt{\widetilde{\PP}_{t-1,t}}
\def\hatVt{\widetilde{\VV}_t}
\def\hatVtm{\widetilde{\VV}_{t-1}}
\def\hatVttm{\widetilde{\VV}_{t,t-1}}
\def\hatUt{\widetilde{\UU}_t}
\def\hatSt{\widetilde{\Ss}_t}
\def\hatSttm{\widetilde{\Ss}_{t,t-1}}
\def\hatSttp{\widetilde{\Ss}_{t,t+1}}
\def\hatBmt{\widetilde{\Bm}_t}
\def\hatCat{\widetilde{\Ca}_t}
\def\hatCbt{\widetilde{\Cb}_t}
\def\hatZmt{\widetilde{\Zm}_t}
\def\YYr{\dot{\mbox{$\pmb{Y}$}}}
\def\yyr{\dot{\mbox{$\pmb{y}$}}}
\def\aar{\dot{\mbox{$\mathbf a$}}}
\def\ZZr{\dot{\mbox{$\mathbf Z$}}}
\def\RRr{\dot{\mbox{$\mathbf R$}}}
\def\IR{\nabla}
\usepackage[round]{natbib} % to get references that are like in ecology papers
% \citet{} for inline citation name (year); \citep for citation in parens (name year)

%allow lines in matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\setcounter{tocdepth}{1} %no subsections in toc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\SweaveOpts{concordance=TRUE}
\author{E. E. Holmes\footnote{Northwest Fisheries Science Center, NOAA Fisheries, Seattle, WA 98112, 
       eli.holmes@noaa.gov, http://faculty.washington.edu/eeholmes}}
\title{Computation of Standardized Residuals for MARSS Models}
\maketitle

This report discusses the computation of the variance of the conditional model (and state) residuals for MARSS models of the  form:
\begin{equation}\label{eq:residsMARSS}
\begin{gathered}
\xx_t = \BB_t\xx_{t-1} + \uu_t + \ww_t, \text{ where } \WW_t \sim \MVN(0,\QQ_t)\\
\yy_t = \ZZ_t\xx_t + \aa_t + \vv_t, \text{ where } \VV_t \sim \MVN(0,\RR_t)\\
\XX_0 \sim \MVN(\xixi,\LAM)
\end{gathered}
\end{equation}

Given a set of observed data $\yy_t$ and states $\xx_t$, the model residuals are $\yy_t - (\ZZ_t \xx_t + \aa_t)=\vv_t$.  The model residual is a random variable since $\yy_t$ and $\xx_t$ are drawn from the joint multivariate distribution of $\YY_t$ and $\XX_t$ defined by the MARSS equation.
The unconditional\footnote{meaning not conditioning on any particular set of observed data but rather taking the expectation across all possible values of $\yy_t$ and $\xx_t$.} variance of the model residuals is
\begin{equation}\label{eqn:unconditiondistofVt}
\var(\YY_t - (\ZZ_t \XX_t + \aa_t)) = \var(\VV_t) = \RR_t\\
\end{equation}
based on the definition of $\VV_t$.  

Once we have data, $\RR_t$ is not the variance of our model residuals because our residuals are now conditioned on a set of observed data.
There are two types of conditional model residuals used in MARSS analyses: innovations and smoothations.  Innovations are the model residuals at time $t$ using the expected value of $\XX_t$ conditioned on the data from 1 to $t-1$.  Smoothations  are the model residuals using the expected value of $\xx_t$ conditioned on all the data, $t=1$ to $T$.  Smoothations are used in computing standardized residuals for outlier and structural break detection \citep{Harveyetal1998, deJongPenzer1998, CommandeurKoopman2007}.  

\section{Distribution of the MARSS conditional residuals}

This report discusses computation of the variance of the model and state residuals conditioned on all the data from $t=1$ to $T$.  MARSS residuals are often used for outlier detection and shock detection, and in this case you only need the distribution of the model residuals for the observed values.  However if you wanted to do a leave-one-out cross-validation, you would need to know the distribution of the residuals for data points you left out (treated as unobserved).  The equations in this report give you the former and the later, while the algorithm by \citet{Harveyetal1998} gives only the former.

Throughout, I follow the convention that capital letters are random variables and small letters are a realization from the random variable.  This only applies to random variables; parameters are not random variables\footnote{in a frequentist framework}.

\subsection{Model residuals conditioned on all the data}
Define the smoothations $\hat{\vv}_t$ as:
\begin{equation}
\hat{\vv}_t = \yy_t - \ZZ_t\hatxt - \aa_t,
\end{equation}
where  $\hatxt$ is $\E[\XX_t|\yy^{(1)}]$ and is output by the Kalman smoother. $\yy^{(1)}$ means all the observed data from $t=1$ to $T$; the unobserved $\yy$ will be termed $\yy^{(2)}$. 
$\hat{\vv}_t$ is sample from the random variable $\hat{\VV}_t$ since $\YY^{(1)}$ is a random variable and the data we have collected $\yy^{(1)}$ is a sample from that.  We want to compute the unconditional mean and variance of this random variable $\hat{\VV}_t$; unconditional here means we take the expectations over all possibles values that $\yy$, both $\yy^{(1)}$ and $\yy^{(1)}$, might take.  The mean is 0 and we are concerned only with computing the variance:
\begin{equation}
\var[\hat{\VV}_t] = \var_Y[\YY_t - \ZZ_t\E[\XX_t|\yy^{(1)}] - \aa_t]
\end{equation}
Notice we have an unconditional variance over $\YY$ on the outside and a conditional variance over a specific value of $\yy^{(1)}$ on the inside.

To compute this, I will use the ``law of total variance'':
\begin{equation}
\var[A] = \var_B[\E_{A|b}[A|b]] + \E_B[\var_{A|b}[A|b]]
\end{equation}
The subscripts on the inner expectations make it explicit that the expectations are being taken over the conditional distributions.  However, going forward, I will write this more succinctly as
\begin{equation}
\var[A] = \var_B[\E[A|b]] + \E_B[\var[A|b]]
\end{equation}
It is understood that $\E[A|b]$ is the conditional expectation conditioned on $B=b$ and $\var(A|b)$ is the conditional variance.

From the law of total variance , we can write
\begin{equation}\label{varvvtgeneral}
\var[\hat{\VV}_t] = \var_Y[\E[\hat{\VV}_t|\yy^{(1)}]] + \E_Y[\var[\hat{\VV}_t|\yy^{(1)}]]
\end{equation}
$\var_Y$ and $\E_Y$ are expectations over both $\YY^{(1)}$ and $\YY^{(2)}$, so all possible values of $\YY$.

\subsubsection{First term in Equation \ref{varvvtgeneral}}

Notice that $\E[\hat{\VV}_t|\yy^{(1)}]=\E[\YY_t|\yy^{(1)}] - \ZZ_t\E[\XX_t|\yy^{(1)}] - \aa_t=\E[\VV_t|\yy^{(1)}]$. So the first term is $\var_Y[\E[\VV_t|\yy^{(1)}]]$.  From the law of total variance, we can write
\begin{equation}\label{eqn:varianceVt}
\var[\VV_t] = \var_Y[\E[\VV_t|\yy^{(1)}]] + \E_Y[\var[\VV_t|\yy^{(1)}]]
\end{equation}
From Equation \ref{eqn:varianceVt}, we can solve for $\var_Y[\E[\VV_t|\yy^{(1)}]]$:
\begin{equation}
\var_Y[\E[\VV_t|\yy^{(1)}]] = \var[\VV_t] - \E_Y[\var[\VV_t|\yy^{(1)}]]
\end{equation}
From Equation \ref{eqn:unconditiondistofVt}, $\var[\VV_t]=\RR_t$.  The second term to the right of the $=$, $\var[\VV_t|\yy^{(1)}]$, is the variance of $\VV_t$ holding $\yy^{(1)}$ fixed but allowing $\XX_t$ (and the rest of the $\XX$) to be random variables:
\begin{equation}\label{eqn:varvtcondy}
\var[\VV_t|\yy^{(1)}] = \var[ \YY_t - \ZZ_t\XX_t-\aa_t | \yy^{(1)} ].
\end{equation}
where $\aa_t$ is a fixed value and can be dropped. 
Equation \ref{eqn:varvtcondy} can then be written as
\begin{equation}
\begin{split}
\var[\VV_t|\yy^{(1)}] &= \var[ \YY_t - \ZZ_t\XX_t | \yy^{(1)} ]\\
&=\var[ - \ZZ_t\XX_t | \yy^{(1)} ] + \var[ \YY_t|\yy^{(1)}] + \cov[ \YY_t, - \ZZ_t\XX_t | \yy^{(1)} ] + \cov[ - \ZZ_t\XX_t, \YY_t | \yy^{(1)} ]\\
&=\ZZ_t \hatVt \ZZ_t^\top + \hatUt - \hatSt\ZZ_t^\top - \ZZ_t\hatSt^\top
\end{split}
\end{equation}
$\hatVt = \var[ \XX_t | \yy^{(1)} ]$ and is output by the Kalman smoother. $\hatUt=\var[\YY_t|\yy^{(1)}]$ and $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$. The equations for these are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package.

$\hatVt$, $\hatUt$ and $\hatSt$ do not depend on the actual values of $\yy$ merely that there is some $\yy$.  They depend instead on the parameters values, $\QQ$, $\BB$, $\RR$, etc., in the MARSS equation. Thus $\E_Y[\var[\VV_t|\yy^{(1)}]] = \var[\VV_t|\yy^{(1)}]$ and 
\begin{equation}\label{eqn:conditionalvtfinal}
\var[\hat{\VV}_t]  = \var[\VV_t] - \var[\VV_t|\yy^{(1)}] = \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top
\end{equation}

\subsubsection{Second term in Equation \ref{varvvtgeneral}}

Consider the second term in Equation \ref{varvvtgeneral}.  This term is 
$$\E_Y[\var[\YY_t-\ZZ_t\E[\XX_t|\yy^{(1)}]-\aa_t|\yy^{(1)}]]$$
$\E[\XX_t|\yy^{(1)}]$ is a fixed value; it is not $\XX_t$ but its expected value.   Thus the second term reduces to $\E_Y[\var[\YY|\yy^{(1)}]]=\E_Y[\hatUt]$.  =
$\hatUt$ is not a function of $\yy$ is is only a function of the MARSS parameters.  Thus the second term in Equation \ref{varvvtgeneral} is simply $\hatUt$.

\subsubsection{Putting together the two terms}
\begin{equation}
\begin{split}
\var[\hat{\VV}_t] &= \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top + \hatUt\\
&= \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top
\end{split}
\end{equation}
This will reduce to $\RR_t - \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ has no missing values and to $\RR_t + \ZZ_t \hatVt \ZZ_t^\top$ is $\yy_t$ is all missing values.

\subsection{State residuals conditioned on the data}
The state residuals are $\xx_t - (\BB_t \xx_{t-1} + \uu_t)=\ww_t$.  The unconditional expected value of the state residuals is $\E(\XX_t - (\BB_t \XX_{t-1} + \uu_t)) = \E(\WW_t) = 0$ and the unconditional variance of the state residuals is
\begin{equation}
\var[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \var[\WW_t] = \QQ_t
\end{equation}
based on the definition of $\WW_t$.
The conditional state residuals (conditioned on the full data) are defined as
\begin{equation}
\hat{\ww}_t = \hatxt - \BB_t\hatxtm - \uu_t.
\end{equation}
It is a sample from the random variable $\hat{\WW}_t$; random over different possible data sets.  The expected value of $\hat{\WW}_t$ is 0, and we can compute $\var_Y[\hat{\WW}_t]$ from the law of total variance using the observation that $\hat{\ww}_t=\E[\WW_t|\yy^{(1)}]$.
\begin{equation}
\var[\WW_t] = \var_Y[\E[\WW_t|\yy^{(1)}]] + \E_Y[\var[\WW_t|\yy^{(1)}]]
\end{equation}
Thus,
\begin{equation}\label{eqn:varwwt}
\var_Y[\hat{\ww}_t] = \var_Y[\E[\WW_t|\yy^{(1)}]] = \var(\WW_t) - \E_Y[\var[\WW_t|\yy^{(1)}]]
\end{equation}

The variance in the expectation on the far right is
\begin{equation}
\begin{split}
\var[\WW_t&|\yy^{(1)}] = \var[ \XX_t - \BB_t\XX_{t-1}-\uu_t | \yy^{(1)} ]\\
&\uu\mbox{ is not a random variable and can be dropped}\\
& = \var[ \XX_t - \BB_t\XX_{t-1} | \yy^{(1)} ] \\
& = \var[ \XX_t | \yy^{(1)} ] + \var[\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[\XX_t, -\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[ -\BB_t\XX_{t-1}, \XX_t | \yy^{(1)} ]\\
& = \hatVt + \BB_t \hatVtm \BB_t^\top - \hatVttm \BB_t^\top - \BB_t\widetilde{\VV}_{t-1,t}
\end{split}
\end{equation}
This conditional variance does not depend on the actual values of $\yy$.  It depends only on the parameters values, $\QQ$, $\BB$, $\RR$, etc.
Using the above and $\var[\WW_t]=\QQ_t$ in Equation \ref{eqn:varwwt}, the variance of the conditional state residuals is
\begin{equation}
\var_Y[\hat{\WW}_t]  =  \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t}
\end{equation}

\subsection{Covariance of the conditional model and state residuals}
The unconditional model and state residuals, $\VV_t$ and $\WW_t$, are independent (by definition), i.e. $\cov[\VV_t,\WW_t]=0$.  However the conditional model and state residuals, $\cov[\hat{\VV}_t,\hat{\WW}_t]$, are not independent since both depend on $\yy^{(1)}$.  
Using the law of total covariance, we can write
\begin{equation}\label{eqn:covhatVtWt1}
\cov[\hat{\VV}_t,\hat{\WW}_t] = 
\cov_Y[\E[\hat{\VV}_t|\yy^{(1)}],\E[\hat{\WW}_t|\yy^{(1)}]] + \E_Y[\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]]
\end{equation}
The covariance in the second term on the right can be written out as
\begin{equation}
\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}] =  \E_Y[\cov[\YY_t-\ZZ_t\E[\XX_t|\yy^{(1)}]-\aa_t, \E[\XX_{t-1}|\yy^{(1)}]-\BB_t\E[\XX_{t-1}|\yy^{(1)}]-\uu_t|\yy^{(1)}]]
\end{equation}
The $\E[\XX_t|\yy^{(1)}]$ are fixed values for a given set of data. The covariance of a random variable with a fixed value is 0, thus $\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]$ is 0.  Thus Equation \ref{eqn:covhatVtWt1} reduces to
\begin{equation}\label{eqn:covhatVtWt2}
\cov[\hat{\VV}_t,\hat{\WW}_t] = \cov_Y[\E[\hat{\VV}_t|\yy^{(1)}],\E[\hat{\WW}_t|\yy^{(1)}]] + 0 = \cov_Y[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]
\end{equation}
Since $\E[\hat{\VV}_t|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}]$ and $\E[\hat{\WW}_t|\yy^{(1)}]=\E[\WW_t|\yy^{(1)}]$.
In the same way we used the law of total variance, we can use the law of total covariance  to obtain $\cov_Y[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]$:
\begin{equation}\label{eqn:covhatVtWt3}
\cov[\VV_t, \WW_t] = \E_Y[\cov[\VV_t, \WW_t|\yy^{(1)}]] + \cov_Y[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]\\ 
\end{equation}
The unconditional covariance of $\VV_t$ and $\WW_t$ is 0. Thus the right side of Equation \ref{eqn:covhatVtWt3} is 0 and combining Equation \ref{eqn:covhatVtWt2} and \ref{eqn:covhatVtWt3},
\begin{equation}\label{eqn:conditionalcovvtwt}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \E_Y[ \cov[\VV_t, \WW_t|\yy^{(1)}] ] 
\end{equation}
and our problem reduces to solving for the conditional covariance of the model and state residuals.  

The conditional covariance $\cov[\VV_t, \WW_t|\yy^{(1)}]$ can be written out as
\begin{equation}
\cov[\VV_t, \WW_t|\yy^{(1)}] = \cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_t-\BB_t\XX_{t-1}-\uu_t|\yy^{(1)}]
\end{equation}
$\aa_t$ and $\uu_t$ are fixed values and can be dropped. Thus
\begin{equation}
\begin{split}
\cov&[\VV_t, \WW_t|\yy^{(1)}] =\cov[\YY_t-\ZZ_t\XX_t, \XX_t-\BB_t\XX_{t-1}|\yy^{(1)}y] \\
& =\cov[\YY_t,\XX_t|\yy^{(1)}] + \cov[\YY_t,-\BB_t\XX_{t-1}|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,\XX_t] + \cov[-\ZZ_t\XX_t,-\BB_t\XX_{t-1}]\\
& = \hatSt - \hatSttm\BB_t^\top - \ZZ_t\hatVt + \ZZ_t\hatVttm\BB_t^\top
\end{split}
\end{equation}
where $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$ and $\hatSttm=\cov[\YY_t,\XX_{t-1}|\yy^{(1)}]$; the equations for $\hatSt$ and $\hatSttm$ are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package.
$\hatVt$, $\hatVttm$, $\hatSt$ and $\hatSttm$ are only functions of the MARSS parameters not of $\yy$.  Thus 
\begin{equation}
\E_Y[ \cov[\VV_t, \WW_t|\yy^{(1)}] ]= \cov[\VV_t, \WW_t|\yy^{(1)}] = \hatSt - \hatSttm\BB_t^\top + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt
\end{equation}
$\cov[\hat{\VV}_t,\hat{\WW}_t]$ is the negative of this (Equation \ref{eqn:conditionalcovvtwt}), thus
\begin{equation}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \hatSt + \hatSttm\BB_t^\top - \ZZ_t\hatVttm\BB_t^\top + \ZZ_t\hatVt
\end{equation}

The Harvey et al. algorithm shown below gives the joint distribution of the model residuals at time $t$ and state residuals at time $t+1$.  Using the law of total covariance as above The covariance in this case is
\begin{equation}
\cov_Y[\E[\VV_t|\yy^{(1)}],\E[\WW_{t+1}|\yy^{(1)}]] = - \E_Y[ \cov[\VV_t, \WW_{t+1}|\yy^{(1)}] ] 
\end{equation}
and
\begin{equation}
\begin{split}
\cov[\VV_t, \WW_{t+1}|\yy^{(1)}] & =\cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_{t+1}-\BB_{t+1}\XX_t-\uu_{t+1}|\yy^{(1)}] \\
& =\cov[\YY_t-\ZZ_t\XX_t, \XX_{t+1}-\BB_{t+1}\XX_t|\yy^{(1)}] \\
& = \hatSttp - \hatSt\BB_{t+1}^\top - \ZZ_t\widetilde{\VV}_{t,t+1} + \ZZ_t\hatVt\BB_{t+1}^\top
\end{split}
\end{equation}
Thus,
$$\cov_Y[\E[\VV_t|\yy],\E[\WW_{t+1}|\yy^{(1)}]] = - \E_Y[ \cov[\VV_t, \WW_{t+1}|\yy^{(1)}] ] = - \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top.$$

\subsection{Joint distribution of the conditional residuals}
We now the write the variance of the joint distribution of the conditional residuals. Define
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_t\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \hatxt - \BB_t\hatxtm-\uu_t \end{bmatrix}.
\end{equation}
where $\hatxt$ and $\hatxtm$ are conditioned on $\yy{(1)}$, the observed $\yy$.
$\hat{\varepsilon}_t$ is a sample drawn from the distribution of $\hat{\mathcal{E}}_t$ conditioned on observations at the $(1)$ locations in $\YY$.  The expected value of $\hat{\mathcal{E}}_t$ over all possible $\yy$ is 0 and the variance of $\hat{\mathcal{E}}_t$  is
\begin{equation}\label{eqn:jointcondresid1general}
 \begin{bmatrix}[c|c]
 \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top+\ZZ_t\hatSt^\top&
 \hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt \\
 \rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
 (\hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt)^\top& 
 \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t} \end{bmatrix}
\end{equation}

If the residuals are defined as in \citet{Harveyetal1998},
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \tilde{\xx}_{t+1} - \BB_{t+1}\hatxt-\uu_{t+1} \end{bmatrix}
\end{equation}
and the variance of $\hat{\mathcal{E}}_t$ is
\begin{equation}\label{eqn:jointcondresid2}
\begin{bmatrix}[c|c]
\RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t\hatSt^\top&
- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top \\
\rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
(- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top)^\top& 
\QQ_{t+1} - \widetilde{\VV}_{t+1} - \BB_{t+1} \hatVt \BB_{t+1}^\top + \widetilde{\VV}_{t+1,t} \BB_{t+1}^\top + \BB_{t+1}\widetilde{\VV}_{t,t+1} \end{bmatrix}
\end{equation}

The above gives the variance of both `observed' model residuals (the ones associated with $\yy^{(1)}$) and the unobserved model residuals (the ones associated with $\yy^{(2)}$).  
When there are no missing values in $\yy_t$, the $\hatSt$ and $\hatSttm$ terms equal 0 and drop out.

\section{Harvey et al 1998 algorithm for the conditional residuals}
\citet[pgs 112-113]{Harveyetal1998} give a recursive algorithm for computing the variance of the conditional residuals when the time-varying MARSS equation is written as: 
\begin{equation}\label{eqn:residsMARSSHarvey}
\begin{gathered}
\xx_{t+1} = \BB_{t+1}\xx_t + \uu_{t+1} + \GG_{t+1}\epsilon_{t},\\
\yy_t = \ZZ_t\xx_t + \aa_t + \HH_t\epsilon_t,\\
\mbox{ where } \epsilon_t \sim \MVN(0,\II_{m+n \times m+n}), \GG_t\GG_t^\top=\QQ_t\text{ and }\HH_t\HH_t^\top=\RR_t
\end{gathered}
\end{equation}
$\GG_t$ has $m$  rows and $m+n$ columns with the last $n$ columns all 0; $\HH_t$ has $n$ rows and $m+n$ columns with the last $m$ columns all zero.  The algorithm in \citet{Harveyetal1998} gives the variance of the `normalized' residuals, the $\epsilon_t$.  I have modified their algorithm so it  returns the `non-normalized' residuals:
$$\varepsilon_t=\begin{bmatrix}\HH_t\epsilon_t\\ \GG_{t+1}\epsilon_t\end{bmatrix}=\begin{bmatrix}\vv_t\\ \ww_{t+1} \end{bmatrix}.$$

The Harvey et al. algorithm is a backwards recursion using output from the Kalman filter: the one-step ahead prediction covariance $\FF_t$  and the Kalman gain $\KK_t$. Starting from $t=T$ and working backwards to $t=1$ and using $r_T=0$ and $N_T=0$, the algorithm is 
% algorithm with \GG_t and \HH_t
% \begin{equation}\label{eqn:Harveyalgo}
% \begin{gathered}
% \FF_t = \ZZ_t\hatVt\ZZ_t^\top+\RR_t\\
% G_t= \GG_t\QQ_t\GG_t^\top, \mbox{  }H_t = \HH_t\RR_t\HH_t^\top,  \mbox{ } K_t = \BB_t\KK^{*}_t\\
% L_t = \BB_t - K_t\ZZ_t, \mbox{ } J_t= H_t - K_t G_t, \mbox{ } u_t = \FF_t^{-1} - K_t^\top r_t\\
% r_{t-1} = \ZZ_t^\top u_t + \BB_t^\top r_t, \mbox{ } N_{t-1} = K_t^\top N_t K_t + L_t^\top N_t L_t
% \end{gathered}
% \end{equation}

\begin{equation}\label{eqn:Harveyalgo}
\begin{gathered}
\QQ^*_{t+1}=\begin{bmatrix}\QQ_{t+1}&0_{m \times n}\end{bmatrix}, \mbox{    } \RR^*_t=\begin{bmatrix}0_{n \times m}&\RR_t^*\end{bmatrix}\\
\FF_t = \ZZ_t^*\hatVt{\ZZ_t^*}^\top+\RR_t^*\mbox{,   } K_t = \BB_{t+1}\KK_t\\
L_t = \BB_{t+1} - K_t\ZZ_t^*, \mbox{    } J_t= \QQ^*_{t+1} - K_t \RR^*_t, \mbox{    } u_t = \FF_t^{-1} - K_t^\top r_t\\
r_{t-1} = {\ZZ_t^*}^\top u_t + \BB_{t+1}^\top r_t, \mbox{    } N_{t-1} = K_t^\top N_t K_t + L_t^\top N_t L_t
\end{gathered}
\end{equation}
Bolded terms are the same as in Equation \ref{eqn:residsMARSSHarvey}.  Unbolded terms are terms used in \citet{Harveyetal1998}.  The * on $\ZZ_t$ and $\RR_t$, indicates that they are the missing value modified versions  discussed in \citet[section 6.4]{ShumwayStoffer2006}: the rows of $\ZZ_t$ corresponding to missing rows of $\yy_t$ are set to zero and the $(i,j)$ and $(j,i)$ terms of $\RR_t$ corresponding the missing rows of $\yy_t$ are set to zero.  For the latter, this means if the $i$-th row of $\yy_t$ is missing, then then all the $(i,j)$ and $(j,i)$ terms, including $(i,i)$ are set to 0. It is assumed that a missing values modified inverse of $\FF_t$ is used; for example 0 on diagonal replaced with 1, inverse taken, and 1 on diagonal replaced back with 0.

The residuals are
\begin{equation}\label{eqn:Harveyresiduals}
\hat{\varepsilon}^*_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =({\RR^*_t})^\top u_t + ({\QQ^*_{t+1}})^\top r_t
\end{equation}
with mean of 0 ($\E_Y(\hat{\varepsilon}_t)=0$) and variance
\begin{equation}\label{eqn:Harveyvariance}
\Sigma_t^* = \var_Y(\hat{\varepsilon}_t) ={\RR^*_t}^\top \FF_t^{-1} \RR^*_t + J_t^\top N_t J_t
\end{equation}
The * signifies that these are the missing values modified $\hat{\varepsilon}_t$ and $\Sigma_t$; see comments above.

If you compare their state equation (their equation 20) with my state equation, you will notice that my time indexing on $\BB$ matches the left $\xx$ while in theirs, it matches the right $\xx$. Thus $\BB_{t+1}$ (and $\QQ_{t+1}$) appears in my implementation of their algorithm instead of $\BB_t$.
\citet[eqns. 19, 20]{Harveyetal1998} use $G_t$ to refer to the $\chol(\RR_t)^\top$ (essentially) and $H_t$ to refer to $\chol(\QQ_t)^\top$.  I've replaced these with $\RR_t^*$ and $\QQ_t^*$, respectively, which causes my variant of their algorithm to give the `non-normalized' variance of the residuals.  Their $T_t$ is my $\BB_{t+1}$. 
$\KK_t$ is the Kalman gain output by the MARSS package.  The Kalman gain as used in the \citet{Harveyetal1998} algorithm is $K_t=\BB_{t+1}\KK_t$.

\subsection{Computing the standardized residuals}
The standardized residuals are computed by multiplying $\hat{\varepsilon}_t$ by the inverse of the square root of the variance-covariance matrix from which $\hat{\varepsilon}_t$ is ``drawn'':
\begin{equation}
(\Sigma_t^*)^{-1/2}\hat{\varepsilon}_t^*
\end{equation}
Notice that the missing values modified $\hat{\varepsilon}_t^*$ and $\Sigma_t^*$ are used. if the $i$-th row of $\yy_t$ is missing, the $i$-th row of $\hat{\varepsilon}_t$ is set to 0 and the $i$-th row and column of $\Sigma_t$ is set to all 0.
There will be 0s on the diagonal of $\Sigma_t^*$ so your code will need to deal with these.


\bibliography{./tex/Manual}
\bibliographystyle{apalike}

\end{document}
