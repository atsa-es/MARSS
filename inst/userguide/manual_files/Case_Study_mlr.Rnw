\SweaveOpts{keep.source=TRUE, prefix.string=./figures/MLR-, eps=FALSE, split=TRUE}
\chapter{Multivariate linear regression}
\label{chap:mlr}
\chaptermark{Multivariate linear regression}

<<RUNFIRST, echo=FALSE>>=
tabledir <- "figures/"
options(prompt = " ", continue = " ", width = 60)
@
\index{multivariate linear regression}
%Add footnote with instructions for getting code
\blfootnote{Type \texttt{RShowDoc("Chapter\_MLR.R",package="MARSS")} at the R command line to open a file with all the code for the examples in this chapter.}


<<Cs_000_required_libraries, echo=FALSE>>=
library(MARSS)
library(xtable)
library(lattice)
library(nlme)
library(stringr)
library(lme4)
@


This chapter shows how to write regression models with multivariate responses and multivariate explanatory variables in MARSS form.  \R has many excellent functions and packages for multiple linear regression.  We will be showing how to use the \verb@MARSS()@ function to fit these models, but note that \R's standard linear regression functions would be much better choices in most cases.  The purpose of this chapter is to show the relationship between multivariate linear regression and the MARSS equation.

In a classic linear regression, the response variable ($y$) is univariate and there may be one to multiple explanatory variables ($d_1$, $d_2$, $\dots$) plus an optional intercept ($\alpha$):
\begin{equation}\label{eqn:lm}
y_t = \alpha + \sum_k\beta_k d_k + e_t, \text{ where } e_t \sim \N(0,\sigma^2) 
\end{equation}
Here the subscript, $t$ is used since we are working with time-series data.  Explanatory variables are normally denoted $x$ in linear regression however $x$ is not used here since $x$ is already used in MARSS models to denote the hidden process trajectory.  Instead $d$ is used when the explanatory variables appear in the $y$ part of the equation (and $c$ if they appear in the $x$ part). 

This chapter will start with classical linear regression where the explanatory variables are treated as inputs that are known without error and where we are trying to explain the variation in $y$ with our explanatory variables.  We will extend this to the case of autocorrelated errors.  

\section{Univariate linear regression}

A vanilla linear regression where our data are time ordered but we treat them as independent can be written as 
\begin{equation}\label{eqn:lm2}
y_t=\alpha + \beta_1 d_{1,t} + \beta_2 d_{2,t} + e_t,
\end{equation}
where the $d$ are our explanatory variables.  This model can be written in many different ways in as a MARSS equation. Here we use a specific form where the i.i.d. component of the errors is $v_t$ in the $y$ part of the MARSS equation and autocorrelated errors will appear as $x_t$ in the $y$ equation.  Specifying the MARSS model this way allows us to use the EM-algorithm to fit the model which will prove to be important.

\begin{equation}\label{eqn:lm.marss1}
\begin{gathered}
y_t = \alpha + \begin{bmatrix}\beta_1&\beta_2&\dots\end{bmatrix}\begin{bmatrix}d_{1,t}\\ d_{2,t} \\ \vdots\end{bmatrix} + v_t + x_t, v_t \sim \N(0,r)\\
x_t = b x_{t-1} + w_t, w_t ~ \N(0, q)\\
x_0 = 0
\end{gathered}
\end{equation}
The $v_t$ are the i.i.d. errors and the $x_t$ are the AR(1) errors.

\subsection{Univariate response using the Longley dataset: example 1}
We will start by using an example from Chapter 6 in Linear Models in R \citep{Faraway2004}.  This example uses the built-in R dataset ``longley" which has the
number of people employed from 1947 to 1962 and a number of predictors. For this example we will regress the number of people employed against gross National product and population size (following Faraway).

\begin{figure}[htp]
\begin{center}
<<Cs_001_example1_plot,fig=TRUE,echo=FALSE,width=6, height=6, keep.source=FALSE>>=
data(longley)
plot(longley$Year, longley$Employed, type = "l", ylab = "Employed", xlab = "")
@
\end{center}
\caption{Employment time series from the Longley dataset.}
\label{fig:longley.fig.1}
\end{figure}

Mathematically, the model we are fitting is
\begin{equation}\label{eqn:longley1}
\begin{gathered}
Employed_t = \alpha + \begin{bmatrix}\beta_{GNP}&\beta_{Pop}\end{bmatrix}\begin{bmatrix} GNP_t\\ Pop_t\end{bmatrix} + v_t, v_t \sim \N(0,r) \\
\end{gathered}
\end{equation}
$x$ does not appear in the vanilla linear regression since we do not have autocorrelated errors (yet). We are trying to estimate $\alpha$ (intercept), $\beta_{GNP}$ and $\beta_{Pop}$.  

A full multivariate MARSS model looks like
\begin{equation}\label{eqn:mlm.marss}
\begin{gathered}
\yy_t = \ZZ\xx_t + \aa + \DD\dd_t + \vv_t, \text{ where } \vv_t \sim \MVN(0,\RR) \\
\xx_t = \BB\xx_{t-1} + \uu + \CC\cc_t + \ww_t, \text{ where } \ww_t \sim \MVN(0,\QQ)
\end{gathered}
\end{equation}
We need to specify the parameters in Equation \ref{eqn:mlm.marss} such that we get Equation 
\ref{eqn:longley1}. 

First, we load the data and set up $y$, the response variable number of people employed, as a matrix with time going across the columns.
<<Cs_002_example1-data>>=
data(longley)
Employed <- matrix(longley$Employed, nrow = 1)
@
Second create a list to hold our model specification.
<<Cs_003_example1-params>>=
longley.model <- list()
@
Set the $\uu$, $\QQ$ and $\xx_0$ parameters to 0.  We will also set $\aa$ and $\CC$ to 0 and $\BB$ and $\ZZ$ to identity although this is not necessary since these are the defaults.
<<Cs_004_example1-params1>>=
longley.model$U <- longley.model$Q <- "zero"
longley.model$C <- "zero"
longley.model$B <- longley.model$Z <- "identity"
longley.model$x0 <- "zero"
longley.model$tinitx <- 0
@
We will estimate $\RR$, the variance of the i.i.d. errors (residuals).
<<Cs_005_example1-paramsR>>=
longley.model$R <- matrix("r")
@
The $\DD$ matrix has the two $\beta$ (slope) parameters for GNP and Population and $a$ has the intercept.\footnote{A better way to fit the model is to put the intercept into $\DD$ by adding a row of 1s to $\dd$ and putting the intercept parameter on the first row of $\DD$.  This reduces by one the number of matrices being estimated by the EM algorithm. It's not done here just so the equations look more like standard linear regression equations.}
<<Cs_006_example1-paramsD>>=
longley.model$A <- matrix("intercept")
longley.model$D <- matrix(c("GNP", "Pop"), nrow = 1)
@
Last we set up our explanatory variables.  This is the $\dd$ matrix and we need each explanatory variable in a row with time across the columns.
<<Cs_007_example1-eVar>>=
longley.model$d <- rbind(longley$GNP, longley$Population)
@

Now we can fit the model:
<<Cs_008_example1-marss, results=hide>>=
mod1 <- MARSS(Employed, model = longley.model)
@
and look at the estimates.
<<Cs_009_example1-marss, results=hide>>=
coef(mod1, type = "vector")
@
\verb@method="BFGS"@ can also be used and gives similar results.

We can compare the fit to that from lm() and see that we get the same estimates:
<<Cs_010_example1-lm>>=
mod1.lm <- lm(Employed ~ GNP + Population, data = longley)
coef(mod1.lm)
@

\subsection{Univariate response using auto-correlated errors: example 1}
\index{multivariate linear regression!with autocorrelated errors}
As \citet{Faraway2004} discusses, the errors in this dataset are temporally correlated.  We can model the errors as an AR(1) process to account for this. This changes our model to 
\begin{equation}\label{eqn:longley.correrr}
\begin{gathered}
Employed_t = \alpha + \begin{bmatrix}\beta_{GNP}&\beta_{Pop}\end{bmatrix}\begin{bmatrix}GNP_t\\ Pop_t\end{bmatrix} + v_t + x_t, v_t \sim \N(0,r)\\
x_t = b x_{t-1}+w_t, w_t \sim \N(0,q)\\
\xx_0 = 0
\end{gathered}
\end{equation}
We assume the AR(1) errors have mean 0 so $u=0$ in the $x_t$ equation.  Setting $u$ to anything else would make the mean of our errors equal to $u/(1-b)$ for $-1<b<1$. This would lead to two mean levels in our model, $\alpha$ and $u/(1-b)$, and we would not be able to estimate both.  Notice that the model is somewhat confounded since if $b=0$ then $x_t$ is i.i.d. errors same as $v_t$.  In this case, either  $q$ or $r$ would be redundant.  It is thus possible that either $r$ or $q$ will go to zero.

To fit the model with autoregressive errors, we add the $x$ parameters to our the model list.  We estimate $b$ and $q$.
<<Cs_011_example2-params>>=
longley.ar1 <- longley.model
longley.ar1$B <- matrix("b")
longley.ar1$Q <- matrix("q")
@

Now we can fit the model as before
<<Cs_012_example2-marss, results=hide>>=
mod2 <- MARSS(Employed, model = longley.ar1)
@
however, this is a difficult model to fit and takes a long, long time to converge.  The default \verb@maxit@ used in the call above is not nearly enough iterations. Using \verb@method="BFGS"@ helps a little but not much.  We can improve behavior by using the fit of the model with i.i.d. errors as initial conditions for $\DD$ and $a$.\index{troubleshooting!non-convergence}

<<Cs_013_example2-marss-with-inits, results=hide>>=
inits <- list(A = coef(mod1)$A, D = coef(mod1)$D)
mod2 <- MARSS(Employed,
  model = longley.ar1, inits = inits,
  control = list(maxit = 1000)
)
ests.marss <- c(
  b = coef(mod2)$B, alpha = coef(mod2)$A,
  GNP = coef(mod2)$D[1], Population = coef(mod2)$D[2],
  logLik = logLik(mod2)
)
@
\index{functions!logLik}

We can the fit the same model using \verb@gls()@ (in the \{nlme\} package).  The $b$ term is called Phi in the \verb@gls()@ call and is somewhat difficult to recover although it is printed by \verb@summary()@.\index{functions!gls}
<<Cs_014_example2-gls>>=
library(nlme)
mod2.gls <- gls(Employed ~ GNP + Population,
  correlation = corAR1(), data = longley, method = "ML"
)
mod2.gls.phi <- coef(mod2.gls$modelStruct[[1]], unconstrained = FALSE)
ests.gls <- c(
  b = mod2.gls.phi, alpha = coef(mod2.gls)[1],
  GNP = coef(mod2.gls)[2], Population = coef(mod2.gls)[3],
  logLik = logLik(mod2.gls)
)
@
Note we need to set \verb@method="ML"@ to maximize the likelihood because the default is to maximize the restricted maximum-likelihood (\verb@method="REML"@) and that gives a different answer from the \verb@MARSS()@ function since \verb@MARSS()@ is maximizing the likelihood.

Both functions return similar values though \verb@gls()@ is much faster and the EM algorithm has not quite converged even with 1000 iterations.
<<Cs_014b_compare_marss_gls>>=
rbind(MARSS = ests.marss, GLS = ests.gls)
@

\subsection{Univariate response using the Longley dataset: example 2}
\index{troubleshooting!collinearity}
The full Longley dataset is often used to test the performance of numerical methods for fitting linear regression models because it has severe collinearity problems (Figure \ref{fig:longley.fig2}).  We can compare the EM and BFGS algorithms for the full dataset and see how fitting a MARSS model with the BFGS algorithm leads to estimates far from the maximum-likelihood values for this problem.

\begin{figure}[htp]
\begin{center}
<<Cs_015_example2-plot, fig=TRUE, echo=FALSE>>=
pairs(longley)
@
\end{center}
\caption{Pairs plot showing collinearity in the Longley explanatory variables.}
\label{fig:longley.fig2}
\end{figure}

We can fit a regression of Employed to all the Longley explanatory variables using the following code.  The mathematical model is the same as in Equation \ref{eqn:longley1} except that instead of two explanatory variables with have all seven shown in Figure \ref{fig:longley.fig2}.
<<Cs_016_full-model-list>>=
eVar.names <- colnames(longley)[-7]
eVar <- t(longley[, eVar.names])
longley.model <- list()
longley.model$U <- longley.model$Q <- "zero"
longley.model$C <- "zero"
longley.model$B <- longley.model$Z <- "identity"
longley.model$A <- matrix("intercept")
longley.model$R <- matrix("r")
longley.model$D <- matrix(eVar.names, nrow = 1)
longley.model$d <- eVar
longley.model$x0 <- "zero"
longley.model$tinitx <- 0
@

Then we fit as usual.  We will fit with the EM-algorithm (the default) and compare to BFGS.
<<Cs_017_full-model-fit, results=hide>>=
mod3.em <- MARSS(Employed, model = longley.model)
mod3.bfgs <- MARSS(Employed, model = longley.model, method = "BFGS")
@
Here are the EM estimates with the log-likelihood.
<<Cs_018_full-em-ests>>=
par.names <- c("A.intercept", paste("D", eVar.names, sep = "."))
c(coef(mod3.em, type = "vector")[par.names], logLik = mod3.em$logLik)
@
Compared to the BFGS estimates:
<<Cs_019_full-bfgs-ests>>=
c(coef(mod3.bfgs, type = "vector")[par.names], logLik = mod3.bfgs$logLik)
@
And compared to the estimates from the \verb@lm()@ function:
<<Cs_020_full-lm-ests>>=
mod3.lm <- lm(Employed ~ 1 + GNP.deflator + GNP + Unemployed
  + Armed.Forces + Population + Year, data = longley)
c(coef(mod3.lm), logLik = logLik(mod3.lm))
@
As you can see the BFGS algorithm struggles with the ridge-like likelihood caused by the collinearity in the explanatory variables.

We can also compare the performance of the model with AR(1) errors.  This is Equation \ref{eqn:longley.correrr} but with all seven explanatory variables.  We set up the MARSS model\footnote{Notice that $x_0$ is set at 0.  The model is having a hard time fitting $x_0$ because the time series is short.  Estimating $x_0$ or using a diffuse prior by setting $\VV_0$ big, leads to poor estimates.  Since this is just the error term, we set $x_0=0$ since the mean of the errors is assumed to be 0.} for a linear regression with correlated errors as before with the addition of $b$ (called Phi in \verb@gls()@) and $q$.\index{functions!gls}
<<Cs_021_full-model-correrr>>=
longley.correrr.model <- longley.model
longley.correrr.model$B <- matrix("b")
longley.correrr.model$Q <- matrix("q")
@

We fit as usual and compare the EM-algorithm (the default) to fits using BFGS.  We will use the estimate from the model with i.i.d. errors as initial conditions.
<<Cs_022_full-correrr-fit-hide, results=hide>>=
inits <- list(A = coef(mod3.em)$A, D = coef(mod3.em)$D)
mod4.em <- MARSS(Employed, model = longley.correrr.model, inits = inits)
mod4.bfgs <- MARSS(Employed,
  model = longley.correrr.model,
  inits = inits, method = "BFGS"
)
@
Here are the EM estimates with the log-likelihood.  We only show $\phi$ (the $b$ term in the AR(1) error equation) and the log-likelihood.
<<Cs_023_full-correrr-em-ests>>=
c(coef(mod4.em, type = "vector")["B.b"], logLik = mod4.em$logLik)
@
Compared to the BFGS estimates:
<<Cs_024_full-correrr-bfgs-ests>>=
c(coef(mod4.bfgs, type = "vector")["B.b"], logLik = mod4.bfgs$logLik)
@
And compared to the estimates from the \verb@gls()@ function:
<<Cs_025_full-gls-ests>>=
mod4.gls <- gls(Employed ~ 1 + GNP.deflator + GNP + Unemployed
  + Armed.Forces + Population + Year,
correlation = corAR1(), data = longley, method = "ML"
)
mod4.gls.phi <- coef(mod4.gls$modelStruct[[1]], unconstrained = FALSE)
c(mod4.gls.phi, logLik = logLik(mod4.gls))
@
Again we see that the BFGS algorithm struggles with the ridge-like likelihood caused by the collinearity in the explanatory variables.


\section{Multivariate response example using longitudinal data}
We will illustrate linear regression with a multivariate response using longitudinal data from a sleep study on 18 subjects from the \{lme4\} \R package. These are data on reaction time of subjects after 0 to 9 days of being restricted to 3 hours of sleep.

We load the data from the \{lme4\} package:
<<Cs_026_loadsleep>>=
data(sleepstudy, package = "lme4")
@

\begin{figure}[htp]
\begin{center}
<<Cs_027_sleep-plot, fig=TRUE, echo=FALSE>>=
library(lattice)
xyplot(Reaction ~ Days | Subject, sleepstudy,
  type = c("g", "p", "r"),
  index = function(x, y) coef(lm(y ~ x))[1],
  xlab = "Days of sleep deprivation",
  ylab = "Average reaction time (ms)", aspect = "xy"
)
@
\end{center}
\caption{Plot of the sleep study data (package lme4).}
\label{fig:sleep}
\end{figure}


We set up the data into a matrix for the \verb@MARSS()@ function with each subject as a row with day across the columns. The explanatory variable is the the day number 0 to 9 and we make this into a matrix with one row and day across the columns.
<<Cs_028_setupdata, keep.source=TRUE>>=
# number of subjects
nsub <- length(unique(sleepstudy$Subject))
ndays <- length(sleepstudy$Days) / nsub
dat <- matrix(sleepstudy$Reaction, nsub, ndays, byrow = TRUE)
rownames(dat) <- paste("sub", unique(sleepstudy$Subject), sep = ".")
exp.var <- matrix(sleepstudy$Days, 1, ndays, byrow = TRUE)
@

Let's start with a simple regression where each subject has a separate intercept (reaction time at day 0) but the slope (increase in reaction time with each successive day) is the same across the 18 subjects. Mathematically the model is
\begin{equation}\label{eqn:sleep.mod1}
\begin{gathered}
\begin{bmatrix}resp_1\\resp_2\\ \dots \\resp_{18}\end{bmatrix}_t = \begin{bmatrix}\alpha_1\\ \alpha_2\\ \dots \\ \alpha_{18}\end{bmatrix} + \begin{bmatrix}\beta\\ \beta \\ \dots \\ \beta \end{bmatrix} day_t + \begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \\
\begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}r&0&\dots&0\\ 0&r&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&r\end{bmatrix} \right)
\end{gathered}
\end{equation}
The response time of subject $i$ is a subject specific intercept ($\alpha_i$) plus an effect of day at time $t$ that doesn't vary by subject and error that is i.i.d. across subject and day.

We specify and fit this model as follows
<<Cs_029_sleepmod1, results=hide>>=
sleep.model <- list(
  A = "unequal", B = "zero", x0 = "zero", U = "zero",
  D = matrix("b1", nsub, 1), d = exp.var, tinitx = 0, Q = "zero"
)
sleep.mod1 <- MARSS(dat, model = sleep.model)
@
This is the same as the following with \verb@lm()@:
<<Cs_030_sleepmod1-lm>>=
sleep.lm1 <- lm(Reaction ~ -1 + Subject + Days, data = sleepstudy)
@

Now let's allow each subject to have different slopes (increase in reaction time with each successive day) across subjects. This model is
\begin{equation}\label{eqn:sleep.mod2}
\begin{gathered}
\begin{bmatrix}resp_1\\resp_2\\ \dots \\resp_{18}\end{bmatrix}_t = \begin{bmatrix}\alpha_1\\ \alpha_2\\ \dots \\ \alpha_{18}\end{bmatrix} + \begin{bmatrix}\beta_1\\ \beta_2 \\ \dots \\ \beta_{18} \end{bmatrix} day_t + \begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \\
\begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}r&0&\dots&0\\ 0&r&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&r\end{bmatrix} \right)
\end{gathered}
\end{equation}

We specify and fit this model as
<<Cs_031_sleepmod2>>=
sleep.model <- list(
  A = "unequal", B = "zero", x0 = "zero", U = "zero",
  D = "unequal", d = exp.var, tinitx = 0, Q = "zero"
)
sleep.mod2 <- MARSS(dat, model = sleep.model, silent = TRUE)
@
This is the same as the following with \verb@lm()@:
<<Cs_032_sleepmod2-lm>>=
sleep.lm2 <- lm(Reaction ~ 0 + Subject + Days:Subject, data = sleepstudy)
@

We can repeat the above but allow the residual variance to differ across subjects by setting \verb@R="diagonal and unequal"@. This model is
\begin{equation}\label{eqn:sleep.mod3}
\begin{gathered}
\begin{bmatrix}resp_1\\resp_2\\ \dots \\resp_{18}\end{bmatrix}_t = \begin{bmatrix}\alpha_1\\ \alpha_2\\ \dots \\ \alpha_{18}\end{bmatrix} + \begin{bmatrix}\beta_1\\ \beta_2 \\ \dots \\ \beta_{18} \end{bmatrix} day_t + \begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \\
\begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}r_1&0&\dots&0\\ 0&r_2&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&r_{18}\end{bmatrix} \right)
\end{gathered}
\end{equation}
<<Cs_033_sleepmod3>>=
sleep.model <- list(
  A = "unequal", B = "zero", x0 = "zero", U = "zero",
  D = "unequal", d = exp.var, tinitx = 0, Q = "zero",
  R = "diagonal and unequal"
)
sleep.mod3 <- MARSS(dat, model = sleep.model, silent = TRUE)
@

Or we can allow AR(1) errors across subjects and allow each subject to have its own AR(1) parameters for this error.  This model is \index{multivariate linear regression!with autocorrelated errors}
\begin{equation}\label{eqn:sleep.mod4}
\begin{gathered}
\begin{bmatrix}resp_1\\resp_2\\ \dots \\resp_{18}\end{bmatrix}_t = \begin{bmatrix}\alpha_1\\ \alpha_2\\ \dots \\ \alpha_{18}\end{bmatrix} + \begin{bmatrix}\beta_1\\ \beta_2 \\ \dots \\ \beta_{18} \end{bmatrix} day_t + \begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t + \begin{bmatrix}x_1\\x_2\\ \dots \\x_{18}\end{bmatrix}_t\\
\begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}r_1&0&\dots&0\\ 0&r_2&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&r_{18}\end{bmatrix} \right)\\
\begin{bmatrix}x_1\\x_2\\ \dots \\x_{18}\end{bmatrix}_t = \begin{bmatrix}b_1&0&\dots&0\\ 0&b_2&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&b_{18}\end{bmatrix} \begin{bmatrix}x_1\\x_2\\ \dots \\x_{18}\end{bmatrix}_{t-1} + \begin{bmatrix}w_1\\w_2\\ \dots \\w_{18}\end{bmatrix}_t\\
\begin{bmatrix}w_1\\w_2\\ \dots \\w_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}q_1&0&\dots&0\\ 0&q_2&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&q_{18}\end{bmatrix} \right)
\end{gathered}
\end{equation}
We fit this model as
<<Cs_034_sleepmod4>>=
inits <- list(A = coef(sleep.mod3)$A, D = coef(sleep.mod3)$D)
# estimate a separate intercept for each but slope is the same
sleep.model <- list(
  A = "unequal", B = "diagonal and unequal", x0 = "zero", U = "zero",
  D = "unequal", d = exp.var, tinitx = 0, Q = "diagonal and unequal",
  R = "diagonal and unequal"
)
sleep.mod4 <- MARSS(dat, model = sleep.model, inits = inits, silent = TRUE)
@
It is not obvious how to specify these last two models using \verb@gls()@ or if it is possible.

We can also allow each subject to have his/her own error process but specify that the parameters of these ($b$, $q$ and $r$) are the same across subjects.  We do this by using \verb@"diagonal and equal"@.  Mathematically this model is
\begin{equation}\label{eqn:sleep.mod5}
\begin{gathered}
\begin{bmatrix}resp_1\\resp_2\\ \dots \\resp_{18}\end{bmatrix}_t = \begin{bmatrix}\alpha_1\\ \alpha_2\\ \dots \\ \alpha_{18}\end{bmatrix} + \begin{bmatrix}\beta_1\\ \beta_2 \\ \dots \\ \beta_{18} \end{bmatrix} day_t + \begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t + \begin{bmatrix}x_1\\x_2\\ \dots \\x_{18}\end{bmatrix}_t\\
\begin{bmatrix}v_1\\v_2\\ \dots \\v_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}r&0&\dots&0\\ 0&r&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&r\end{bmatrix} \right)\\
\begin{bmatrix}x_1\\x_2\\ \dots \\x_{18}\end{bmatrix}_t = \begin{bmatrix}b&0&\dots&0\\ 0&b&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&b\end{bmatrix} \begin{bmatrix}x_1\\x_2\\ \dots \\x_{18}\end{bmatrix}_{t-1} + \begin{bmatrix}w_1\\w_2\\ \dots \\w_{18}\end{bmatrix}_t\\
\begin{bmatrix}w_1\\w_2\\ \dots \\w_{18}\end{bmatrix}_t \sim \N \left(0, \begin{bmatrix}q&0&\dots&0\\ 0&q&\dots&0\\ \dots&\dots&\dots&\dots \\ 0&0&0&q\end{bmatrix} \right)
\end{gathered}
\end{equation}
We specify and fit this model as
<<Cs_035_sleepmod5>>=
inits <- list(A = coef(sleep.mod3)$A, D = coef(sleep.mod3)$D)
# estimate a separate intercept for each but slope is the same
sleep.model <- list(
  A = "unequal", B = "diagonal and equal", x0 = "zero", U = "zero",
  D = "unequal", d = exp.var, tinitx = 0, Q = "diagonal and equal",
  R = "diagonal and equal"
)
sleep.mod5 <- MARSS(dat, model = sleep.model, inits = inits, silent = TRUE)
@
This is fairly close to this model fit with \verb@gls()@.\index{functions!gls}
<<Cs_036_mod5-gls>>=
sleep.mod5.gls <- gls(Reaction ~ 0 + Subject + Days:Subject,
  data = sleepstudy,
  correlation = corAR1(form = ~ 1 | Subject), method = "ML"
)
@
The way the variance-covariance structure is modeled is a little different but it is the same idea.

<<Cs_037_makemodeltable, echo=FALSE, keep.source=FALSE>>=
if (!exists("tabledir")) tabledir <- ""
slope.names <- paste("D", rownames(dat), sep = ".")
phi.names <- names(coef(sleep.mod4, type = "vector"))[str_detect(names(coef(sleep.mod4, type = "vector")), "B.")]

model.data <- cbind(
  c(logLik(sleep.lm2), coef(sleep.lm2)[19:36], rep(NA, nsub)),
  c(sleep.mod2$logLik, coef(sleep.mod2, type = "vector")[slope.names], rep(NA, nsub)),
  c(sleep.mod3$logLik, coef(sleep.mod3, type = "vector")[slope.names], rep(NA, nsub)),
  c(sleep.mod4$logLik, coef(sleep.mod4, type = "vector")[c(slope.names, phi.names)]),
  c(sleep.mod5$logLik, coef(sleep.mod5, type = "vector")[c(slope.names, rep("B.diag", nsub))]),
  c(logLik(sleep.mod5.gls), coef(sleep.mod5.gls)[19:36], rep(coef(sleep.mod5.gls$modelStruct[[1]], unconstrained = FALSE), nsub))
)
rownames(model.data) <- c("logLik", paste("slope", unique(sleepstudy$Subject)), paste("phi", unique(sleepstudy$Subject)))
colnames(model.data) <- c("lm", "mod2 em", "mod3 em", "mod4 em", "mod5 em", "mod5 gls")
tmpaln <- "c" # figure out the number of cols automatically
for (i in 1:ncol(model.data)) tmpaln <- paste(tmpaln, "c", sep = "")
thetable <- xtable(model.data, caption = "Parameter estimates of different versions of the model where each subject has a separate intercept (response time on normal sleep) and different slope by day (increase in response time with each day of sleep deprivation).  The model types are discussed in the text.", label = "ref:tablesleepstudy", align = tmpaln, digits = 2)
print(thetable, type = "latex", file = paste(tabledir, "tablesleepstudy.tex", sep = ""), include.rownames = TRUE, include.colnames = TRUE, caption.placement = "top", table.placement = "htp", sanitize.text.function = function(x) {
  x
}, hline.after = c(-1, 0, nrow(model.data)))
@
\input{figures/tablesleepstudy}

\section{Discussion}
The purpose of this chapter is to illustrate how linear regression models with multivariate explanatory variables can be written in MARSS form and fit with the \verb@MARSS()@ function\footnote{with caveat that one must always be careful when the likelihood surface has prominent ridges which will occur with collinear explanatory variables.}.  This is to help one understand the relationship between the MARSS model form and the more familiar multivariate linear model forms. Obviously \R has many, many excellent packages for linear regression and generalized linear regression (non-Gaussian errors).  While the \{MARSS\} package can fit a variety of linear regression models with Gaussian errors, that is not what the package is designed to do.  The \{MARSS\} package is designed for fitting models that cannot be fit with typical linear regression: multivariate autoregressive state-space models with inputs (explanatory variables) and linear constraints.

<<reset, echo=FALSE, include.source=FALSE>>=
options(prompt = "> ", continue = " +", width = 120)
@
